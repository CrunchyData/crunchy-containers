= Ceph Setup

this is sort of experimental in that I do not know enough about Ceph
to advise on this, but I was able to get this example working after 
some *effort*.

== Ceph Host Setup

for this test, I installed ceph on a centos
host, I then connect to it from the RHEL ose host.

Here is the how-to that I followed when
installing ceph on the centos host:

link:https://docs.openshift.org/latest/install_config/storage_examples/ceph_example.html

link:http://docs.ceph.com/docs/master/start/quick-start-preflight/#red-hat-package-manager-rpm

link:https://access.redhat.com/discussions/1161713

here is the list of commands on the centos box:

Here is the yum repo to add:
....
[root@centos ~]# cat /etc/yum.repos.d/ceph.repo 
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://ceph.com/rpm-giant/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc
....

then

....
yum -y install ceph-deploy
yum -y install ntp ntpdate ntp-doc
yum -y install yum-plugin-priorities
....

then as ceph user, for the host called *centos* we create
a single node on the *centos* host:

....
mkdir my-cluster
cd my-cluster
ceph-deploy new centos
<<made edits to ceph.conf>>
ceph-deploy install centos
ceph-deploy mon create-initial
sudo mkdir /var/local/osd0
sudo chown -R ceph:ceph /var/local/osd0
ceph-deploy osd prepare centos:/var/local/osd0
ceph-deploy osd activate centos:/var/local/osd0
sudo chmod +r /etc/ceph/ceph.client.admin.keyring
ceph health
ceph-deploy mds create centos
ceph-deploy mon create-initial
ceph osd pool create default 100 100
ceph osd lspools
rados df
rados put testobject testfile.txt  --pool=default
rados -p default ls
ceph osd map default testobject
ceph auth get-key client.admin | base64
ceph-deploy admin 192.168.0.109
rbd ls
....

the ceph.conf looks like:

....
[global]
fsid = 7e99717e-bd56-4cb9-8f9d-b901722bded6
mon_initial_members = centos
mon_host = 192.168.0.108
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
filestore_xattr_use_omap = true
osd pool default size = 1
public network = 192.168.0.108/32
....

the yum.repos.d/ceph.conf looks like:

....
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-infernalis/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-infernalis/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-infernalis/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
....


== OSE Host Setup

on the RHEL ose host (ceph client), I ran the following
to test out a ceph mounted RBD (block device):

....
yum -y install ceph-common
rbd create foo --size 1024 
sudo rbd map foo --name client.admin
sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo
rbd
ls /dev/rbd*
sudo mkfs.ext4 -m0 /dev/rbd0
sudo mkdir /mnt/ceph-block-device
sudo mount /dev/rbd0 /mnt/ceph-block-device/
cd /mnt/ceph-block-device/
touch it
sudo chmod 777 /mnt/ceph-block-device/
touch it
....


=== Debugging

I found that if you can not run this command, your pod will fail to startup:

....
sudo rbd map disk01 --pool rbd --id admin -m 192.168.0.108:6789 --key=AQCGqzNXnxd2HRAAksfE3fnHmC64ZWxNPpah9Q==
....

You can find the image name using 
....
rbd ls
....

You can find the pool name using
....
ceph osd lspools
....

You can find the key value using
....
ceph auth get-key client.admin
....

This command is attempted by Kube, so the values in your PV must match what will work in this command!

