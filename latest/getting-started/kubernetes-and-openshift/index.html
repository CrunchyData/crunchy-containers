<!DOCTYPE html>
<html>
  <head>
    <title>Crunchy Data Container Suite Documentation</title>
    
      <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="generator" content="Hugo 0.48" />

<title>Kubernetes and Openshift :: Crunchy Data Container Suite Documentation</title>
<link rel="shortcut icon" href="https://crunchydata.github.io/crunchy-containers/latest/favicon.ico" type="image/x-icon" />
<link href="https://crunchydata.github.io/crunchy-containers/latest/css/nucleus.css" rel="stylesheet">
<link href="https://crunchydata.github.io/crunchy-containers/latest/css/font-awesome.min.css" rel="stylesheet">
<link href="https://crunchydata.github.io/crunchy-containers/latest/css/hybrid.css" rel="stylesheet">
<link href="https://crunchydata.github.io/crunchy-containers/latest/css/featherlight.min.css" rel="stylesheet">
<link href="https://crunchydata.github.io/crunchy-containers/latest/css/auto-complete.css" rel="stylesheet">
<link href="https://crunchydata.github.io/crunchy-containers/latest/theme-original/style.css" rel="stylesheet">

<link rel="stylesheet" href="https://crunchydata.github.io/crunchy-containers/latest/css/bootstrap.min.css">
<script src="https://crunchydata.github.io/crunchy-containers/latest/js/jquery-2.x.min.js"></script>
<style type="text/css">
  :root #header + #content > #left > #rlblock_left {
    display:none !important;
  }
</style>
<meta name="description" content="">


    
  </head>
  <body data-url="/getting-started/kubernetes-and-openshift/">
    
      <div id="headermain"></div>
<nav id="sidebar" class="">



<div class="highlightable">
  <div id="header-wrapper">
    <div id="header">
      
	
  
    <a class="baselink" href="https://crunchydata.github.io/crunchy-containers/latest/">Crunchy Data Container Suite Documentation</a>
  

    </div>
        <div class="searchbox">
		    <label for="search-by"><i class="fa fa-search"></i></label>
		    <input data-search-input id="search-by" type="text" placeholder="Search...">
		    <span data-search-clear=""><i class="fa fa-close"></i></span>
		</div>
		<script type="text/javascript" src="https://crunchydata.github.io/crunchy-containers/latest/js/lunr.min.js"></script>
		<script type="text/javascript" src="https://crunchydata.github.io/crunchy-containers/latest/js/auto-complete.js"></script>
		<script type="text/javascript">
        
            var baseurl = "https:\/\/crunchydata.github.io\/crunchy-containers\/latest\/";
        
		</script>
		<script type="text/javascript" src="https://crunchydata.github.io/crunchy-containers/latest/js/search.js"></script>
  </div>

      <ul class="topics">
            <li data-nav-id="/" class="dd-item">
            <a href="https://crunchydata.github.io/crunchy-containers/latest/"><i class="fa fa-fw fa-home"></i></a>
            </li>
    <li data-nav-id="/installation/" class="dd-item haschildren
        ">
      <div>
      <a href="https://crunchydata.github.io/crunchy-containers/latest/installation/">Installation</a><i class="fa fa-angle-right fa-lg category-icon"></i>

      </div>
        <ul>
      <li data-nav-id="/installation/environment-setup/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/installation/environment-setup/">
            Environment Setup
          </a>
        </div>
    </li>
      <li data-nav-id="/installation/build-the-containers/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/installation/build-the-containers/">
            Build the Containers
          </a>
        </div>
    </li>
      <li data-nav-id="/installation/storage-configuration/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/installation/storage-configuration/">
            Storage Configuration
          </a>
        </div>
    </li>
        </ul>
    </li>
    <li data-nav-id="/getting-started/" class="dd-item parent haschildren
        ">
      <div>
      <a href="https://crunchydata.github.io/crunchy-containers/latest/getting-started/">Getting Started</a>
            <i class="fa fa-angle-down fa-lg category-icon"></i>

      </div>
        <ul>
    <li data-nav-id="/getting-started/kubernetes-and-openshift/" class="dd-item parent active
        ">
      <div>
      <a href="https://crunchydata.github.io/crunchy-containers/latest/getting-started/kubernetes-and-openshift/">Kubernetes and Openshift</a>

      </div>
    </li>
        </ul>
    </li>
    <li data-nav-id="/container-specifications/" class="dd-item haschildren
        ">
      <div>
      <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/">Container Specifications</a><i class="fa fa-angle-right fa-lg category-icon"></i>

      </div>
        <ul>
      <li data-nav-id="/container-specifications/crunchy-postgres/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-postgres/">
            crunchy-postgres
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-postgres-gis/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-postgres-gis/">
            crunchy-postgres-gis
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-backup/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-backup/">
            crunchy-backup
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-backrest-restore/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-backrest-restore/">
            crunchy-backrest-restore
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-pgdump/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-pgdump/">
            crunchy-pgdump
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-pgrestore/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-pgrestore/">
            crunchy-pgrestore
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-collect/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-collect/">
            crunchy-collect
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-scheduler/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-scheduler/">
            crunchy-scheduler
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-prometheus/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-prometheus/">
            crunchy-prometheus
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-grafana/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-grafana/">
            crunchy-grafana
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-pgadmin4/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-pgadmin4/">
            crunchy-pgadmin4
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-watch/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-watch/">
            crunchy-watch
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-vacuum/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-vacuum/">
            crunchy-vacuum
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-upgrade/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-upgrade/">
            crunchy-upgrade
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-sim/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-sim/">
            crunchy-sim
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-pgbouncer/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-pgbouncer/">
            crunchy-pgbouncer
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-pgpool/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-pgpool/">
            crunchy-pgpool
          </a>
        </div>
    </li>
      <li data-nav-id="/container-specifications/crunchy-pgbadger/" class="dd-item">
        <div>
          <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-pgbadger/">
            crunchy-pgbadger
          </a>
        </div>
    </li>
        </ul>
    </li>
    <li data-nav-id="/contributing/" class="dd-item
        ">
      <div>
      <a href="https://crunchydata.github.io/crunchy-containers/latest/contributing/">Contributing</a>

      </div>
    </li>



        <section id="shortcuts">
                <li class="" role=""><h3>More</h3><a href="https://github.com/CrunchyData/crunchy-containers" target="_blank" rel="noopener"><i class='fa fa-github'></i> <label>Github repo</label></a></li>
                <li class="" role=""><a href="https://kubernetes.io/docs/" target="_blank" rel="noopener"><i class='fa fa-bookmark'></i> <label>Kubernetes Documentation</label></a></li>
                <li class="" role=""><a href="https://github.com/CrunchyData/crunchy-containers/blob/master/LICENSE.md" target="_blank" rel="noopener"><i class='fa fa-file'></i> <label>License</label></a></li>
        </section>

    <hr />
    <li></li>

    </ul>

 <section id="footer">
    </section>
  </div>
</nav>



<section id="body">
<div id="overlay"></div>
<div class="padding highlightable">

  <div id="top-bar">
    
      
      
      
    <div id="top-github-link">
      <a class="github-link" href="https://crunchydata.github.io/crunchy-containers/latest/contributing/" target="blank">
        <i class="fa fa-info-circle"></i>
        How to contribute
      </a>
      <a class="github-link" href="https://github.com/CrunchyData/crunchy-containers/edit/master/hugo/content/getting-started/kubernetes-and-openshift/_index.adoc" target="blank">
        <i class="fa fa-code-fork"></i>
        Improve this page
      </a>
    </div><div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
        <span id="sidebar-toggle-span">
          <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
            <i class="fa fa-bars"></i>
          </a>
        </span>
        <span id="toc-menu"><i class="fa fa-list-alt"></i></span>
        <span class="links">
        







 <a href='https://crunchydata.github.io/crunchy-containers/latest/'>Crunchy Data Container Suite</a> > <a href='https://crunchydata.github.io/crunchy-containers/latest/getting-started/'>Getting Started</a> > Kubernetes and Openshift






        </span>
    </div>

    
    <div class="progress">
        <div class="wrapper">
    
        </div>
    </div>
    

  </div>


<div id="body-inner">
  
    <h1>Kubernetes and Openshift</h1>
  



    
    
    

<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_getting_started">Getting Started</a></li>
<li><a href="#_example_conventions">Example Conventions</a></li>
<li><a href="#_administration">Administration</a>
<ul class="sectlevel2">
<li><a href="#_password_management">Password Management</a></li>
<li><a href="#_kubernetes_secrets">Kubernetes Secrets</a></li>
<li><a href="#_pgadmin4">pgAdmin4</a></li>
<li><a href="#_pgadmin4_with_tls">pgAdmin4 with TLS</a></li>
<li><a href="#_upgrade">Upgrade</a></li>
<li><a href="#_crunchy_scheduler">Crunchy Scheduler</a></li>
<li><a href="#_vacuum">Vacuum</a></li>
<li><a href="#_systemd">systemd</a></li>
<li><a href="#_centralized_logging">Centralized Logging</a></li>
</ul>
</li>
<li><a href="#_backup_and_restoration">Backup and Restoration</a>
<ul class="sectlevel2">
<li><a href="#_pg_dump">pg_dump</a></li>
<li><a href="#_pg_restore">pg_restore</a></li>
<li><a href="#_pgbackrest">pgBackRest</a></li>
<li><a href="#_pg_basebackup">pg_basebackup</a></li>
<li><a href="#_point_in_time_recovery_pitr">Point in Time Recovery (PITR)</a></li>
</ul>
</li>
<li><a href="#_connection_pooling">Connection Pooling</a>
<ul class="sectlevel2">
<li><a href="#_pgbouncer">pgBouncer</a></li>
<li><a href="#_pgpool_ii">pgPool II</a></li>
</ul>
</li>
<li><a href="#_database">Database</a>
<ul class="sectlevel2">
<li><a href="#_single_primary">Single Primary</a></li>
<li><a href="#_postgresql_deployment">PostgreSQL Deployment</a></li>
<li><a href="#_replication">Replication</a></li>
<li><a href="#_synchronous_replication">Synchronous Replication</a></li>
<li><a href="#_statefulsets">Statefulsets</a></li>
<li><a href="#_geospatial_postgis">Geospatial (PostGIS)</a></li>
<li><a href="#_custom_configuration">Custom Configuration</a></li>
<li><a href="#_ssl_authentication">SSL Authentication</a></li>
<li><a href="#_docker_swarm">Docker Swarm</a></li>
</ul>
</li>
<li><a href="#_failover">Failover</a>
<ul class="sectlevel2">
<li><a href="#_watch">Watch</a></li>
</ul>
</li>
<li><a href="#_metrics_and_performance">Metrics and Performance</a>
<ul class="sectlevel2">
<li><a href="#_pgbadger">pgBadger</a></li>
<li><a href="#_metrics_collection">Metrics Collection</a></li>
<li><a href="#_pg_audit">pg_audit</a></li>
</ul>
</li>
</ul>
</div>
<div class="paragraph">
<p>Latest Release: 2.2.0 2018-11-27</p>
</div>
<div class="sect1">
<h2 id="_getting_started">Getting Started</h2>
<div class="sectionbody">
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>The Kubernetes and OpenShift examples provided on this page have been designed using single-node Kubernetes/OCP clusters
whose host machines provide any required supporting infrastructure or services (e.g. local hostPath storage or access
to an NFS share). Therefore, for the best results when running these examples, it is recommended that you utilize a
single-node architecture as well.</p>
</div>
</div>

</div>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>The examples located in the <strong>kube</strong> directory work on both Kubernetes and OpenShift. Ensure the <code>CCP_CLI</code> environment variable
is set to the correct binary for your environment.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Set the environment variable in <code>.bashrc</code> to ensure the examples will work in your environment.</p>
</div>
<div class="literalblock">
<div class="content">
<pre># Kubernetes
export CCP_CLI=kubectl

# OpenShift
export CCP_CLI=oc</pre>
</div>
</div>
<div class="paragraph">
<p>Here are some useful resources for finding the right commands to troubleshoot &amp; modify containers in
the various environments shown in this guide:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="http://www.bogotobogo.com/DevOps/Docker/Docker-Cheat-Sheet.php">Docker Cheat Sheet</a></p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/">Kubectl Cheat Sheet</a></p>
</li>
<li>
<p><a href="https://github.com/nekop/openshift-sandbox/blob/master/docs/command-cheatsheet.md">OpenShift Cheat Sheet</a></p>
</li>
<li>
<p><a href="https://github.com/kubernetes/helm/blob/master/docs/using_helm.md">Helm Cheat Sheet</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_example_conventions">Example Conventions</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The examples provided in Container Suite are simple examples that
are meant to demonstrate key Container Suite features.  These
examples can be used to build more production level deployments
as dictated by user requirements specific to their operating
environments.</p>
</div>
<div class="paragraph">
<p>The examples generally follow these conventions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>There is a <strong>run.sh</strong> script that you will execute to start the example.</p>
</li>
<li>
<p>There is a <strong>cleanup.sh</strong> script that you will execute to shutdown and cleanup the example.</p>
</li>
<li>
<p>Each example will create resources such as Secrets, ConfigMaps, Services, and PersistentVolumeClaims, all which follow a naming convention of <strong>&lt;example name&gt;-&lt;optional description suffix&gt;</strong>. For example an example called <strong>primary</strong> might have a PersistentVolumeClaim called <strong>primary-pgconf</strong> to describe the purpose of that particular PVC.</p>
</li>
<li>
<p>The folder names for each example give a clue as to which Container Suite feature it demonstrates. For instance, the <strong>examples/kube/pgaudit</strong> example demonstrates how to enable the pg_audit capability of the crunchy-postgres container.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_administration">Administration</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_password_management">Password Management</h3>
<div class="paragraph">
<p>The passwords used for the PostgreSQL user accounts are generated
by the OpenShift <code>process</code> command.  To inspect what value is
supplied, you can inspect the primary pod as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} get pod pr-primary -o json | grep -C 1 'PG_USER\|PG_PASSWORD\|PG_DATABASE'</pre>
</div>
</div>
<div class="paragraph">
<p>This will give you the environment variable values for the database created by default
in addition to the username and password of the standard user.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>PG_USER</code></p>
</li>
<li>
<p><code>PG_PASSWORD</code></p>
</li>
<li>
<p><code>PG_DATABASE</code></p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_kubernetes_secrets">Kubernetes Secrets</h3>
<div class="paragraph">
<p>You can use Kubernetes Secrets to set and maintain your database
credentials.  Secrets requires you base64 encode your user and password
values as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>echo -n 'myuserid' | base64</pre>
</div>
</div>
<div class="paragraph">
<p>You will paste these values into  your JSON secrets files for values.</p>
</div>
<div class="paragraph">
<p>This example allows you to set the PostgreSQL passwords
using Kubernetes Secrets.</p>
</div>
<div class="paragraph">
<p>The secret uses a base64 encoded string to represent the
values to be read by the container during initialization.  The
encoded password value is <strong>password</strong>.  Run the example
as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/secret
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The secrets are mounted in the <code>/pguser</code>, <code>/pgprimary</code>, and <code>/pgroot</code> volumes within the
container and read during initialization.  The container
scripts create a PostgreSQL user with those values, and sets the passwords
for the primary user and PostgreSQL superuser using the mounted secret volumes.</p>
</div>
<div class="paragraph">
<p>When using secrets, you do <strong>NOT</strong> have to specify the following
environment variables if you specify all three secrets volumes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>PG_USER</code></p>
</li>
<li>
<p><code>PG_PASSWORD</code></p>
</li>
<li>
<p><code>PG_ROOT_PASSWORD</code></p>
</li>
<li>
<p><code>PG_PRIMARY_USER</code></p>
</li>
<li>
<p><code>PG_PRIMARY_PASSWORD</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can test the container as follows. In all cases, the password is <strong>password</strong>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h secret -U pguser1 postgres
psql -h secret -U postgres postgres
psql -h secret -U primaryuser postgres</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pgadmin4">pgAdmin4</h3>
<div class="paragraph">
<p>This example deploys the pgadmin4 v2 web user interface
for PostgreSQL without TLS.</p>
</div>
<div class="paragraph">
<p>After running the example, you should be able to browse to <a href="http://127.0.0.1:5050" class="bare">http://127.0.0.1:5050</a>
and log into the web application with the following configured credentials:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Username : <strong>admin@admin.com</strong></p>
</li>
<li>
<p>Password: <strong>password</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you are running this example using Kubernetes or
OpenShift, it is required to use a port-forward proxy to access the dashboard.</p>
</div>
<div class="paragraph">
<p>To start the port-forward proxy run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} port-forward pgadmin4-http 5050:5050</pre>
</div>
</div>
<div class="paragraph">
<p>To access the pgAdmin4 dashboard through the proxy, navigate to <strong><a href="http://127.0.0.1:5050" class="bare">http://127.0.0.1:5050</a></strong>
in a browser.</p>
</div>
<div class="paragraph">
<p>See the <a href="http://pgadmin.org">pgAdmin4 documentation</a> for more details.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker">Docker</h4>
<div class="paragraph">
<p>To run this example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgadmin4-http
./run.sh</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Start the container as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgadmin4-http
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>An emptyDir with write access must be mounted to the <code>/run/httpd</code> directory in OpenShift.</p>
</div>
</div>

</div>
</div>
</div>
<div class="sect2">
<h3 id="_pgadmin4_with_tls">pgAdmin4 with TLS</h3>
<div class="paragraph">
<p>This example deploys the pgadmin4 v2 web user interface
for PostgreSQL with TLS.</p>
</div>
<div class="paragraph">
<p>After running the example, you should be able to browse to <a href="https://127.0.0.1:5050" class="bare">https://127.0.0.1:5050</a>
and log into the web application with the following configured credentials:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Username : <strong>admin@admin.com</strong></p>
</li>
<li>
<p>Password: <strong>password</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you are running this example using Kubernetes or
OpenShift, it is required to use a port-forward proxy to access the dashboard.</p>
</div>
<div class="paragraph">
<p>To start the port-forward proxy run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} port-forward pgadmin4-https 5050:5050</pre>
</div>
</div>
<div class="paragraph">
<p>To access the pgAdmin4 dashboard through the proxy, navigate to <strong><a href="https://127.0.0.1:5050" class="bare">https://127.0.0.1:5050</a></strong>
in a browser.</p>
</div>
<div class="paragraph">
<p>See the <a href="http://pgadmin.org">pgadmin4 documentation</a> for more details.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_2">Docker</h4>
<div class="paragraph">
<p>To run this example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgadmin4-https
./run.sh</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_2">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Start the container as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgadmin4-https
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>An emptyDir with write access must be mounted to the <code>/run/httpd</code> directory in OpenShift.</p>
</div>
</div>

</div>
</div>
</div>
<div class="sect2">
<h3 id="_upgrade">Upgrade</h3>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>This example assumes you have run <strong>primary</strong> using a PG 9.5 or 9.6 image
such as <code>centos7-9.5.15-2.2.0</code> prior to running this upgrade.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Starting in release 1.3.1, the upgrade container will let
you perform a <code>pg_upgrade</code> either from a PostgreSQL version 9.5 database to
9.6 or from 9.6 to 10.</p>
</div>
<div class="paragraph">
<p>Prior to running this example, make sure your <code>CCP_IMAGE_TAG</code>
environment variable is using the next major version of PostgreSQL that you
want to upgrade to. For example, if you&#8217;re upgrading from 9.5 to 9.6, make
sure the variable references a PG 9.6 image such as <code>centos7-9.6.11-2.2.0</code>.</p>
</div>
<div class="paragraph">
<p>This will create the following in your Kubernetes environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>a Kubernetes Job running the <strong>crunchy-upgrade</strong> container</p>
</li>
<li>
<p>a new data directory name <strong>upgrade</strong> found in the <strong>pgnewdata</strong> PVC</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>Data checksums on the Crunchy PostgreSQL container were enabled by default in version 2.1.0.
When trying to upgrade, it&#8217;s required that both the old database and the new database
have the same data checksums setting.  Prior to upgrade, check if <code>data_checksums</code>
were enabled on the database by running the following SQL: <code>SHOW data_checksums</code></p>
</div>
</div>

</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_3">Kubernetes and OpenShift</h4>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>Before running the example, ensure you edit <code>upgrade.json</code> and update the <code>OLD_VERSION</code>
and <code>NEW_VERSION</code> parameters to the ones relevant to your situation.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Start the upgrade as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/upgrade
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>If successful, the Job will end with a <strong>successful</strong> status. Verify
the results of the Job by examining the Job&#8217;s pod log:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} get pod -l job-name=upgrade
${CCP_CLI} logs -l job-name=upgrade</pre>
</div>
</div>
<div class="paragraph">
<p>You can verify the upgraded database by running the <code>post-upgrade.sh</code> script in the
<code>examples/kube/upgrade</code> directory.  This will create a PostgreSQL pod that mounts the
upgraded volume.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_crunchy_scheduler">Crunchy Scheduler</h3>
<div class="paragraph">
<p>The Crunchy Scheduler container implements a cronlike microservice within a namespace
to automate backups of a PostgreSQL database.</p>
</div>
<div class="paragraph">
<p>Currently Crunchy Scheduler only supports two types of tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>pgBackRest</p>
</li>
<li>
<p>pgBaseBackup</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This service watches Kubernetes for config maps with the label <code>crunchy-scheduler=true</code>.
If found the scheduler will parse the data found in the config map (json object) and
convert it to a scheduled task.  If the config map is removed, the scheduler will
delete the task.</p>
</div>
<div class="paragraph">
<p>See the following examples for creating config maps that Crunchy Scheduler can parse:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/kube/scheduler/configs/schedule-backrest-diff.json">pgBackRest Diff Backup</a></p>
</li>
<li>
<p><a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/kube/scheduler/configs/schedule-backrest-full.json">pgBackRest Full Backup</a></p>
</li>
<li>
<p><a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/kube/scheduler/configs/schedule-pgbasebackup.json">pgBaseBackup Backup</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Crunchy Scheduler requires a Service Account to create jobs (pgBaseBackup) and to
exec (pgBackRest).  See the <a href="https://github.com/CrunchyData/crunchy-containers/blob/scheduler/examples/kube/scheduler/scheduler-sa.json">scheduler example</a>
for the required permissions on this account.</p>
</div>
<div class="sect3">
<h4 id="_pgbackrest_schedules">pgBackRest Schedules</h4>
<div class="paragraph">
<p>To configure Crunchy Scheduler to create pgBackRest backups the following is required:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>pgBackRest schedule definition requires a deployment name.  The PostgreSQL pod should be created by a deployment.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_pgbasebackup_schedules">pgBaseBackup Schedules</h4>
<div class="paragraph">
<p>To configure Crunchy Scheduler to create pgBaseBackup scheduled backups, the following is required:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The name of the secret that contains the username and password the Scheduler will use to
configure the job template.  See <a href="https://github.com/CrunchyData/crunchy-containers/blob/scheduler/examples/kube/scheduler/primary/secret.json">the primary secret example</a>.
for the structure required by the Scheduler.</p>
</li>
<li>
<p>The name of the PVC created for the backups.  This should be created by the user prior to scheduling the task.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>When using pgBaseBackup schedules, it may be required to apply specific <code>supplementalGroups</code> or an <code>fsGroup</code>
to the backup job created by the scheduler.  To apply a specific <code>securityContext</code> for your
storage provider, mount a <code>backup-template.json</code> to <code>/configs</code> on the scheduler pod.</p>
</div>
<div class="paragraph">
<p>For an example of applying a custom template, <a href="https://github.com/CrunchyData/crunchy-containers/blob/scheduler/examples/kube/scheduler">see the schedule example</a>.</p>
</div>
</div>

</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_4">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>First, start the PostgreSQL example created for the Scheduler by running the following commands:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># Kubernetes
cd $CCPROOT/examples/kube/scheduler/primary
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The pod created should show a ready status before proceeding.</p>
</div>
<div class="paragraph">
<p>Next, start the scheduler by running the following command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># Kubernetes
cd $CCPROOT/examples/kube/scheduler
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Once the scheduler is deployed, register the backup tasks by running the following command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># Kubernetes
cd $CCPROOT/examples/kube/scheduler
./add-schedules.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The scheduled tasks will (these are just for fast results, not recommended for production):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>take a backup every minute using pgBaseBackup</p>
</li>
<li>
<p>take a full pgBackRest backup every even minute</p>
</li>
<li>
<p>take a diff pgBackRest backup every odd minute</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>View the logs for the <code>scheduler</code> pod until the tasks run:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} logs scheduler -f</pre>
</div>
</div>
<div class="paragraph">
<p>View the <code>pgBaseBackup</code> pods results after the backup completes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} logs &lt;basebackup pod name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>View the <code>pgBackRest</code> backups via exec after the backup completes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} exec -ti &lt;primary deployment pod name&gt; -- pgbackrest info</pre>
</div>
</div>
<div class="paragraph">
<p>Clean up the examples by running the following commands:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$CCPROOT/examples/kube/scheduler/primary/cleanup.sh
$CCPROOT/examples/kube/scheduler/cleanup.sh</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_vacuum">Vacuum</h3>
<div class="paragraph">
<p>You can perform a PostgreSQL vacuum command by running the crunchy-vacuum
container. You specify a database to vacuum using environment variables. By default,
vacuum is executed against the <strong>primary</strong> example container.</p>
</div>
<div class="paragraph">
<p>The crunchy-vacuum container image exists to allow a DBA a way to run a job either
individually or scheduled to perform a variety of vacuum operations.</p>
</div>
<div class="paragraph">
<p>This example performs a vacuum on a single table in the primary PostgreSQL
database. The crunchy-vacuum image is executed with the PostgreSQL connection
parameters to the single-primary PostgreSQL container.  The type of vacuum performed is
dictated by the environment variables passed into the job; these are defined with further detail
<a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/crunchy-vacuum">here</a>.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_3">Docker</h4>
<div class="paragraph">
<p>Run the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/vacuum
./run.sh</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_5">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/vacuum/
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Verify the job is completed:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} get job</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_systemd">systemd</h3>
<div class="paragraph">
<p>The crunchy-pg.service is an example of a systemd unit file
that starts and stops a container named crunchy-pg that
has already been created.</p>
</div>
<div class="paragraph">
<p>The example scripts are located in the following directory:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$CCPROOT/examples/systemd/</pre>
</div>
</div>
<div class="paragraph">
<p>There are two scripts within the directory.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>test-start.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This script is called by the systemd start execution.  The trick
with this script is that it blocks forever after starting the
docker crunchy-pg container. The blocking in the script
keeps systemd happy and thinking that this is a normal daemon.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>test-stop.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This script stops the test-start.sh script and also stops the
crunchy-pg Docker container.</p>
</div>
</div>
<div class="sect2">
<h3 id="_centralized_logging">Centralized Logging</h3>
<div class="paragraph">
<p>The logs generated by containers are critical for deployments because they provide insights into the
health of the system.  PostgreSQL logs are very detailed and there is some information that can only be
obtained from logs (but not limited to):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Connections and Disconnections of users</p>
</li>
<li>
<p>Checkpoint Statistics</p>
</li>
<li>
<p>PostgreSQL Server Errors</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Aggregrating container logs across multiple hosts allows administrators to audit, debug problems and prevent
repudiation of misconduct.</p>
</div>
<div class="paragraph">
<p>In the following example we will demonstrate how to setup Kubernetes and OpenShift to use centralized logging by using
an EFK (Elasticsearch, Fluentd and Kibana) stack.  Fluentd will run as a daemonset on each host within the Kubernetes
cluster and extract container logs, Elasticsearch will consume and index the logs gathered by Fluentd and Kibana will allow
users to explore and visualize the logs via a web dashboard.</p>
</div>
<div class="paragraph">
<p>To learn more about the EFK stack, see the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://www.elastic.co/products/elasticsearch" class="bare">https://www.elastic.co/products/elasticsearch</a></p>
</li>
<li>
<p><a href="https://www.fluentd.org/architecture" class="bare">https://www.fluentd.org/architecture</a></p>
</li>
<li>
<p><a href="https://www.elastic.co/products/kibana" class="bare">https://www.elastic.co/products/kibana</a></p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_configure_postgresql_for_centralized_logging">Configure PostgreSQL for Centralized Logging</h4>
<div class="paragraph">
<p>By default, Crunchy PostgreSQL logs to files in the <code>/pgdata</code> directory.  In order to get the logs
out of the container we need to configure PostgreSQL to log to <code>stdout</code>.</p>
</div>
<div class="paragraph">
<p>The following settings should be configured in <code>postgresql.conf</code> to make PostgreSQL log to <code>stdout</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>log_destination = 'stderr'
logging_collector = off</code></pre>
</div>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>Changes to logging settings require a restart of the PostgreSQL container to take effect.</p>
</div>
</div>

</div>
</div>
<div class="sect3">
<h4 id="_deploying_the_efk_stack_on_openshift_container_platform">Deploying the EFK Stack On OpenShift Container Platform</h4>
<div class="paragraph">
<p>OpenShift Container Platform can be installed with an EFK stack.  For more information about
configuring OpenShift to create an EFK stack, see the official documentation:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://docs.openshift.com/container-platform/3.11/install_config/aggregate_logging.html" class="bare">https://docs.openshift.com/container-platform/3.11/install_config/aggregate_logging.html</a></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_deploying_the_efk_stack_on_kubernetes">Deploying the EFK Stack On Kubernetes</h4>
<div class="paragraph">
<p>First, deploy the EFK stack by running the example using the following commands:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/centralized-logging/efk
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>Elasticsearch is configured to use an <code>emptyDir</code> volume in this example.  Configure this example to provide a
persistent volume when deploying into production.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Next, verify the pods are running in the <code>kube-system</code> namespace:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>${CCP_CLI?} get pods -n kube-system --selector=k8s-app=elasticsearch-logging
${CCP_CLI?} get pods -n kube-system --selector=k8s-app=fluentd-es
${CCP_CLI?} get pods -n kube-system --selector=k8s-app=kibana-logging</code></pre>
</div>
</div>
<div class="paragraph">
<p>If all pods deployed successfully elasticsearch should already be receiving container logs from Fluentd.</p>
</div>
<div class="paragraph">
<p>Next we will deploy a PostgreSQL Cluster (primary and replica deployments) to demonstrate PostgreSQL logs
are being captured by Fluentd.</p>
</div>
<div class="paragraph">
<p>Deploy the PostgreSQL cluster by running the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/centralized-logging/postgres-cluster
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Next, verify the pods are running:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} get pods --selector=k8s-app=postgres-cluster</pre>
</div>
</div>
<div class="paragraph">
<p>With the PostgreSQL successfully deployed, we can now query the logs in Kibana.</p>
</div>
<div class="paragraph">
<p>We will need to setup a port-forward to the Kibana pod to access it.  To do that
we first get the name of the pod by running the following command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} get pod --selector=k8s-app=kibana-logging -n kube-system</pre>
</div>
</div>
<div class="paragraph">
<p>Next, start the port-forward:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} port-forward &lt;KIBANA POD NAME&gt; 5601:5601 -n kube-system</pre>
</div>
</div>
<div class="paragraph">
<p>To access the web dashboard navigate in a browser to <code>127.0.0.1:5601</code>.</p>
</div>
<div class="paragraph">
<p>First, click the <code>Discover</code> tab and setup an index pattern to use for queries.</p>
</div>
<div class="paragraph">
<p>The index pattern name we will use is <code>logstash-*</code> because Fluentd is configured to
generate logstash style logs.</p>
</div>
<div class="paragraph">
<p>Next we will configure the <code>Time Filter field name</code> to be <code>@timestamp</code>.</p>
</div>
<div class="paragraph">
<p>Now that our index pattern is created, we can query for the container logs.</p>
</div>
<div class="paragraph">
<p>Click the <code>Discover</code> tab and use the following queries:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># KUBERNETES
CONTAINER_NAME: *primary* AND MESSAGE: ".*LOG*"
# OpenShift
kubernetes.pod_name: "primary" AND log</pre>
</div>
</div>
<div class="paragraph">
<p>For more information about querying Kibana, see the official documentation: <a href="https://www.elastic.co/guide/en/beats/packetbeat/current/kibana-queries-filters.html" class="bare">https://www.elastic.co/guide/en/beats/packetbeat/current/kibana-queries-filters.html</a></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_backup_and_restoration">Backup and Restoration</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_pg_dump">pg_dump</h3>
<div class="paragraph">
<p>The script assumes you are going to backup the <strong>primary</strong> example and that container
is running.</p>
</div>
<div class="paragraph">
<p>This example assumes you have configured a storage filesystem as described
in the <a href="https://crunchydata.github.io/crunchy-containers/latest/installation/storage-configuration/">Storage Configuration</a> document.</p>
</div>
<div class="paragraph">
<p>A successful backup will perform pg_dump/pg_dumpall on the primary and store
the resulting files in the mounted volume under a directory named <code>&lt;HOSTNAME&gt;-backups</code>
as a sub-directory, then followed by a unique backup directory based upon a
date and timestamp - allowing any number of backups to be kept.</p>
</div>
<div class="paragraph">
<p>For more information on how to configure this container, please see the <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/">Container Specifications</a> document.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_4">Docker</h4>
<div class="paragraph">
<p>Run the backup with this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgdump
./run.sh</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_6">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgdump
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The Kubernetes Job type executes a pod and then the pod exits.  You can
view the Job status using this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} get job</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>pgdump.json</code> file within that directory specifies options that control the behavior of the pgdump job.
Examples of this include whether to run pg_dump vs pg_dumpall and advanced options for specific backup use cases.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pg_restore">pg_restore</h3>
<div class="paragraph">
<p>The script assumes you are going to restore to the <strong>primary</strong> example and that container
is running and a backup has been created using the <code>pgdump</code> example..</p>
</div>
<div class="paragraph">
<p>This example assumes you have configured a storage filesystem as described
in the <a href="https://crunchydata.github.io/crunchy-containers/latest/installation/storage-configuration/">Storage Configuration</a> document.</p>
</div>
<div class="paragraph">
<p>Successful use of the <code>crunchy-pgrestore</code> container will run a job to restore files generated by
pg_dump/pg_dumpall to a container via psql/pg_restore; then container will terminate successfully
and signal job completion.</p>
</div>
<div class="paragraph">
<p>For more information on how to configure this container, please see the <a href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/">Container Specifications</a> document.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_5">Docker</h4>
<div class="paragraph">
<p>Run the restore with this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgrestore
./run.sh</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_7">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>By default, pgrestore container will automatically restore from the most recent backup.
If you want to restore to a specific backup, edit the <code>pgrestore.json</code> file and update the
<code>PGRESTORE_BACKUP_TIMESTAMP</code> setting to specify the backup path you want to restore with. For example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"name":"PGRESTORE_BACKUP_TIMESTAMP",
"value":"2018-03-27-14-35-33"</pre>
</div>
</div>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgrestore
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The Kubernetes Job type executes a pod and then the pod exits.  You can
view the Job status using this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} get job</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>pgrestore.json</code> file within that directory specifies options that control the behavior of the pgrestore job.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pgbackrest">pgBackRest</h3>
<div class="paragraph">
<p>pgbackrest is a utility that performs a backup, restore, and archive
function for a PostgreSQL database. pgbackrest is written and
maintained by David Steele, and more information can be found on the
<a href="http://www.pgbackrest.org/">official website</a>.</p>
</div>
<div class="paragraph">
<p>Backups are currently performed by manually executing pgbackrest commands against the desired pod.
Restores can now be performed via the crunchy-backrest-restore container, which offers FULL or
DELTA restore capability.</p>
</div>
<div class="paragraph">
<p>pgbackrest is configured using a <code>pgbackrest.conf</code> file that is
mounted into the crunchy-postgres container at <code>/pgconf</code>.</p>
</div>
<div class="paragraph">
<p>If you place a <code>pgbackrest.conf</code> file within this mounted directory, it
will trigger the use of pgbackrest within the PostgreSQL container
as the <code>archive_command</code> and will turn on the <code>archive_mode</code> to begin
archival.  It is still required to define the <code>ARCHIVE_TIMEOUT</code> environment
variable within your container configuration as it is set to
a disable value of 0 by default.</p>
</div>
<div class="paragraph">
<p>The following changes will be made to the container&#8217;s <code>postgresql.conf</code>
file:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ARCHIVE_MODE=on
ARCHIVE_TIMEOUT=60
ARCHIVE_COMMAND='pgbackrest --stanza=db archive-push %p'</pre>
</div>
</div>
<div class="paragraph">
<p>If you are using a crunchy-postgres image older than 1.7.1, <code>archive_command</code> must specify where
the <code>pgbackrest.conf</code> file is located:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ARCHIVE_COMMAND='pgbackrest --config=/pgconf/pgbackrest.conf --stanza=db archive-push %p'</pre>
</div>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>This requires you use a pgbackrest stanza name of <strong>db</strong> within the
<code>pgbackrest.conf</code> file you mount.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>When set, WAL files generated by the database will be written
out to the <code>/backrestrepo/HOSTNAME-backups</code> mount point.</p>
</div>
<div class="paragraph">
<p>Additionally, the Crunchy Postgres container can templatize <code>pgbackrest.conf</code> files
by searching for the HOSTNAME values in a mounted <code>pgbackrest.conf</code> file.</p>
</div>
<div class="paragraph">
<p>For example, <code>db-path=/pgdata/HOSTNAME</code> will render to <code>db-path=/pgdata/primary</code> if
the container&#8217;s hostname is primary.  HOSTNAME will be replaced with the value of
<code>PGDATA_PATH_OVERRIDE</code> when working with deployments/replicasets.</p>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>The templating example above works for <code>db-path</code> settings, however, <code>repo-path</code> should
follow the convention <code>repo-path=/backrestrepo/HOSTNAME-backups</code> in cases where
volumes are being mounted to a single mount point (such as hostPath or NFS).  Without
the additional <code>-backups</code> the backups will populate in the <code>pgdata</code> directory.</p>
</div>
</div>

</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_8">Kubernetes and OpenShift</h4>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>The BackRest examples on Kubernetes/OpenShift can be configured to use the PostGIS images
by setting the following environment variable: <code>export CCP_PG_IMAGE='-gis'</code></p>
</div>
</div>

</div>
<div class="sect4">
<h5 id="_backup">Backup</h5>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/backup
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Kubernetes environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>configMap containing <code>pgbackrest.conf</code></p>
</li>
<li>
<p>PostgreSQL pod with pgBackRest configured</p>
</li>
<li>
<p>PostgreSQL service</p>
</li>
<li>
<p>PVC for the PGDATA directory</p>
</li>
<li>
<p>PVC for the BackRest Backups and Archives directory</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Examine the <code>/backrestrepo</code> location to view the archive directory and ensure WAL archiving is working.</p>
</div>
<div class="paragraph">
<p>You can create a backup using backrest using this command within the container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} exec -it backrest /bin/bash
pgbackrest --stanza=db backup --type=full</pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_async_archiving">Async Archiving</h5>
<div class="paragraph">
<p>pgBackRest supports asyncronous archiving to pull and push Write Ahead Logs.
Asynchronous operation is more efficient because it can reuse connections and take
advantage of parallelism.  For more information on async archiving, see the pgBackRest
<a href="https://pgbackrest.org/user-guide.html#async-archiving">official documentation</a>.</p>
</div>
<div class="paragraph">
<p>This will create the following in your Kubernetes environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>configMap containing <code>pgbackrest.conf</code></p>
</li>
<li>
<p>PostgreSQL pod with pgBackRest configured and archiving asynchronously.</p>
</li>
<li>
<p>PostgreSQL service</p>
</li>
<li>
<p>PVC for the PGDATA directory</p>
</li>
<li>
<p>PVC for the BackRest Backups and Archives directory</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/async-archiving
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Examine the <code>/backrestrepo/HOSTNAME-backups</code> location to view the archive directory
and ensure WAL archiving is working.</p>
</div>
<div class="paragraph">
<p>Examine the <code>/pgdata/HOSTNAME-spool</code> location to view the transient directory
used for async archiving.</p>
</div>
<div class="paragraph">
<p>You can create a backup using backrest using this command within the container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} exec -it backrest-async-archive /bin/bash
pgbackrest --stanza=db backup</pre>
</div>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>A spooling directory is automatically created in both <code>/pgdata</code> and <code>/pgwal</code>.  It is
advised to configure pgBackRest to use the spooling location closest to the Write Ahead Log.</p>
</div>
<div class="paragraph">
<p>If the PostgreSQL container was created using the <code>XLOGDIR</code> variable, the <code>/pgwal/HOSTNAME-spool</code>
directory should be configured in <code>pgbackrest.conf</code> as such: <code>spool-path=/pgwal/HOSTNAME-spool</code>.
If WAL resides on PGDATA, use: <code>spool-path=/pgdata/HOSTNAME-spool</code></p>
</div>
</div>

</div>
</div>
</div>
<div class="sect3">
<h4 id="_restore">Restore</h4>
<div class="paragraph">
<p>There are three options to choose from when performing a restore:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Delta - only restore missing files from PGDATA</p>
</li>
<li>
<p>Full - restore all files, pgdata must be empty</p>
</li>
<li>
<p>Point in Time Recovery (PITR) - delta restore to a certain point in time</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="_pitr">PITR</h5>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>This example uses the <code>backrest/backup</code> example.  It should be left running and a
pgBackRest backup has been created.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/pitr
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Kubernetes environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>configMap containing <code>pgbackrest.conf</code></p>
</li>
<li>
<p>Backrest-Restore pod with pgBackRest configured for PITR restore</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>pgBackRest will restore the <code>pgdata</code> volume mounted to the restore container
to the point in time specified by the <code>PITR_TARGET</code> environment variable.  To get
a compliant timestamp, PostgreSQL can be queried using the following SQL:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -U postgres -Atc 'select current_timestamp'</pre>
</div>
</div>
<div class="paragraph">
<p>After a successful restore, run the following to start the restored PostgreSQL container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/pitr
./post-restore.sh</pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_full">Full</h5>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>This example uses the <code>backrest/backup</code> example.  It does not need to be running but a
pgBackRest backup is required.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/full
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Kubernetes environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>configMap containing <code>pgbackrest.conf</code></p>
</li>
<li>
<p>Backrest-Restore pod with pgBackRest configured for full restore</p>
</li>
<li>
<p>New PVC for the PGDATA directory (full restores require PGDATA to be empty)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>pgBackRest will restore all files to the <code>pgdata</code> volume mounted to the restore container.</p>
</div>
<div class="paragraph">
<p>After a successful restore, run the following to start the restored PostgreSQL container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/full
./post-restore.sh</pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_delta">Delta</h5>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>This example uses the <code>backrest/backup</code> example.  It does not need to be running but a
pgBackRest backup is required.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/delta
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Kubernetes environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>configMap containing <code>pgbackrest.conf</code></p>
</li>
<li>
<p>Backrest-Restore pod with pgBackRest configured for full restore</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>pgBackRest will restore files missing to the <code>pgdata</code> volume mounted to the restore container.</p>
</div>
<div class="paragraph">
<p>After a successful restore, run the following to start the restored PostgreSQL container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/delta
./post-restore.sh</pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_docker_6">Docker</h4>
<div class="sect4">
<h5 id="_backup_2">Backup</h5>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/backrest/backup
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Docker environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>PostgreSQL container with pgBackRest configured</p>
</li>
<li>
<p>Volume for the PGDATA directory</p>
</li>
<li>
<p>Volume for the <code>pgbackrest.conf</code> configuration</p>
</li>
<li>
<p>Volume for the BackRest Backups and Archives directory</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Examine the <code>/backrestrepo</code> location to view the archive directory and ensure WAL archiving is working.</p>
</div>
<div class="paragraph">
<p>You can create a backup using backrest using this command within the container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker exec -it backrest /bin/bash
pgbackrest --stanza=db backup --type=full</pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_async_archiving_2">Async Archiving</h5>
<div class="paragraph">
<p>This will create the following in your Docker environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>PostgreSQL container with pgBackRest configured</p>
</li>
<li>
<p>Volume for the PGDATA directory</p>
</li>
<li>
<p>Volume for the <code>pgbackrest.conf</code> configuration</p>
</li>
<li>
<p>Volume for the BackRest Backups and Archives directory</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/backrest/async-archiving
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Examine the <code>/backrestrepo/HOSTNAME-backups</code> location to view the archive directory
and ensure WAL archiving is working.</p>
</div>
<div class="paragraph">
<p>Examine the <code>/pgdata/HOSTNAME-spool</code> location to view the transient directory
used for async archiving.</p>
</div>
<div class="paragraph">
<p>You can create a backup using backrest using this command within the container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker exec -it backrest /bin/bash
pgbackrest --stanza=db backup</pre>
</div>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>A spooling directory is automatically created in both <code>/pgdata</code> and <code>/pgwal</code>.  It is
advised to configure pgBackRest to use the spooling location closest to the Write Ahead Log.</p>
</div>
<div class="paragraph">
<p>If the PostgreSQL container was created using the <code>XLOGDIR</code> variable, the <code>/pgwal/HOSTNAME-spool</code>
directory should be configured in <code>pgbackrest.conf</code> as such: <code>spool-path=/pgwal/HOSTNAME-spool</code>.
If WAL resides on PGDATA, use: <code>spool-path=/pgdata/HOSTNAME-spool</code></p>
</div>
</div>

</div>
</div>
</div>
<div class="sect3">
<h4 id="_restore_2">Restore</h4>
<div class="sect4">
<h5 id="_pitr_2">PITR</h5>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>This example uses the <code>backrest/backup</code> example.  It should be left running and a
pgBackRest backup has been created.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/backrest/pitr
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Docker environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Backrest-Restore container with pgBackRest configured for PITR restore</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>pgBackRest will restore the <code>pgdata</code> volume mounted to the restore container
to the point in time specified by the <code>PITR_TARGET</code> environment variable.  To get
a compliant timestamp, PostgreSQL can be queried using the following SQL:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -U postgres -Atc 'select current_timestamp'</pre>
</div>
</div>
<div class="paragraph">
<p>After a successful restore, run the following to start the restored PostgreSQL container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/backrest/pitr
./post-restore.sh</pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_full_2">Full</h5>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>This example uses the <code>backrest/backup</code> example.  It does not need to be running but a
pgBackRest backup is required.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/backrest/full
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Docker environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Backrest-Restore pod with pgBackRest configured for full restore</p>
</li>
<li>
<p>New Volume for the PGDATA directory (full restores require PGDATA to be empty)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>pgBackRest will restore all files to the <code>pgdata</code> volume mounted to the restore container.</p>
</div>
<div class="paragraph">
<p>After a successful restore, run the following to start the restored PostgreSQL container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/backrest/full
./post-restore.sh</pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_delta_2">Delta</h5>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>This example uses the <code>backrest/backup</code> example.  It does not need to be running but a
pgBackRest backup is required.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/delta
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create the following in your Docker environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Backrest-Restore pod with pgBackRest configured for full restore</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>pgBackRest will restore files missing to the <code>pgdata</code> volume mounted to the restore container.</p>
</div>
<div class="paragraph">
<p>After a successful restore, run the following to start the restored PostgreSQL container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backrest/delta
./post-restore.sh</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pg_basebackup">pg_basebackup</h3>
<div class="paragraph">
<p>The script assumes you are going to backup the <strong>primary</strong>
container created in the first example, so you need to ensure
that container is running. This example assumes you have configured storage as described
in the <a href="https://crunchydata.github.io/crunchy-containers/latest/installation/storage-configuration/">Storage Configuration documentation</a>. Things to point out with this example
include its use of persistent volumes and volume claims to store the backup data files.</p>
</div>
<div class="paragraph">
<p>A successful backup will perform <code>pg_basebackup</code> on the <strong>primary</strong> container and store
the backup in the <code>$CCP_STORAGE_PATH</code> volume under a directory named <code>$CCP_NAMESPACE-primary-backups</code>. Each
backup will be stored in a subdirectory with a timestamp as the name, allowing any number of backups to be kept.</p>
</div>
<div class="paragraph">
<p>The backup script will do the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Start up a backup container named backup</p>
</li>
<li>
<p>Run <code>pg_basebackup</code> on the container named <strong>primary</strong></p>
</li>
<li>
<p>Store the backup in the <code>/tmp/backups/primary-backups</code> directory</p>
</li>
<li>
<p>Exit after the backup</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When you are ready to restore from the backup, the restore example runs a PostgreSQL container
using the backup location. Upon initialization, the container will use rsync to copy the backup
data to this new container and then launch PostgreSQL using the original backed-up data.</p>
</div>
<div class="paragraph">
<p>The restore script will do the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Start up a container named <strong>restore</strong></p>
</li>
<li>
<p>Copy the backup files from the previous backup example into <code>/pgdata</code></p>
</li>
<li>
<p>Start up the container using the backup files</p>
</li>
<li>
<p>Map the PostgreSQL port of 5432 in the container to your local host port of 12001</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_7">Docker</h4>
<div class="paragraph">
<p>Run the backup with this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/backup
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>When you&#8217;re ready to restore, a <strong>restore</strong> example is provided.</p>
</div>
<div class="paragraph">
<p>It&#8217;s required to specified a backup path for this example.  To get the correct path
check the <code>backup</code> job logs or a timestamp:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker logs backup-vpk9l | grep BACKUP_PATH
Wed May  9 20:32:00 UTC 2018 INFO: BACKUP_PATH is set to /pgdata/primary-backups/2018-05-09-20-32-00.</pre>
</div>
</div>
<div class="paragraph">
<p>BACKUP_PATH can also be discovered by looking at the backup mount directly (if access
to the storage is available to the user).</p>
</div>
<div class="paragraph">
<p>An example of BACKUP_PATH is as followed:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"name": "BACKUP_PATH",
"value": "primary-backups/2018-05-09-20-32-00"</pre>
</div>
</div>
<div class="paragraph">
<p>When you are ready to restore from the backup created, run the following example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/restore
./run.sh</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_9">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/backup
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The Kubernetes Job type executes a pod and then the pod exits.  You can
view the Job status using this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} get job</pre>
</div>
</div>
<div class="paragraph">
<p>When you&#8217;re ready to restore, a <strong>restore</strong> example is provided.</p>
</div>
<div class="paragraph">
<p>It&#8217;s required to specified a backup path for this example.  To get the correct path
check the <code>backup</code> job logs or a timestamp:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>kubectl logs backup-vpk9l | grep BACKUP_PATH
Wed May  9 20:32:00 UTC 2018 INFO: BACKUP_PATH is set to /pgdata/primary-backups/2018-05-09-20-32-00.</pre>
</div>
</div>
<div class="paragraph">
<p>BACKUP_PATH can also be discovered by looking at the backup mount directly (if access
to the storage is available to the user).</p>
</div>
<div class="paragraph">
<p>An example of BACKUP_PATH defined as a variable within the JSON script is as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>"name": "BACKUP_PATH",
"value": "primary-backups/2018-05-09-20-32-00"</pre>
</div>
</div>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/restore
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Test the restored database as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h restore -U postgres postgres</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_point_in_time_recovery_pitr">Point in Time Recovery (PITR)</h3>
<div class="paragraph">
<p>PITR (point-in-time-recovery) is a feature that allows for recreating a database
from backup and log files at a certain point in time. This is done using a write
ahead log (WAL) which is kept in the <code>pg_wal</code> directory within <code>PGDATA</code>. Changes
made to the database files over time are recorded in these log files, which allows
it to be used for disaster recovery purposes.</p>
</div>
<div class="paragraph">
<p>When using PITR as a backup method, in order to restore from the last checkpoint in
the event of a database or system failure, it is only necessary to save these log
files plus a full backup. This provides an additional advantage in that it is not
necessary to keep multiple full backups on hand, which consume space and time to create.
This is because point in time recovery allows you to "replay" the log files and recover
your database to any point since the last full backup.</p>
</div>
<div class="paragraph">
<p>More detailed information about Write Ahead Log (WAL) archiving can be found
<a href="https://www.postgresql.org/docs/10/static/continuous-archiving.html">here.</a></p>
</div>
<div class="paragraph">
<p>By default in the crunchy-postgres container, WAL logging is <strong>not</strong> enabled.
To enable WAL logging <strong>outside of this example</strong>, set the following environment
variables when starting the crunchy-postgres container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ARCHIVE_MODE=on
ARCHIVE_TIMEOUT=60</pre>
</div>
</div>
<div class="paragraph">
<p>These variables set the same name settings within the <code>postgresql.conf</code>
file that is used by the database. When set, WAL files generated by the database
will be written out to the <code>/pgwal</code> mount point.</p>
</div>
<div class="paragraph">
<p>A full backup is required to do a PITR.  crunchy-backup currently
performs this role within the example, running a <code>pg_basebackup</code> on the database.
This is a requirement for PITR. After a backup is performed, code is added into
crunchy-postgres which will also check to see if you want to do a PITR.</p>
</div>
<div class="paragraph">
<p>There are three volume mounts used with the PITR example.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>/recover</code> - When specified within a crunchy-postgres container, PITR is activated during container startup.</p>
</li>
<li>
<p><code>/backup</code> - This is used to find the base backup you want to recover from.</p>
</li>
<li>
<p><code>/pgwal</code> - This volume is used to write out new WAL files from the newly restored database container.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Some environment variables used to manipulate the point in time recovery logic:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>RECOVERY_TARGET_NAME</code> environment variable is used to tell the PITR logic what the name of the target is.</p>
</li>
<li>
<p><code>RECOVERY_TARGET_TIME</code> is also an optional environment variable that restores using a known time stamp.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you don&#8217;t specify either of these environment variables, then the PITR logic will assume you want to
restore using all the WAL files or essentially the last known recovery point.</p>
</div>
<div class="paragraph">
<p>The <code>RECOVERY_TARGET_INCLUSIVE</code> environment variable is also available to
let you control the setting of the <code>recovery.conf</code> setting <code>recovery_target_inclusive</code>.
If you do not set this environment variable the default is <strong>true</strong>.</p>
</div>
<div class="paragraph">
<p>Once you recover a database using PITR, it will be in read-only mode. To
make the database resume as a writable database, run the following SQL command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>postgres=# select pg_wal_replay_resume();</pre>
</div>
</div>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>If you&#8217;re running the PITR example for <strong>PostgreSQL versions 9.5 or 9.6</strong>, please note that
starting in PostgreSQL version 10, the <code>pg_xlog</code> directory was renamed to <code>pg_wal</code>. Additionally, all usages
of the function <code>pg_xlog_replay_resume</code> were changed to <code>pg_wal_replay_resume</code>.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>It takes about 1 minute for the database to become ready for use after initially starting.</p>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>WAL segment files are written to the <strong>/tmp</strong> directory. Leaving the example running
for a long time could fill up your /tmp directory.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_8">Docker</h4>
<div class="paragraph">
<p>Create a database container as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pitr
./run-pitr.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Next, we will create a base backup of that database using this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./run-backup-pitr.sh</pre>
</div>
</div>
<div class="paragraph">
<p>After creating the base backup of the database, WAL segment files are created every 60 seconds
that contain any database changes. These segments are stored in the
<code>/tmp/pitr/pitr/pg_wal</code> directory.</p>
</div>
<div class="paragraph">
<p>Next, create some recovery targets within the database by running
the SQL commands against the <strong>pitr</strong> database as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./run-sql.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create recovery targets named <code>beforechanges</code>, <code>afterchanges</code>, and
<code>nomorechanges</code>.  It will create a table, <strong>pitrtest</strong>, between
the <code>beforechanges</code> and <code>afterchanges</code> targets.  It will also run a SQL
<code>CHECKPOINT</code> to flush out the changes to WAL segments. These labels can be
used to mark the points in the recovery process that will be referenced when
creating the restored database.</p>
</div>
<div class="paragraph">
<p>Next, now that we have a base backup and a set of WAL files containing
our database changes, we can shut down the <strong>pitr</strong> database
to simulate a database failure.  Do this by running the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker stop pitr</pre>
</div>
</div>
<div class="paragraph">
<p>Next, let&#8217;s edit the restore script to use the base backup files
created in the step above.  You can view the backup path name
under the <code>/tmp/backups/pitr-backups/</code> directory. You will see
another directory inside of this path with a name similar to
<code>2018-03-21-21-03-29</code>.  Copy and paste that value into the
<code>run-restore-pitr.sh</code> script in the <code>BACKUP</code> environment variable.</p>
</div>
<div class="paragraph">
<p>After that, run the script.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>vi ./run-restore-pitr.sh
./run-restore-pitr.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The WAL segments are read and applied when restoring from the database
backup.  At this point, you should be able to verify that the
database was restored to the point before creating the test table:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'</pre>
</div>
</div>
<div class="paragraph">
<p>This SQL command should show that the pitrtest table does not exist
at this recovery time. The output should be similar to:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ERROR: relation "pitrtest" does not exist</pre>
</div>
</div>
<div class="paragraph">
<p>PostgreSQL allows you to pause the recovery process if the target name
or time is specified.  This pause would allow a DBA a chance to review
the recovery time/name and see if this is what they want or expect.  If so,
the DBA can run the following command to resume and complete the recovery:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_wal_replay_resume()'</pre>
</div>
</div>
<div class="paragraph">
<p>Until you run the statement above, the database will be left in read-only
mode.</p>
</div>
<div class="paragraph">
<p>Next, run the script to restore the database
to the <code>afterchanges</code> restore point. Update the <code>RECOVERY_TARGET_NAME</code> to <code>afterchanges</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>vi ./run-restore-pitr.sh
./run-restore-pitr.sh</pre>
</div>
</div>
<div class="paragraph">
<p>After this restore, you should be able to see the test table:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_wal_replay_resume()'</pre>
</div>
</div>
<div class="paragraph">
<p>Lastly, start a recovery using all of the WAL files. This will get the
restored database as current as possible. To do so, edit the script
to remove the <code>RECOVERY_TARGET_NAME</code> environment setting completely:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./run-restore-pitr.sh
sleep 30
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'create table foo (id int)'</pre>
</div>
</div>
<div class="paragraph">
<p>At this point, you should be able to create new data in the restored database
and the test table should be present.  When you recover the entire
WAL history, resuming the recovery is not necessary to enable writes.</p>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_10">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Start by running the example database container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pitr
./run-pitr.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This step will create a database container, <strong>pitr</strong>.  This
container is configured to continuously write WAL segment files
to a mounted volume (<code>/pgwal</code>).</p>
</div>
<div class="paragraph">
<p>After you start the database, you will create a base backup
using this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./run-backup-pitr.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create a backup and write the backup files to a persistent
volume (<code>/pgbackup</code>).</p>
</div>
<div class="paragraph">
<p>Next, create some recovery targets within the database by running
the SQL commands against the <strong>pitr</strong> database as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./run-sql.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This will create recovery targets named <code>beforechanges</code>, <code>afterchanges</code>, and
<code>nomorechanges</code>.  It will create a table, <strong>pitrtest</strong>, between
the <code>beforechanges</code> and <code>afterchanges</code> targets.  It will also run a SQL
<code>CHECKPOINT</code> to flush out the changes to WAL segments.</p>
</div>
<div class="paragraph">
<p>Next, now that we have a base backup and a set of WAL files containing
our database changes, we can shut down the <strong>pitr</strong> database
to simulate a database failure.  Do this by running the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} delete pod pitr</pre>
</div>
</div>
<div class="paragraph">
<p>Next, we will create 3 different restored database containers based
upon the base backup and the saved WAL files.</p>
</div>
<div class="paragraph">
<p>First, get the BACKUP_PATH created by the <code>backup-pitr</code> example by viewing the pods logs:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} logs backup-pitr-8sfkh | grep PATH
Thu May 10 18:07:58 UTC 2018 INFO: BACKUP_PATH is set to /pgdata/pitr-backups/2018-05-10-18-07-58.</pre>
</div>
</div>
<div class="paragraph">
<p>Edit the <code>restore-pitr.json</code> file and change the <code>BACKUP_PATH</code> environment variable
using the path discovered above (note: <code>/pgdata/</code> is not required and should be excluded
in the variable):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>{
    "name": "BACKUP_PATH",
    "value": "pitr-backups/2018-05-10-18-07-58"
{</pre>
</div>
</div>
<div class="paragraph">
<p>Next, we restore prior to the <code>beforechanges</code> recovery target.  This
recovery point is <strong>before</strong> the <strong>pitrtest</strong> table is created.</p>
</div>
<div class="paragraph">
<p>Edit the <code>restore-pitr.json</code> file, and edit the environment
variable to indicate we want to use the <code>beforechanges</code> recovery
point:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>{
    "name": "RECOVERY_TARGET_NAME",
    "value": "beforechanges"
{</pre>
</div>
</div>
<div class="paragraph">
<p>Then run the following to create the restored database container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./run-restore-pitr.sh</pre>
</div>
</div>
<div class="paragraph">
<p>After the database has restored, you should be able to perform
a test to see if the recovery worked as expected:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h restore-pitr -U postgres postgres -c 'table pitrtest'
psql -h restore-pitr -U postgres postgres -c 'create table foo (id int)'</pre>
</div>
</div>
<div class="paragraph">
<p>The output of these commands should show that the <strong>pitrtest</strong> table is not
present.  It should also show that you can not create a new table
because the database is paused in read-only mode.</p>
</div>
<div class="paragraph">
<p>To make the database resume as a writable database, run the following
SQL command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>select pg_wal_replay_resume();</pre>
</div>
</div>
<div class="paragraph">
<p>It should then be possible to write to the database:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h restore-pitr -U postgres postgres -c 'create table foo (id int)'</pre>
</div>
</div>
<div class="paragraph">
<p>You can also test that if <code>afterchanges</code> is specified, that the
<strong>pitrtest</strong> table is present but that the database is still in recovery
mode.</p>
</div>
<div class="paragraph">
<p>Lastly, you can test a full recovery using <strong>all</strong> of the WAL files, if
you remove the <code>RECOVERY_TARGET_NAME</code> environment variable completely.</p>
</div>
<div class="paragraph">
<p>The storage portions of this example can all be found under <code>$CCP_STORAGE_PATH/$CCP_NAMESPACE-restore-pitr</code>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_connection_pooling">Connection Pooling</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_pgbouncer">pgBouncer</h3>
<div class="paragraph">
<p>Crunchy pgBouncer is a lightweight connection pooler for PostgreSQL databases.</p>
</div>
<div class="paragraph">
<p>The following examples create the following containers:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>pgBouncer Primary</p>
</li>
<li>
<p>pgBouncer Replica</p>
</li>
<li>
<p>PostgreSQL Primary</p>
</li>
<li>
<p>PostgreSQL Replica</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In Kubernetes and OpenShift, this example will also create:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>pgBouncer Primary Service</p>
</li>
<li>
<p>pgBouncer Replica Service</p>
</li>
<li>
<p>Primary Service</p>
</li>
<li>
<p>Replica Service</p>
</li>
<li>
<p>PostgreSQL Secrets</p>
</li>
<li>
<p>pgBouncer Secrets</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To cleanup the objects created by this example, run the following in the <code>pgbouncer</code> example directory:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>For more information on <code>pgBouncer</code>, see the <a href="https://pgbouncer.github.io">official website</a>.</p>
</div>
</div>

</div>
<div class="sect3">
<h4 id="_docker_9">Docker</h4>
<div class="paragraph">
<p>Run the <code>pgbouncer</code> example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgbouncer
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Once all containers have deployed and are ready for use, <code>psql</code> to the target
databases through <code>pgBouncer</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -d userdb -h 0.0.0.0 -p 6432 -U testuser
psql -d userdb -h 0.0.0.0 -p 6433 -U testuser</pre>
</div>
</div>
<div class="paragraph">
<p>To connect to the administration database within <code>pgbouncer</code>, connect using <code>psql</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -d pgbouncer -h 0.0.0.0 -p 6432 -U pgbouncer
psql -d pgbouncer -h 0.0.0.0 -p 6433 -U pgbouncer</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_11">Kubernetes and OpenShift</h4>
<div class="paragraph">
<div class="notices tip" ><div class="paragraph">
<p>OpenShift: If custom configurations aren&#8217;t being mounted, an <strong>emptydir</strong> volume is required
to be mounted at <code>/pgconf</code>.</p>
</div>
</div>

</div>
<div class="paragraph">
<p>Run the <code>pgbouncer</code> example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgbouncer
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Once all containers have deployed and are ready for use, <code>psql</code> to the target
databases through <code>pgBouncer</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -d userdb -h pgbouncer-primary -p 6432 -U testuser
psql -d userdb -h pgbouncer-replica -p 6432 -U testuser</pre>
</div>
</div>
<div class="paragraph">
<p>To connect to the administration database within <code>pgbouncer</code>, connect using <code>psql</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -d pgbouncer -h pgbouncer-primary -p 6432 -U pgbouncer -c "SHOW SERVERS"
psql -d pgbouncer -h pgbouncer-replica -p 6432 -U pgbouncer -c "SHOW SERVERS"</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pgpool_ii">pgPool II</h3>
<div class="paragraph">
<p>An example is provided that will run a <strong>pgPool II</strong> container in conjunction with the
<strong>primary-replica</strong> example provided above.</p>
</div>
<div class="paragraph">
<p>You can execute both <code>INSERT</code> and <code>SELECT</code> statements after connecting to pgpool.
The container will direct <code>INSERT</code> statements to the primary and <code>SELECT</code> statements
will be sent round-robin to both the primary and replica.</p>
</div>
<div class="paragraph">
<p>The container creates a default database called <strong>userdb</strong>, a default user called
<strong>testuser</strong> and a default password of <strong>password</strong>.</p>
</div>
<div class="paragraph">
<p>You can view the nodes that pgpool is configured for by running:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h pgpool -U testuser userdb -c 'show pool_nodes'</pre>
</div>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_10">Docker</h4>
<div class="paragraph">
<p>Create the container as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgpool
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The example is configured to allow the <strong>testuser</strong> to connect
to the <strong>userdb</strong> database.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h localhost -U testuser -p 12003 userdb</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_12">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Run the following command to deploy the pgpool service:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgpool
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>The example is configured to allow the <strong>testuser</strong> to connect
to the <strong>userdb</strong> database.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h pgpool -U testuser userdb</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_database">Database</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_single_primary">Single Primary</h3>
<div class="paragraph">
<p>This example starts a single PostgreSQL container and service, the most simple
of examples.</p>
</div>
<div class="paragraph">
<p>The container creates a default database called <strong>userdb</strong>, a default user called <strong>testuser</strong>
and a default password of <strong>password</strong>.</p>
</div>
<div class="paragraph">
<p>For all environments, the script additionally creates:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A persistent volume claim</p>
</li>
<li>
<p>A crunchy-postgres container named <strong>primary</strong></p>
</li>
<li>
<p>The database using predefined environment variables</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>And specifically for the Kubernetes and OpenShift environments:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A pod named <strong>primary</strong></p>
</li>
<li>
<p>A service named <strong>primary</strong></p>
</li>
<li>
<p>A PVC named <strong>primary-pgdata</strong></p>
</li>
<li>
<p>The database using predefined environment variables</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_11">Docker</h4>
<div class="paragraph">
<p>To create the example and run the container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/primary
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Connect from your local host as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h localhost -U testuser -W userdb</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_13">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>To create the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/primary
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Connect from your local host as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h primary -U postgres postgres</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_helm">Helm</h4>
<div class="paragraph">
<p>This example resides under the <code>$CCPROOT/examples/helm</code> directory. View the README to run this
example using Helm <a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/helm/primary/README.md">here</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_postgresql_deployment">PostgreSQL Deployment</h3>
<div class="paragraph">
<p>Starting in release 1.2.8, the PostgreSQL container can accept
an environment variable named <code>PGDATA_PATH_OVERRIDE</code>.  If set,
the <code>/pgdata/subdir</code> path will use a subdirectory name of your
choosing instead of the default which is the hostname of the container.</p>
</div>
<div class="paragraph">
<p>This example shows how a Deployment of a PostgreSQL primary is
supported. A pod is a deployment that uses a hostname generated by
Kubernetes; because of this, a new hostname will be defined upon
restart of the primary pod.</p>
</div>
<div class="paragraph">
<p>For finding the <code>/pgdata</code> that pertains to the pod, you will need
to specify a <code>/pgdata/subdir</code> name that never changes. This requirement is
handled by the <code>PGDATA_PATH_OVERRIDE</code> environment variable.</p>
</div>
<div class="paragraph">
<p>The container creates a default database called <strong>userdb</strong>, a default user called
<strong>testuser</strong> and a default password of <strong>password</strong>.</p>
</div>
<div class="paragraph">
<p>This example will create the following in your Kubernetes and OpenShift environments:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>primary and replica services</p>
</li>
<li>
<p>primary-deployment deployment</p>
</li>
<li>
<p>replica-deployment statefulset</p>
</li>
<li>
<p>ConfigMap to hold a custom <code>postgresql.conf</code>, <code>setup.sql</code>, and
<code>pg_hba.conf</code> files</p>
</li>
<li>
<p>Secrets for the primary user, superuser, and normal user to
hold the passwords</p>
</li>
<li>
<p>Volume mount for <code>/backrestrepo</code> and <code>/pgwal</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The persisted data for the PostgreSQL primary is found under <code>/pgdata/primary-deployment</code>.
If you delete the primary pod, the deployment will create another
pod for the primary and will be able to start up immediately since
it works out of the same <code>/pgdata/primary-deployment</code> data directory.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_14">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Start the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/primary-deployment
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>To scale the replica statefulset, run the following command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} scale --replicas=2 statefulset replica-deployment</pre>
</div>
</div>
<div class="paragraph">
<div class="notices warning" ><div class="paragraph">
<p>This example only creates enough Persistent Volumes for a maximum of 2 replicas.
If you are not using storage classes, the maximum amount of replicas this example can
be scaled to is 2.</p>
</div>
</div>

</div>
</div>
</div>
<div class="sect2">
<h3 id="_replication">Replication</h3>
<div class="paragraph">
<p>This example starts a primary and a replica pod containing a PostgreSQL database.</p>
</div>
<div class="paragraph">
<p>The container creates a default database called <strong>userdb</strong>, a default user called
<strong>testuser</strong> and a default password of <strong>password</strong>.</p>
</div>
<div class="paragraph">
<p>For the Docker environment, the script additionally creates:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A docker volume using the local driver for the primary</p>
</li>
<li>
<p>A docker volume using the local driver for the replica</p>
</li>
<li>
<p>A container named <strong>primary</strong> binding to port 12007</p>
</li>
<li>
<p>A container named <strong>replica</strong> binding to port 12008</p>
</li>
<li>
<p>A mapping of the PostgreSQL port 5432 within the container to the localhost port 12000</p>
</li>
<li>
<p>The database using predefined environment variables</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>And specifically for the Kubernetes and OpenShift environments:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>emptyDir volumes for persistence</p>
</li>
<li>
<p>A pod named <strong>pr-primary</strong></p>
</li>
<li>
<p>A pod named <strong>pr-replica</strong></p>
</li>
<li>
<p>A pod named <strong>pr-replica-2</strong></p>
</li>
<li>
<p>A service named <strong>pr-primary</strong></p>
</li>
<li>
<p>A service named <strong>pr-replica</strong></p>
</li>
<li>
<p>The database using predefined environment variables</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_12">Docker</h4>
<div class="paragraph">
<p>To create the example and run the container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/primary-replica
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Connect from your local host as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h localhost -p 12007 -U testuser -W userdb
psql -h localhost -p 12008 -U testuser -W userdb</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_15">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Run the following command to deploy a primary and replica database cluster:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/primary-replica
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>It takes about a minute for the replica to begin replicating with the
primary.  To test out replication, see if replication is underway
with this command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} exec -ti pr-primary -- psql -d postgres -c 'table pg_stat_replication'</pre>
</div>
</div>
<div class="paragraph">
<p>If you see a line returned from that query it means the primary is replicating
to the replica.  Try creating some data on the primary:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} exec -ti pr-primary -- psql -d postgres -c 'create table foo (id int)'
${CCP_CLI?} exec -ti pr-primary -- psql -d postgres -c 'insert into foo values (1)'</pre>
</div>
</div>
<div class="paragraph">
<p>Then verify that the data is replicated to the replica:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} exec -ti pr-replica -- psql -d postgres -c 'table foo'</pre>
</div>
</div>
<div class="paragraph">
<p><strong>primary-replica-dc</strong></p>
</div>
<div class="paragraph">
<p>If you wanted to experiment with scaling up the number of replicas, you can run the following example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/primary-replica-dc
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>You can verify that replication is working using the same commands as above.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI?} exec -ti primary-dc -- psql -d postgres -c 'table pg_stat_replication'</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_helm_2">Helm</h4>
<div class="paragraph">
<p>This example resides under the <code>$CCPROOT/examples/helm</code> directory. View the README to run this example
using Helm <a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/helm/primary-replica/README.md">here</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_synchronous_replication">Synchronous Replication</h3>
<div class="paragraph">
<p>This example deploys a PostgreSQL cluster with a primary, a synchronous replica, and
an asynchronous replica. The two replicas share the same service.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_13">Docker</h4>
<div class="paragraph">
<p>To run this example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/sync
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>You can test the replication status on the primary by using the following command
and the password <strong>password</strong>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h 127.0.0.1 -p 12010 -U postgres postgres -c 'table pg_stat_replication'</pre>
</div>
</div>
<div class="paragraph">
<p>You should see 2 rows; 1 for the asynchronous replica and 1 for the synchronous replica.  The
<code>sync_state</code> column shows values of async or sync.</p>
</div>
<div class="paragraph">
<p>You can test replication to the replicas by first entering some data on
the primary, and secondly querying the replicas for that data:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h 127.0.0.1 -p 12010 -U postgres postgres -c 'create table foo (id int)'
psql -h 127.0.0.1 -p 12010 -U postgres postgres -c 'insert into foo values (1)'
psql -h 127.0.0.1 -p 12011 -U postgres postgres -c 'table foo'
psql -h 127.0.0.1 -p 12012 -U postgres postgres -c 'table foo'</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_16">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/sync
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Connect to the <strong>primarysync</strong> and <strong>replicasync</strong> databases as follows for both the
Kubernetes and OpenShift environments:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h primarysync -U postgres postgres -c 'create table test (id int)'
psql -h primarysync -U postgres postgres -c 'insert into test values (1)'
psql -h primarysync -U postgres postgres -c 'table pg_stat_replication'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from test'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from test'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from test'</pre>
</div>
</div>
<div class="paragraph">
<p>This set of queries will show you the IP address of the PostgreSQL replica
container. Note the changing IP address due to the round-robin service proxy
being used for both replicas.  The example queries also show that both
replicas are replicating successfully from the primary.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_statefulsets">Statefulsets</h3>
<div class="paragraph">
<p>This example deploys a statefulset named <strong>statefulset</strong>.  The statefulset
is a new feature in Kubernetes as of version 1.5 and in OpenShift Origin as of
version 3.5. Statefulsets have replaced PetSets going forward.</p>
</div>
<div class="paragraph">
<p>Please view <a href="https://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/">this Kubernetes description</a>
to better understand what a Statefulset is and how it works.</p>
</div>
<div class="paragraph">
<p>This example creates 2 PostgreSQL containers to form the set.  At
startup, each container will examine its hostname to determine
if it is the first container within the set of containers.</p>
</div>
<div class="paragraph">
<p>The first container is determined by the hostname suffix assigned
by Kubernetes to the pod.  This is an ordinal value starting with <strong>0</strong>.
If a container sees that it has an ordinal value of <strong>0</strong>, it will
update the container labels to add a new label of:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>name=$PG_PRIMARY_HOST</pre>
</div>
</div>
<div class="paragraph">
<p>In this example, <code>PG_PRIMARY_HOST</code> is specified as <code>statefulset-primary</code>.</p>
</div>
<div class="paragraph">
<p>By default, the containers specify a value of <code>name=statefulset-replica</code>.</p>
</div>
<div class="paragraph">
<p>There are 2 services that end user applications will use to
access the PostgreSQL cluster, one service (statefulset-primary) routes to the primary
container and the other (statefulset-replica) to the replica containers.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ ${CCP_CLI} get service
NAME            CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes      10.96.0.1       &lt;none&gt;        443/TCP    22h
statefulset-primary    10.97.168.138   &lt;none&gt;        5432/TCP   1h
statefulset-replica   10.97.218.221   &lt;none&gt;        5432/TCP   1h</pre>
</div>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_17">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>First, start the example with the following command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/statefulset
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>You can access the primary database as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h statefulset-primary -U postgres postgres</pre>
</div>
</div>
<div class="paragraph">
<p>You can access the replica databases as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h statefulset-replica -U postgres postgres</pre>
</div>
</div>
<div class="paragraph">
<p>You can scale the number of containers using this command; this will
essentially create an additional replica database.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} scale --replicas=3 statefulset statefulset</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_helm_3">Helm</h4>
<div class="paragraph">
<p>This example resides under the <code>$CCPROOT/examples/helm</code> directory. View the README to
run this example using Helm <a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/helm/statefulset/README.md">here</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_geospatial_postgis">Geospatial (PostGIS)</h3>
<div class="paragraph">
<p>An example is provided that will run a PostgreSQL with PostGIS pod and service in Kubernetes and OpenShift and a container in Docker.</p>
</div>
<div class="paragraph">
<p>The container creates a default database called <strong>userdb</strong>, a default user called
<strong>testuser</strong> and a default password of <strong>password</strong>.</p>
</div>
<div class="paragraph">
<p>You can view the extensions that postgres-gis has enabled by running the following command and viewing the listed PostGIS packages:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h postgres-gis -U testuser userdb -c '\dx'</pre>
</div>
</div>
<div class="paragraph">
<p>To validate that PostGIS is installed and which version is running, run the command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h postgres-gis -U testuser userdb -c "SELECT postgis_full_version();"</pre>
</div>
</div>
<div class="paragraph">
<p>You should expect to see output similar to:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>postgis_full_version
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 POSTGIS="2.4.2 r16113" PGSQL="100" GEOS="3.5.0-CAPI-1.9.0 r4084" PROJ="Rel. 4.8.0, 6 March 2012" GDAL="GDAL 1.11.4, released 2016/01/25" LIBXML="2.9.1" LIBJSON="0.11" TOPOLOGY RASTER
(1 row)</pre>
</div>
</div>
<div class="paragraph">
<p>As an exercise for invoking some of the basic PostGIS functionality for validation, try defining a 2D geometry point while giving inputs of
longitude and latitude through this command.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h postgres-gis -U testuser userdb -c "select ST_MakePoint(28.385200,-81.563900);"</pre>
</div>
</div>
<div class="paragraph">
<p>You should expect to see output similar to:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>                st_makepoint
--------------------------------------------
 0101000000516B9A779C623C40B98D06F0166454C0
(1 row)</pre>
</div>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_14">Docker</h4>
<div class="paragraph">
<p>Create the container as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/postgres-gis
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>Enter the following command to connect to the postgres-gis container that is
mapped to your local port 12000:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql -h localhost -U testuser -p 12000 userdb</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_18">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/postgres-gis
./run.sh</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_custom_configuration">Custom Configuration</h3>
<div class="paragraph">
<p>You can use your own version of the SQL file <code>setup.sql</code> to customize
the initialization of database data and objects when the container and
database are created.</p>
</div>
<div class="paragraph">
<p>This works by placing a file named <code>setup.sql</code> within the <code>/pgconf</code> mounted volume
directory.  Portions of the <code>setup.sql</code> file are required for the container
to work; please see comments within the sample <code>setup.sql</code> file.</p>
</div>
<div class="paragraph">
<p>If you mount a <code>/pgconf</code> volume, crunchy-postgres will look at that directory
for <code>postgresql.conf</code>, <code>pg_hba.conf</code>, <code>pg_ident.conf</code>, SSL server/ca certificates and <code>setup.sql</code>.
If it finds one of them it will use that file instead of the default files.</p>
</div>
<div class="sect3">
<h4 id="_docker_15">Docker</h4>
<div class="paragraph">
<p>This example can be run as follows for the Docker environment:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/custom-config
./run.sh</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_19">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/custom-config
./run.sh</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_ssl_authentication">SSL Authentication</h3>
<div class="paragraph">
<p>This example shows how you can configure PostgreSQL to use SSL for
client authentication.</p>
</div>
<div class="paragraph">
<p>The example requires SSL certificates and keys to be created.  Included in
the examples directory is a script to create self-signed certificates (server
and client) for the example: <code>$CCPROOT/examples/ssl-creator.sh</code>.</p>
</div>
<div class="paragraph">
<p>The example creates a client certificate for the user <code>testuser</code>.  Furthermore,
the server certificate is created for the server name <code>custom-config-ssl</code>.</p>
</div>
<div class="paragraph">
<p>This example can be run as follows for the Docker environment:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/custom-config-ssl
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>And the example can be run in the following directory for the Kubernetes and OpenShift environments:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/custom-config-ssl
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>A required step to make this example work is to define
in your <code>/etc/hosts</code> file an entry that maps <code>custom-config-ssl</code>
to the service IP address for the container.</p>
</div>
<div class="paragraph">
<p>For instance, if your service has an address as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} get service
NAME                CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE
custom-config-ssl   172.30.211.108   &lt;none&gt;        5432/TCP</pre>
</div>
</div>
<div class="paragraph">
<p>Then your <code>/etc/hosts</code> file needs an entry like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>172.30.211.108 custom-config-ssl</pre>
</div>
</div>
<div class="paragraph">
<p>For production Kubernetes and OpenShift installations, it will likely be preferred for DNS
names to resolve to the PostgreSQL service name and generate
server certificates using the DNS names instead of the example
name <code>custom-config-ssl</code>.</p>
</div>
<div class="paragraph">
<p>If as a client it&#8217;s required to confirm the identity of the server, <code>verify-full</code> can be
specified for <code>ssl-mode</code> in the connection string.  This will check if the server and the
server certificate have the same name.  Additionally, the proper connection parameters
must be specified in the connection string for the certificate information required to
trust and verify the identity of the server (<code>sslrootcert</code> and <code>sslcrl</code>), and to
authenticate the client using a certificate (<code>sslcert</code> and <code>sslkey</code>):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql "postgresql://testuser@custom-config-ssl:5432/userdb?\
sslmode=verify-full&amp;\
sslrootcert=$CCPROOT/examples/kube/custom-config-ssl/certs/ca.crt&amp;\
sslcrl=$CCPROOT/examples/kube/custom-config-ssl/certs/ca.crl&amp;\
sslcert=$CCPROOT/examples/kube/custom-config-ssl/certs/client.crt&amp;\
sslkey=$CCPROOT/examples/kube/custom-config-ssl/certs/client.key"</pre>
</div>
</div>
<div class="paragraph">
<p>To connect via IP, <code>sslmode</code> can be changed to <code>require</code>.  This will verify the server
by checking the certificate chain up to the trusted certificate authority, but will not
verify that the hostname matches the certificate, as occurs with <code>verify-full</code>.  The same
connection parameters as above can be then provided for the client and server certificate
information.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql "postgresql://testuser@IP_OF_PGSQL:5432/userdb?\
sslmode=require&amp;\
sslrootcert=$CCPROOT/examples/kube/custom-config-ssl/certs/ca.crt&amp;\
sslcrl=$CCPROOT/examples/kube/custom-config-ssl/certs/ca.crl&amp;\
sslcert=$CCPROOT/examples/kube/custom-config-ssl/certs/client.crt&amp;\
sslkey=$CCPROOT/examples/kube/custom-config-ssl/certs/client.key"</pre>
</div>
</div>
<div class="paragraph">
<p>You should see a connection that looks like the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>psql (10.6)
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

userdb=&gt;</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_docker_swarm">Docker Swarm</h3>
<div class="paragraph">
<p>This example shows how to run a primary and replica database
container on a Docker Swarm (v.1.12) cluster.</p>
</div>
<div class="paragraph">
<p>First, set up a cluster. The Kubernetes libvirt coreos cluster
example works well; see <a href="http://kubernetes.io/docs/getting-started-guides/libvirt-coreos/">coreos-libvirt-cluster.</a></p>
</div>
<div class="paragraph">
<p>Next, on each node, create the Swarm using these
<a href="https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/">Swarm Install instructions.</a></p>
</div>
<div class="paragraph">
<p>Include this command on the manager node:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker swarm init --advertise-addr 192.168.10.1</pre>
</div>
</div>
<div class="paragraph">
<p>Then this command on all the worker nodes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre> docker swarm join \
     --token SWMTKN-1-65cn5wa1qv76l8l45uvlsbprogyhlprjpn27p1qxjwqmncn37o-015egopg4jhtbmlu04faon82u \
         192.168.10.1.37</pre>
</div>
</div>
<div class="paragraph">
<p>Before creating Swarm services, it is necessary
to define an overlay network to be used by the services you will
create. This can be done as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker network create --driver overlay crunchynet</pre>
</div>
</div>
<div class="paragraph">
<p>We want to have the primary database always placed on
a specific node. This is accomplished using node constraints
as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker node inspect kubernetes-node-1 | grep ID
docker node update --label-add type=primary 18yrb7m650umx738rtevojpqy</pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, the <code>kubernetes-node-1</code> node with ID
<code>18yrb7m650umx738rtevojpqy</code> has a user defined label of <strong>primary</strong> added to it.
The primary service specifies <strong>primary</strong> as a constraint when created; this
tells Swarm to place the service on that specific node.  The replica specifies
a constraint of <code>node.labels.type != primary</code> to have the replica
always placed on a node that is not hosting the primary service.</p>
</div>
<div class="sect3">
<h4 id="_docker_16">Docker</h4>
<div class="paragraph">
<p>After you set up the Swarm cluster, you can then run this example as follows on the <strong>Swarm Manager Node</strong>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/swarm-service
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>You can then find the nodes that are running the primary and replica containers
by:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker service ps primary
docker service ps replica</pre>
</div>
</div>
<div class="paragraph">
<p>You can also scale up the number of <strong>replica</strong> containers.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker service scale replica=2
docker service ls</pre>
</div>
</div>
<div class="paragraph">
<p>Verify you have two replicas within PostgreSQL by viewing the <code>pg_stat_replication</code> table.
The password is <strong>password</strong> by default when logged into the <code>kubernetes-node-1</code> host:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>docker exec -it $(docker ps -q) psql -U postgres -c 'table pg_stat_replication' postgres</pre>
</div>
</div>
<div class="paragraph">
<p>You should see a row for each replica along with its replication status.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_failover">Failover</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_watch">Watch</h3>
<div class="paragraph">
<p>Crunchy Watch is an application wrapped in a container that watches a PostgreSQL
primary database and waits for a failure to occur, at which point a failover is
performed to promote a replica.</p>
</div>
<div class="paragraph">
<p>The crunchy-watch container, while originally part of the Container Suite, has been
split out into its own project. More information on the Watch container and it&#8217;s
capabilities can be found in the new project repository located at
<a href="https://github.com/CrunchyData/crunchy-watch" class="bare">https://github.com/CrunchyData/crunchy-watch</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_metrics_and_performance">Metrics and Performance</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_pgbadger">pgBadger</h3>
<div class="paragraph">
<p>pgbadger is a PostgreSQL tool that reads the log files from a specified database
in order to produce a HTML report that shows various PostgreSQL statistics and graphs.
This example runs the pgbadger HTTP server against a crunchy-postgres container and
illustrates how to view the generated reports.</p>
</div>
<div class="paragraph">
<p>The port utilized for this tool is port 14000 for Docker environments and port 10000
for Kubernetes and OpenShift environments.</p>
</div>
<div class="paragraph">
<p>The container creates a default database called <strong>userdb</strong>, a default user called
<strong>testuser</strong> and a default password of <strong>password</strong>.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_17">Docker</h4>
<div class="paragraph">
<p>Run the example as follows:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgbadger
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>After execution, the container will run and provide a simple HTTP
command you can browse to view the report.  As you run queries against
the database, you can invoke this URL to generate updated reports:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>curl -L http://127.0.0.1:14000/api/badgergenerate</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_20">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgbadger
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>After execution, the container will run and provide a simple HTTP
command you can browse to view the report.  As you run queries against
the database, you can invoke this URL to generate updated reports:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>curl -L http://pgbadger:10000/api/badgergenerate</pre>
</div>
</div>
<div class="paragraph">
<p>You can view the database container logs using these commands:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} logs pgbadger -c pgbadger
${CCP_CLI} logs pgbadger -c postgres</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_metrics_collection">Metrics Collection</h3>
<div class="paragraph">
<p>You can collect various PostgreSQL metrics from your database
container by running a crunchy-collect container that points
to your database container.</p>
</div>
<div class="paragraph">
<p>This example starts up 5 containers:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Collect (crunchy-collect)</p>
</li>
<li>
<p>Grafana (crunchy-grafana)</p>
</li>
<li>
<p>PostgreSQL (crunchy-postgres)</p>
</li>
<li>
<p>Prometheus (crunchy-prometheus)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Every 5 seconds by default, Prometheus will scrape the Collect container
for metrics.  These metrics will then be visualized by Grafana, which by default can be accessed
with the following credentials:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Username : <strong>admin</strong></p>
</li>
<li>
<p>Password: <strong>password</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>By default, Prometheus detects which environment its running on (Docker, Kubernetes, or OpenShift Container Platform)
and applies a default configuration.</p>
</div>
<div class="paragraph">
<p>When running in Kuberenetes and OpenShift, the following two labels are required by
the deployments:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>"crunchy_collect": "true"</code></p>
</li>
<li>
<p><code>"name": "some-pod-name-here"</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>crunchy_collect</code> label allows Prometheus to find all pods that are serving metrics
to be scraped for storage.</p>
</div>
<div class="paragraph">
<p>The <code>name</code> label allows Prometheus to rewrite the name of the pod so if it changes there&#8217;s not
duplicate entries.</p>
</div>
<div class="paragraph">
<p>Additionally, the collect container uses a special PostgreSQL role <code>ccp_monitoring</code>.
This user is created by setting the <code>PGMONITOR_PASSWORD</code> environment variable on the
PostgreSQL container.</p>
</div>
<div class="paragraph">
<p>Discovering pods requires a cluster role service account.  See the
<a href="https://github.com/crunchydata/crunchy-containers/blob/master/examples/kube/metrics/metrics.json">Kubernetes and OpenShift</a>
metrics JSON file for more details.</p>
</div>
<div class="paragraph">
<p>For Docker environments the collect hostname must be specified as an environment
variable.</p>
</div>
<div class="paragraph">
<p>To shutdown the instance and remove the container for each example, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./cleanup.sh</pre>
</div>
</div>
<div class="sect3">
<h4 id="_docker_18">Docker</h4>
<div class="paragraph">
<p>To start this set of containers, run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/metrics
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>You will be able to access the Grafana and Prometheus services from the following
web addresses:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Grafana (<a href="http://0.0.0.0:3000" class="bare">http://0.0.0.0:3000</a>)</p>
</li>
<li>
<p>Prometheus (<a href="http://0.0.0.0:9090" class="bare">http://0.0.0.0:9090</a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The crunchy-postgres container is accessible on port <strong>5432</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_21">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Running the example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/metrics
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>It&#8217;s required to use <code>port-forward</code> to access the Grafana dashboard.  To start the
port-forward, run the following command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} port-forward metrics 3000:3000
${CCP_CLI} port-forward metrics 9090:9090</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Grafana dashboard can be then accessed from <code><a href="http://127.0.0.01:3000" class="bare">http://127.0.0.01:3000</a></code></p>
</li>
<li>
<p>Prometheus dashboard can be then accessed from <code><a href="http://127.0.0.01:9090" class="bare">http://127.0.0.01:9090</a></code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can view the container logs using these command:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>${CCP_CLI} logs -c grafana metrics
${CCP_CLI} logs -c prometheus metrics
${CCP_CLI} logs -c collect primary-metrics
${CCP_CLI} logs -c postgres primary-metrics
${CCP_CLI} logs -c collect replica-metrics
${CCP_CLI} logs -c postgres replica-metrics</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_pg_audit">pg_audit</h3>
<div class="paragraph">
<p>This example provides an example of enabling pg_audit output.
As of release 1.3, pg_audit is included in the crunchy-postgres
container and is added to the PostgreSQL shared library list in
<code>postgresql.conf</code>.</p>
</div>
<div class="paragraph">
<p>Given the numerous ways pg_audit can be configured, the exact
pg_audit configuration is left to the user to define.  pg_audit
allows you to configure auditing rules either in <code>postgresql.conf</code>
or within your SQL script.</p>
</div>
<div class="paragraph">
<p>For this test, we place pg_audit statements within a SQL script
and verify that auditing is enabled and working.  If you choose
to configure pg_audit via a <code>postgresql.conf</code> file, then you will
need to define your own custom file and mount it to override the
default <code>postgresql.conf</code> file.</p>
</div>
<div class="sect3">
<h4 id="_docker_19">Docker</h4>
<div class="paragraph">
<p>Run the following to create a database container:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/docker/pgaudit
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This starts an instance of the pg_audit container (running crunchy-postgres)
on port 12005 on localhost. The test script is then automatically executed.</p>
</div>
<div class="paragraph">
<p>This test executes a SQL file which contains pg_audit configuration
statements as well as executes some basic SQL commands.  These
SQL commands will cause pg_audit to create log messages in
the <code>pg_log</code> log file created by the database container.</p>
</div>
</div>
<div class="sect3">
<h4 id="_kubernetes_and_openshift_22">Kubernetes and OpenShift</h4>
<div class="paragraph">
<p>Run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgaudit
./run.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This script will create a PostgreSQL pod with the pgAudit extension configured and ready
to use</p>
</div>
<div class="paragraph">
<p>Once the pod is deployed successfully run the following command to test the extension:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cd $CCPROOT/examples/kube/pgaudit
./test-pgaudit.sh</pre>
</div>
</div>
<div class="paragraph">
<p>This example has been configured to log directly to stdout of the pod.  To view the PostgreSQL logs
run the following:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$CCP_CLI logs pgaudit</pre>
</div>
</div>
</div>
</div>
</div>
</div>



    
    
          <footer class=" footline" >
	
</footer>
  </div>
</div>

<div id="navigation">
<a class="nav nav-prev" href="https://crunchydata.github.io/crunchy-containers/latest/getting-started/" title="Getting Started"> <i class="fa fa-chevron-left"></i><label>Getting Started</label></a>
    <a class="nav nav-next" href="https://crunchydata.github.io/crunchy-containers/latest/container-specifications/" title="Container Specifications" style="margin-right: 0px;"><label>Container Specifications</label><i class="fa fa-chevron-right"></i></a></div>

</section>
<div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
  <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
</div>    

<script src="https://crunchydata.github.io/crunchy-containers/latest/js/clipboard.min.js"></script>
<script src="https://crunchydata.github.io/crunchy-containers/latest/js/featherlight.min.js"></script>
<script src="https://crunchydata.github.io/crunchy-containers/latest/js/html5shiv-printshiv.min.js"></script>

<script src="https://crunchydata.github.io/crunchy-containers/latest/js/modernizr.custom.71422.js"></script>
<script src="https://crunchydata.github.io/crunchy-containers/latest/js/docdock.js"></script>
<script src="https://crunchydata.github.io/crunchy-containers/latest/theme-original/script.js"></script>


    

    
    

    
  </body>
</html>