<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="generator" content="AsciiDoc 8.6.8, bootstrap backend 4.5.0">
    <title>Openshift Examples - Crunchy Containers for PostgreSQL</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Crunchy Data Solutions, Inc.">
    <!-- AsciiDoc Bootstrap styles -->
    <link rel="stylesheet" type="text/css" id="bootstrapTheme" href="./stylesheets/asciidoc-bootstrap.min.css">
    <!-- Back to top (jquery plugin) -->
    <link rel="stylesheet" type="text/css" href="./stylesheets/ui.totop.css">

    <!--[if (lt IE 9) & (!IEMobile)]>
        <script src="./javascripts/html5shiv.min.js"></script>
        <script src="./javascripts/respond.min.js"></script>
    <![endif]-->

  </head>
  <body id="toc-top">
    <div id="page">


      <div class="jumbotron">
        <div class="container">
          <h1>Openshift Examples - Crunchy Containers for PostgreSQL</h1>
        </div>
      </div>

  <div id="content" class="container">

    <div class="row">




        <div class="col-md-9" role="main">
<div class="section">
    <h1 class="page-header" id="examples_for_the_openshift_environment">1. Examples for the Openshift Environment</h1>
<div class="paragraph"><p>The examples/openshift directory containers examples for
running the Crunchy containers in an Openshift environment.</p></div>
<div class="paragraph"><p>The examples are explained below.</p></div>
<h2 id="openshift_example_1_simple_master">1.1. Openshift Example 1 - simple master</h2>
<div class="paragraph"><p>This openshift template will create a single master PostgreSQL instance.</p></div>
<div class="paragraph"><p>Running the example:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>cd single-master
./run.sh</pre>
</div></div>
<div class="paragraph"><p>You can see what passwords were generated by running this command:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc describe pod single-master | grep PG</pre>
</div></div>
<div class="paragraph"><p>You can run the following command to test the database, entering
the value of PG_PASSWORD from above for the password when prompted:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -h single-master.openshift.svc.cluster.local -U testuser userdb</pre>
</div></div>
<h2 id="openshift_example_2_slave_json">1.2. Openshift Example 2 - slave.json</h2>
<div class="paragraph"><p>This Openshift template will create a single slave PostgreSQL instance
that will connect to the master created in Example 1.</p></div>
<div class="paragraph"><p>You will need to edit the slave.json file and enter the PG_MASTER_PASSWORD
value generated in Example 1 as found in the master environment variables.</p></div>
<div class="paragraph"><p>Running the example:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f slave.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>You can run the following command to test the database, entering
the value of PG_PASSWORD from above for the password when prompted:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -c 'select * from pg_stat_replication' -h pg-master.pgproject.svc.cluster.local -U master userdb
psql -c 'create table foo (id int)' -h pg-master.pgproject.svc.cluster.local -U master userdb
psql -c 'insert into foo values (123)' -h pg-master.pgproject.svc.cluster.local -U master userdb
psql -c 'table foo' -h pg-slave.pgproject.svc.cluster.local -U master userdb</pre>
</div></div>
<div class="paragraph"><p>If replication is working, you should see a row returned in the
pg_stat_replication table query and also a row returned from
the pg-slave query.</p></div>
<h2 id="openshift_example_3_pgpool_for_master_slave_examples">1.3. Openshift Example 3 - pgpool for master slave examples</h2>
<div class="paragraph"><p>You can create a pgpool service that will work with the
master and slave created in the previous Example 1 and Example 2.</p></div>
<div class="paragraph"><p>You will need to edit the pgpool-for-master-slave-rc-dc-slaves-only.json and supply the
testuser password that was generated when you created
the master slave pods, then run the following command
to deploy the pgpool service:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>cd examples/openshift/pgpooltest
./run.sh</pre>
</div></div>
<div class="paragraph"><p>Next, you can access the master slave cluster via the pgpool
service by entering the following command:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -h pgpool-rc -U testuser userdb
psql -h pgpool-rc -U testuser postgres</pre>
</div></div>
<div class="paragraph"><p>when prompted, enter the password for the PG_USERNAME testuser
that was set for the pg-master pod, typically it is <strong>password</strong>.</p></div>
<div class="paragraph"><p>At this point, you can enter SELECT and INSERT statements and
pgpool will proxy the SQL commands to the master or slave(s)
depending on the type of SQL command.  Writes will always
be sent to the master, and reads will be sent (round-robin)
to the slave(s).</p></div>
<h2 id="openshift_example_4_master_slave_rc_dc_slaves_only_json">1.4. Openshift Example 4 - master-slave-rc-dc-slaves-only.json</h2>
<div class="paragraph"><p>This example is similar to the previous examples but
builds a master pod, and a single slave that can be scaled up
using a replication controller.   The master is implemented as
a single pod since it can not be scaled like read-only slaves.</p></div>
<div class="paragraph"><p>Running the example:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f master-slave-rc-dc-slaves-only.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>Connect to the PostgreSQL instances with the following:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -h pg-master-rc-dc.pgproject.svc.cluster.local -U testuser userdb
psql -h pg-slave-rc-dc.pgproject.svc.cluster.local -U testuser userdb</pre>
</div></div>
<div class="paragraph"><p>Here is an example of increasing or scaling up the Postgres <em>slave</em> pods to 2:</p></div>
<div class="paragraph"><p>Here is an example of increasing or scaling up the Postgres <em>slave</em> pods to 2:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc scale rc pg-slave-rc-dc-1 --replicas=2</pre>
</div></div>
<div class="paragraph"><p>Enter the following commands to verify the PostgreSQL replication is working.</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -c 'table pg_stat_replication' -h pg-master-rc.pgproject.svc.cluster.local -U master postgres
psql -h pg-slave-rc.pgproject.svc.cluster.local -U master postgres</pre>
</div></div>
<div class="paragraph"><p>You can see that the slave service is load balancing between
multiple slaves by running a command as follows, run the command
multiple times and the ip address should alternate between
the slaves:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -c 'select inet_server_addr()' -h pg-slave-rc -U master postgres</pre>
</div></div>
<h2 id="openshift_example_5_performing_a_backup">1.5. Openshift Example 5 - Performing a Backup</h2>
<div class="paragraph"><p>This example assumes you have a master database pod running called pg-master
as created by Openshift Example 1 and that you have configured NFS as decribed
in the Prerequisites section of this document..</p></div>
<div class="paragraph"><p>You can perform a database backup by executing the following
step:</p></div>
<div class="ulist"><ul>
<li>
<p>
as openshift admin, run:
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f pv-backups.json</pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
as normal user, run:
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f pvc-backups.json
oc create -f backup-job-nfs.json</pre>
</div></div>
<div class="paragraph"><p>A successful backup will perform pg_basebackup on the pg-master and store
the backup in the NFS mounted volume under a directory named pg-master, each
backup will be stored in a subdirectory with a timestamp as the name.  This
allows any number of backups to be kept.</p></div>
<div class="paragraph"><p>The pv-backups.json specifies a ReclaimPolicy of Retain to tell Openshift
that we want to keep the volume contents after the removal of the PV.</p></div>
<div class="paragraph"><p>Lastly, in order to rerun this same backup, you will need to
remove the Job, PVC, and PV used in this example, this is done as follows:</p></div>
<div class="ulist"><ul>
<li>
<p>
as admin user
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc delete pv backup-pg-master</pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
as normal user
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc delete pvc backup-claim-pg-master
oc delete job backupjob-pg-master</pre>
</div></div>
<h2 id="openshift_example_5_nfs_example">1.6. Openshift Example 5 - NFS Example</h2>
<div class="paragraph"><p>I have provided an example of using NFS for the postgres data volume.
On my test nfs server, I had to set the exports file entry as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>/jeffnfs * (rw,insecure,sync)</pre>
</div></div>
<div class="paragraph"><p>First, you can only create persistent volumes as a cluster admin, you can
login in as the admin user as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc login -u system:admin</pre>
</div></div>
<div class="paragraph"><p>To run it, you would execute the following as the openshift administrator:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f master-nfs-pv.json</pre>
</div></div>
<div class="paragraph"><p>Then as the normal openshift user account, create the Persistence Volume
Claim and database pod as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f master-nfs-pvc.json
oc process -f master-nfs.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>This will create a single master postgres pod that is using
an NFS volume to store the postgres data files.</p></div>
<h2 id="openshift_example_6_restore_example">1.7. Openshift Example 6 - Restore Example</h2>
<div class="paragraph"><p>I have provided an example of restoring a database pod using
an existing backup archive located on an NFS volume.</p></div>
<div class="paragraph"><p>First, locate the database backup you want to restore, for example:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>/jeffnfs/pg-master/2016-01-29:22:34:20</pre>
</div></div>
<div class="paragraph"><p>Next,
 * edit the master-restore-pv.json file to use that path in building
the PV,
 * edit the master-restore-pv.json file to use a unique label
 * and then execute as the openshift superuser:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc login -u system:admin
oc create -f master-restore-pv.json</pre>
</div></div>
<div class="paragraph"><p>Next,
 * edit the master-restore-pvc.json file, specify the same unique
label used in the master-restore-pv.json file.
 * Then execute as the normal test user:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f master-restore-pvc.json</pre>
</div></div>
<div class="paragraph"><p>Next, create a database pod as the normal user:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc process -f master-restore.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>When the database pod starts, it will copy the backup files
to the database directory inside the pod and start up postgres as
usual.</p></div>
<div class="paragraph"><p>The restore only takes place if:</p></div>
<div class="ulist"><ul>
<li>
<p>
the /pgdata directory is empty
</p>
</li>
<li>
<p>
the /backups directory contains a valid postgresql.conf file
</p>
</li>
</ul></div>
<h2 id="openshift_example_7_failover_example">1.8. Openshift Example 7 - Failover Example</h2>
<div class="paragraph"><p>An example of performing a database failover is described
in the following steps:</p></div>
<div class="ulist"><ul>
<li>
<p>
create a master and slave replication using master-slave-rc-dc-slaves-only.json
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc process -f master-slave-rc-dc-slaves-only.json | oc create -f -</pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
scale up the number of slaves to 2
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc scale rc pg-slave-rc-1 --replicas=2</pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
delete the master pod
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc delete pod pg-master-rc</pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
exec into a slave and create a trigger file to being
   the recovery process, effectively turning the slave into a master
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc exec -it pg-slave-rc-1-lt5a5
touch /tmp/pg-failover-trigger</pre>
</div></div>
<div class="ulist"><ul>
<li>
<p>
change the label on the slave to pg-master-rc instead of pg-slave-rc
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc edit pod/pg-slave-rc-1-lt5a5
original line: labels/name: pg-slave-rc
updated line: labels/name: pg-master-rc</pre>
</div></div>
<div class="literalblock">
<div class="content monospaced">
<pre>or alternatively:</pre>
</div></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc label --overwrite=true pod pg-slave-rc-1-lt5a5 name=pg-master-rc</pre>
</div></div>
<div class="paragraph"><p>You can test the failover by creating some data on the master
and then test to see if the slaves have the replicated data.</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -c 'create table foo (id int)' -U master -h pg-master-rc postgres
psql -c 'table foo' -U master -h pg-slave-rc postgres</pre>
</div></div>
<div class="paragraph"><p>After a failover, you would most likely want to create a database
backup and be prepared to recreate your cluster from that backup.</p></div>
<h2 id="openshift_example_8_master_slave_deployment_using_nfs">1.9. Openshift Example 8 - Master Slave Deployment using NFS</h2>
<div class="paragraph"><p>This example uses NFS volumes for the master and the slaves.  In
some scenarios, customers might want to have all the Postgres
instances using NFS volumes for persistence.</p></div>
<div class="paragraph"><p>Relevant files for this example:</p></div>
<div class="ulist"><ul>
<li>
<p>
master-slave-rc-nfs.json
This file creates the master and slave deployment, creating pods and services
where the slave is controlled by a Replication Controller, allowing you
to scale up the slaves.
</p>
</li>
</ul></div>
<div class="paragraph"><p>To run the example, follow these steps:</p></div>
<div class="ulist"><ul>
<li>
<p>
as the openshift admin, create the required PV(s) using this command:
</p>
</li>
</ul></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f master-slave-rc-nfs-pv.json
oc create -f master-slave-rc-nfs-pv2.json</pre>
</div></div>
<div class="paragraph"><p>This will create a PV for the master and another PV for the slaves.
 * as the project user, create the required PVC(s) using this command:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f master-slave-rc-nfs-pvc2.json
oc create -f master-slave-rc-nfs-pvc.json</pre>
</div></div>
<div class="paragraph"><p>This will create a PVC for the master and another PVC for the slaves.
 * as the project user, create the master slave deployment:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc process -f master-slave-rc-nfs.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>If you examing your NFS directory, you will see postgres data directories
created and used by your master and slave pods.</p></div>
<div class="paragraph"><p>Next, add some test data to the master:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -c 'create table testtable (id int)' -U master -h pg-master-rc-nfs postgres
psql -c 'insert into testtable values (123)' -U master -h pg-master-rc-nfs postgres</pre>
</div></div>
<div class="paragraph"><p>Next, add a new slave:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc scale rc pg-slave-rc-nfs-1 --replicas=2</pre>
</div></div>
<div class="paragraph"><p>At this point, you should see the new NFS directory created by the new
slave pod, and you should also be able to test that replication is
working on the new slave:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -c 'table testtable' -U master -h pg-slave-rc-nfs postgres</pre>
</div></div>
<h2 id="openshift_example_9_master_with_pgbadger_add_in">1.10. Openshift Example 9 - Master with pgbadger add-in</h2>
<div class="paragraph"><p>This example uses a version of master.json but also adds the pgbadger
container to the pg-master pod.  pgbadger is then served up on port
10000.  Each time you do a GET on <a href="http://pg-master:10000/api/badgergenerate">http://pg-master:10000/api/badgergenerate</a>
it will run pgbadger against the database log files running in the
pg-master container.</p></div>
<div class="paragraph"><p>To run the example:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>cd examples/openshift/badger
./run.sh</pre>
</div></div>
<div class="paragraph"><p>try the following command to see the generated HTML output:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>curl http://pg-master:10000/api/badgergenerate</pre>
</div></div>
<div class="paragraph"><p>You can view this output in a browser if you allow port forwarding
from your container to your server host using a command like
this:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>socat tcp-listen:10001,reuseaddr,fork tcp:pg-master:10000</pre>
</div></div>
<div class="paragraph"><p>This command maps port 10000 of the service/container to port
10001 of the local server.  You can now use your browser to
see the badger report.</p></div>
<div class="paragraph"><p>This is a short-cut way to expose a service to the external world,
Openshift would normally configure a Router whereby you could
<em>expose</em> the service in an Openshift way.  Here is the docs
on installing the Openshift Router:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>https://docs.openshift.com/enterprise/3.0/install_config/install/deploy_router.html</pre>
</div></div>
<h2 id="openshift_example_10_master_with_readiness_probe">1.11. Openshift Example 10 - Master with readiness probe</h2>
<div class="paragraph"><p>This example uses a version of master.json but also adds a Kubernetes
readiness probe specific for postgresql.  This readiness probe
uses the postgres pg_isready utility to attempt a connection
to postgres from within the container using the postgres user and
postgres database as the parameters.</p></div>
<div class="paragraph"><p>Run the following:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f master-ready.json  | oc create -f -</pre>
</div></div>
<h2 id="openshift_example_11_master_with_secrets">1.12. Openshift Example 11 - Master with secrets</h2>
<div class="paragraph"><p>This example allows you to set the Postgresql passwords
using Kube Secrets.</p></div>
<div class="paragraph"><p>The secret uses a base64 encoded string to represent the
values to be read by the container during initialization.  The
encoded password value is <strong>password</strong>.  Run the example
as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>examples/openshift/secret/run.sh</pre>
</div></div>
<div class="paragraph"><p>The secrets are mounted in the <strong>/pguser</strong>, <strong>/pgmaster</strong>, <strong>/pgroot</strong> volumes within the
container and read during initialization.  The container
scripts create a Postgres user with those values, and sets the passwords
for the master user and postgres superuser using the mounted secret volumes.</p></div>
<div class="paragraph"><p>When using secrets, you do NOT have to specify the following
env vars if you specify all three secrets volumes:
 * PG_USER
 * PG_PASSWORD
 * PG_ROOT_PASSWORD
 * PG_MASTER_USER
 * PG_MASTER_PASSWORD</p></div>
<div class="paragraph"><p>You can test the container as follows, in all cases, the password is <strong>password</strong>:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -h secret-pg -U pguser1 postgres
psql -h secret-pg -U postgres postgres
psql -h secret-pg -U master postgres</pre>
</div></div>
<h2 id="openshift_example_12_automated_failover">1.13. Openshift Example 12 - Automated Failover</h2>
<div class="paragraph"><p>This example shows how a form of automated failover can be
configured for a master and slave deployment.</p></div>
<div class="paragraph"><p>First, create a master and 2 slaves:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc process -f master-slave.json
oc scale rc pg-slave-rc-dc-1 --replicas=2</pre>
</div></div>
<div class="paragraph"><p>Next, create an Openshift service account which is used by the crunchy-watch
container to perform the failover, also set policies that allow the
service account the ability to edit resources within the openshift and
default projects :</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc create -f sa.json
oc policy add-role-to-group edit system:serviceaccounts -n openshift
oc policy add-role-to-group edit system:serviceaccounts -n default</pre>
</div></div>
<div class="paragraph"><p>Next, create the container that will <em>watch</em> the Postgresql cluster:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc process -f watch.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>At this point, the watcher will sleep every 20 seconds (configurable) to
see if the master is responding.  If the master doesn&#8217;t respond, the watcher
will perform the following logic:</p></div>
<div class="ulist"><ul>
<li>
<p>
log into openshift using the service account
</p>
</li>
<li>
<p>
set its current project
</p>
</li>
<li>
<p>
find the first slave pod
</p>
</li>
<li>
<p>
delete the master service saving off the master service definition
</p>
</li>
<li>
<p>
create the trigger file on the first slave pod
</p>
</li>
<li>
<p>
wait 20 seconds for the failover to complete on the slave pod
</p>
</li>
<li>
<p>
edit the slave pod&#8217;s lable to match that of the master
</p>
</li>
<li>
<p>
recreate the master service using the stored service definition
</p>
</li>
<li>
<p>
loop through the other remaining slave and delete its pod
</p>
</li>
</ul></div>
<div class="paragraph"><p>At this point, clients when access the master&#8217;s service will actually
be accessing the new master.  Also, Openshift will recreate the number
of slaves to its original configuration which each slave pointed to the
new master.  Replication from the master to the new slaves will be
started as each new slave is started by Openshift.</p></div>
<h2 id="openshift_example_13_metrics_collection">1.14. Openshift Example 13 - Metrics Collection</h2>
<div class="paragraph"><p>This example shows how postgres metrics can be collected
and stored in prometheus and graphed with grafana.</p></div>
<div class="paragraph"><p>First, create the crunchy-scope pod which contains
the prometheus data store and the grafana graphing web application:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>edit scope-pv.json to update the NFS share info
oc create -f scope-pv.json
oc create -f scope-pvc.json
oc process -f scope-nfs.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>At this point, you can view the prometheus web console at
crunchy-scope:9090, the prometheus push gateway at crunchy-scope:9091,
and the grafana web app at crunchy-scope:3000.</p></div>
<div class="paragraph"><p>Next, start a postgres pod that has the crunchy-collect container
as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc process -f master-collect.json | oc create -f -</pre>
</div></div>
<div class="paragraph"><p>At this point, metrics will be collected every 3 minutes and pushed
to prometheus.  You can build graphs off the metrics using grafana.</p></div>
<h2 id="openshift_example_14_vacuum">1.15. Openshift Example 14 - Vacuum</h2>
<div class="paragraph"><p>This example shows how you can run a vacuum job against
a postgres database container.</p></div>
<div class="paragraph"><p>The crunchy-vacuum container image exists to allow a DBA
a way to run a job either one-off or scheduled to perform
a variety of vacuum operations.</p></div>
<div class="paragraph"><p>To run the vacuum a single time, an example is included
as follows from the examples/openshift directory:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>cd examples/openshift/master-slave
./run.sh
cd ../vacuum-job
./run.sh</pre>
</div></div>
<div class="paragraph"><p>This will start a vacuum container that runs as a Kube Job type.  It
will run once.  The crunchy-vacuum image is executed, passed in
the Postgres connection parameters to the single-master postgres
container.  The type of vacuum performed is dictated by the
environment variables passed into the job. The complete set
of environment variables read by the vacuum job include:</p></div>
<div class="ulist"><ul>
<li>
<p>
VAC_FULL - when set to true adds the FULL parameter to the VACUUM command
</p>
</li>
<li>
<p>
VAC_TABLE - when set, allows you to specify a single table to vacuum, when
 not specified, the entire database tables are vacuumed
</p>
</li>
<li>
<p>
JOB_HOST - required variable is the postgres host we connect to
</p>
</li>
<li>
<p>
PG_USER - required variable is the postgres user we connect with
</p>
</li>
<li>
<p>
PG_DATABASE - required variable is the postgres database we connect to
</p>
</li>
<li>
<p>
PG_PASSWORD - required variable is the postgres user password we connect with
</p>
</li>
<li>
<p>
PG_PORT - allows you to override the default value of 5432
</p>
</li>
<li>
<p>
VAC_ANALYZE - when set to true adds the ANALYZE parameter to the VACUUM command
</p>
</li>
<li>
<p>
VAC_VERBOSE - when set to true adds the VERBOSE parameter to the VACUUM command
</p>
</li>
<li>
<p>
VAC_FREEZE - when set to true adds the FREEZE parameter to the VACUUM command
</p>
</li>
</ul></div>
<h2 id="openshift_example_15_custom_configuration_files">1.16. Openshift Example 15 - Custom Configuration Files</h2>
<div class="paragraph"><p>This example shows how you can use your own customized version of setup.sql
when creating a postgres database container.</p></div>
<div class="paragraph"><p>If you mount a /pgconf volume, crunchy-postgres will look at that directory
for postgresql.conf, pg_hba.conf, and setup.sql.  If it finds one of them it
will use that file instead of the default files.</p></div>
<div class="paragraph"><p>The example shows how a custom setup.sql file can be used.
Run it as follows from the examples/openshift/custom-config directory:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>./run.sh</pre>
</div></div>
<div class="paragraph"><p>This will start a database container that will use an NFS mounted /pgconf
directory that will container the custom setup.sql file found in the example
directory.</p></div>
<h2 id="openshift_example_16_pgbouncer">1.17. Openshift Example 16 - pgbouncer</h2>
<div class="paragraph"><p>This example shows how you can use the crunchy-pgbouncer container
when running under Openshift.</p></div>
<div class="paragraph"><p>The example assumes you have run the master/slave example
found here:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>examples/openshift/master-slave-dc</pre>
</div></div>
<div class="paragraph"><p>Then you would start up the pgbouncer container using the following
example:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>examples/openshift/pgbouncer</pre>
</div></div>
<div class="paragraph"><p>The example assumes you have an NFS share path of /nfsfileshare/!  NFS
is required to mount the pgbouncer configuration files which are
then mounted to /pgconf in the crunchy-pgbouncer container.</p></div>
<div class="paragraph"><p>If you mount a /pgconf volume, crunchy-postgres will look at that directory
for postgresql.conf, pg_hba.conf, and setup.sql.  If it finds one of them it
will use that file instead of the default files.</p></div>
<div class="paragraph"><p>Test the example by killing off the master database container as
follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc delete pod pg-master-rc-dc</pre>
</div></div>
<div class="paragraph"><p>Then watch the pgbouncer log as follows to confirm it detects the loss
of the master:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc logs pgbouncer</pre>
</div></div>
<div class="paragraph"><p>After the failover is completed, you should be able to access
the new master using the master service as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -h pg-master-rc-dc.openshift.svc.cluster.local -U master postgres</pre>
</div></div>
<div class="paragraph"><p>and access the slave as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -h pg-slave-rc-dc.openshift.svc.cluster.local -U master postgres</pre>
</div></div>
<div class="paragraph"><p>or via the pgbouncer proxy as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>psql -h pgbouncer.openshift.svc.cluster.local  -U master master</pre>
</div></div>
</div>
<div class="section">
    <h1 class="page-header" id="openshift_tips">2. Openshift Tips</h1>
<h2 id="tip_1_finding_the_postgresql_passwords">2.1. Tip 1: Finding the Postgresql Passwords</h2>
<div class="paragraph"><p>The passwords used for the PostgreSQL user accounts are generated
by the Openshift <em>process</em> command.  To inspect what value was
supplied, you can inspect the master pod as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc get pod pg-master-rc-1-n5z8r -o json</pre>
</div></div>
<div class="paragraph"><p>Look for the values of the environment variables:
- PG_USER
- PG_PASSWORD
- PG_DATABASE</p></div>
<h2 id="tip_2_examining_a_backup_job_log">2.2. Tip 2: Examining a backup job log</h2>
<div class="paragraph"><p>Database backups are implemented as a Kubernetes Job.  A Job is meant to run one time only
and not be restarted by Kubernetes.  To view jobs in Openshift you enter:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc get jobs
oc describe job backupjob</pre>
</div></div>
<div class="paragraph"><p>You can get detailed logs by referring to the pod identifier in the job <em>describe</em>
output as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc logs backupjob-pxh2o</pre>
</div></div>
<h2 id="tip_3_backup_lifecycle">2.3. Tip 3: Backup Lifecycle</h2>
<div class="paragraph"><p>Backups require the use of network storage like NFS in Openshift.
There is a required order of using NFS volumes in the manner
we do database backups.</p></div>
<div class="paragraph"><p>So, first off, there is a one-to-one relationship between
a PV (persistent volume) and a PVC (persistence volume claim).  You
can NOT have a one-to-many relationship between PV and PVC(s).</p></div>
<div class="paragraph"><p>So, to do a database backup repeatably, you will need to following
this general pattern:
 * as openshift admin user, create a unique PV (e.g. backup-pv-mydatabase)
 * as a project user, create a unique PVC (e.g. backup-pvc-mydatabase)
 * reference the unique PVC within the backup-job template
 * execute the backup job template
 * as a project user, delete the job
 * as a project user, delete the pvc
 * as openshift admin user, delete the unique PV</p></div>
<div class="paragraph"><p>This procedure will need to be scripted and executed by the devops team when
performing a database backup.</p></div>
<h2 id="tip_4_persistent_volume_matching">2.4. Tip 4: Persistent Volume Matching</h2>
<div class="paragraph"><p>Restoring a database from an NFS backup requires the building
of a PV which maps to the NFS backup archive path.  For example,
if you have a backup at /backups/pg-foo/2016-01-29:22:34:20
then we create a PV that maps to that NFS path.  We also use
a "label" on the PV so that the specific backup PV can be identified.</p></div>
<div class="paragraph"><p>We use the pod name in the label value to make the PV unique.  This
way, the related PVC can find the right PV to map to and not some other
PV.  In the PVC, we specify the same "label" which lets Kubernetes
match to the correct PV.</p></div>
<h2 id="tip_5_restore_lifecycle">2.5. Tip 5: Restore Lifecycle</h2>
<div class="paragraph"><p>To perform a database restore, we do the following:
 * locate the NFS path to the database backup we want to restore with
 * edit a PV to use that NFS path
 * edit a PV to specify a unique label
 * create the PV
 * edit a PVC to use the previously created PV, specifying the same label
   used in the PV
 * edit a database template, specifying the PVC to be used for mounting
   to the /backup directory in the database pod
 * create the database pod</p></div>
<div class="paragraph"><p>If the /pgdata directory is blank AND the /backup directory contains
a valid postgres backup, it is assumed the user wants to perform a
database restore.</p></div>
<div class="paragraph"><p>The restore logic will copy /backup files to /pgdata before starting
the database.  It will take time for the copying of the files to
occur since this might be a large amount of data and the volumes
might be on slow networks. You can view the logs of the database pod
to measure the copy progress.</p></div>
<h2 id="tip_6_password_mgmt">2.6. Tip 6: Password Mgmt</h2>
<div class="paragraph"><p>Remember that if you do a database restore, you will get
whatever user IDs and passwords that were saved in the
backup.  So, if you do a restore to a new database
and use generated passwords, the new passwords will
not be the same as the passwords stored in the backup!</p></div>
<div class="paragraph"><p>You have various options to deal with managing your
passwords.</p></div>
<div class="ulist"><ul>
<li>
<p>
externalize your passwords using secrets instead of using generated values
</p>
</li>
<li>
<p>
manually update your passwords to your known values after a restore
</p>
</li>
</ul></div>
<div class="paragraph"><p>Note that you can edit the environment variables when there is a <em>dc</em>
using, currently only the slaves have a <em>dc</em> to avoid the possiblity
of creating multiple masters, this might need to change in the future,
to better support password management:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>oc env dc/pg-master-rc PG_MASTER_PASSWORD=foo PG_MASTER=user1</pre>
</div></div>
<h2 id="tip_7_log_aggregation">2.7. Tip 7: Log Aggregation</h2>
<div class="paragraph"><p>Openshift can be configured to include the EFK stack for log aggregation.
Openshift Administrators can configure the EFK stack as documented
here:</p></div>
<div class="paragraph"><p><a href="https://docs.openshift.com/enterprise/3.1/install_config/aggregate_logging.html">https://docs.openshift.com/enterprise/3.1/install_config/aggregate_logging.html</a></p></div>
<h2 id="tip_8_nss_wrapper">2.8. Tip 8: nss_wrapper</h2>
<div class="paragraph"><p>If an Openshift deployment requires that random generated UIDs be
supported by containers, the Crunchy containers can be modifed
similar to those located here to support the use of nss_wrapper
to equate the random generated UIDs/GIDs by openshift with
the postgres user:</p></div>
<div class="paragraph"><p><a href="https://github.com/openshift/postgresql/blob/master/9.4/root/usr/share/container-scripts/postgresql/common.sh">https://github.com/openshift/postgresql/blob/master/9.4/root/usr/share/container-scripts/postgresql/common.sh</a></p></div>
<h2 id="tip_9_build_box_setup">2.9. Tip 9: build box setup</h2>
<div class="paragraph"><p>golang is required to build the pgbadger container, on RH 7.2, golang
is found in the <em>server optional</em> repository and needs to be enabled
to install.</p></div>
<div class="paragraph"><p>golang is required to build the pgbadger container, on RH 7.2, golang
is found in the <em>server optional</em> repository and needs to be enabled
to install.</p></div>
<h2 id="tip_10_encoding_secrets">2.10. Tip 10: encoding secrets</h2>
<div class="paragraph"><p>You can use kubernetes secrets to set and maintain your database
credentials.  Secrets requires you base64 encode your user and password
values as follows:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>echo -n 'myuserid' | base64</pre>
</div></div>
<div class="paragraph"><p>You will paste these values into  your JSON secrets files for values.</p></div>
<h2 id="tip_11_keeping_docker_upgrades_happy">2.11. Tip 11: keeping docker upgrades happy</h2>
<div class="paragraph"><p>docker to be installed.</p></div>
<div class="paragraph"><p>You can keep yum from upgrading docker by including this line
in your /etc/yum.conf file:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>exclude=docker-1.9* docker-selinux-1.9*</pre>
</div></div>
<h2 id="tip_12_dns_configuration_for_openshift_development">2.12. Tip 12: DNS configuration for Openshift development</h2>
<div class="paragraph"><p>Luke Meyer from Redhat wrote an excellent blog on how
to configure dnsmasq and Openshift, it is located here:</p></div>
<div class="paragraph"><p><a href="http://developers.redhat.com/blog/2015/11/19/dns-your-openshift-v3-cluster/">http://developers.redhat.com/blog/2015/11/19/dns-your-openshift-v3-cluster/</a></p></div>
<div class="paragraph"><p>Key things included in this blog are:</p></div>
<div class="ulist"><ul>
<li>
<p>
configuring dhcp to include the local IP address in /etc/resolv.conf upon boot
</p>
</li>
<li>
<p>
configuring dnsmasq
</p>
</li>
<li>
<p>
configuring openshift dns to listen on another port
</p>
</li>
</ul></div>
<div class="paragraph"><p>In my dev setup, I have openshifts DNS listening on 127.0.0.1:8053.
I have my dnsmasq listening on the local IP address 192.168.0.109:53</p></div>
<div class="paragraph"><p>Therefore in my /etc/dhcp/dhclient.conf I have this config:</p></div>
<div class="literalblock">
<div class="content monospaced">
<pre>prepend domain-name-servers 192.168.0.109;</pre>
</div></div>
<div class="paragraph"><p>If you dont have your DNS configured correctly, replication controllers
and deployment configs basically will not work.</p></div>
</div>
<div class="section">
    <h1 class="page-header" id="legal_notices">3. Legal Notices</h1>
<div class="paragraph"><p>Copyright Â© 2016 Crunchy Data Solutions, Inc.</p></div>
<div class="paragraph"><p>CRUNCHY DATA SOLUTIONS, INC. PROVIDES THIS GUIDE "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF NON INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.</p></div>
<div class="paragraph"><p>Crunchy, Crunchy Data Solutions, Inc. and the Crunchy Hippo Logo are trademarks of Crunchy Data Solutions, Inc.</p></div>
</div>
        </div>  <!-- /.col-md-9 -->
        <div class="col-md-3">
        <div id="sidebar">
    <div class="toc2">
<div class="panel panel-default">
<div class="panel-heading">Table of Contents</div>
<div class="panel-body" id="toc">
</div>
</div>
    </div>
</div>
        </div>  <!-- /.col-md-3 -->
    </div>  <!-- /.row -->

  </div>  <!-- /.container -->

    <footer id="footer" role="contentinfo">
        <div class="container">
<div class="row"><div id="footnotes"></div></div>
        <table>
        <tr>
        <td><b>Crunchy Data Solutions, Inc.</b></td>
        <td>v1.0.3</td>
        <td>February 27, 2016</td>
        </tr>
        </table>
        </div>
    </footer>

    <script src="./javascripts/jquery.min.js"></script>
    <script src="./bootstrap/js/bootstrap.min.js"></script>
    <script src="./javascripts/asciidoc.js"></script>
    <!-- Install TOC and/or footnotes (if necessary) -->
    <script type="text/javascript">asciidoc.install(2);</script>

    <script src="./javascripts/jquery.ui.totop.min.js"></script>



    <!-- Remove footnotes if empty block -->
    <script type="text/javascript">$(function(){ if ($("#footnotes div").length == 0) $("#footnotes").parent().remove(); });</script>

    <script type="text/javascript">$(function(){ if ($("#dropdown-menu-versions")) $("#dropdown-menu-versions").parent().remove(); });</script>

    <script type="text/javascript">$(function(){ $().UItoTop(); });</script>
    </div> <!-- page -->
  </body>
</html>
