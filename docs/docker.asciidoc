= Docker Examples - Crunchy Containers for PostgreSQL
Crunchy Data Solutions, Inc.
v1.4.1, {docdate}
:title-logo-image: image:crunchy_logo.png["CrunchyData Logo",align="center",scaledwidth="80%"]

== Docker Examples

Examples of running the Crunchy containers in a pure Docker
environment are found in the $CCPROOT/examples/docker directory.
Those examples are explained below.  The examples make use of
the local Docker volume driver and data volumes created
by the *docker create volume* command.

=== *basic* - Running a single database container

Create the container with this command:
....
cd $CCPROOT/examples/docker/basic
./run.sh
....

This script will do the following:

 * create a data volume to persist PostgreSQL data files to
 * start up a container named *basic*
 * initialize the database using the passed in environment variables, building a user, database, schema, and table based on the environment variable values.
 * maps the PostgreSQL port of 5432 in the container to your local host port of 12000.

 To start the instance, run the following commands:
 ....
 docker start basic
 ....

The container creates a default database called *userdb*, a default
user called *testuser* with a default password of *password*, you can
use this to connect from your local host as follows:
....
psql -h localhost -p 12000 -U testuser -W userdb
....

To shut down the instance, run the following commands:
....
docker stop basic
....

=== *master-replica* - Creating a master and replica database cluster

Run the container with this command:
....
cd $CCPROOT/examples/docker/master-replica
./run.sh
....

This script will do the following:

 * create a docker volume using the local driver for the master
 * create a docker volume using the local driver for the replica
 * start up a container named master binding to port 12000
 * start up a container named replica binding to port 12002
 * initialize the database using the passed in environment variables, building a user, database, schema, and table based on the environment variable values.
 * maps the PostgreSQL port of 5432 in the container to your local host port of 12000.

To start the instance, run the following commands:
....
docker start master replica
....

The container creates a default database called *userdb*, a default
user called *testuser* with a default password of *password*, you can
use this to connect from your local host as follows:
....
psql -h localhost -p 12007 -U testuser -W userdb
psql -h localhost -p 12008 -U testuser -W userdb
....

To shut down the instance, run the following commands:
....
docker stop master replica
....

=== *backup* - Performing a backup

In order to run this backup script, you first need to edit
run.sh to specify your host IP address you are running
on.  The script assumes you are going to backup the *basic*
container created in the first example, so you need to ensure
that container is running.

Run the backup with this command:
....
cd $CCPROOT/examples/docker/backup
./run.sh
....

This script will do the following:

 * start up a backup container named basicbackup
 * run pg_basebackup on the container named master
 * store the backup in /tmp/backups/master directory
 * exit after the backup

=== *restore* - Performing a restore

In order to run this backup script, you first need to edit
run.sh to specify your host IP address you are running
on.  The script assumes you are going to backup the container
created in Example 2.

Run the backup with this command:
....
cd $CCPROOT/examples/docker/restore
./run.sh
....

This script will do the following:

 * start up a container named master-restore
 * copy the backup files from Example 3 into /pgdata
 * start up the container using the backup files
 * maps the PostgreSQL port of 5432 in the container to your local host port of 12001 as to not conflict with the master running in the previous example.

=== *crunchy-proxy* - a smart proxy for Postgres

A *crunchy-proxy* example is provided that will run a  container that
is configured to be used with the master and replica example provided
in the *master-replica* example.

You can create the proxy by running:
....
cd $CCPROOT/examples/docker/crunchy-proxy
./run.sh
....

This proxy will listen on localhost:12432.  You can access the
*master-replica* cluster by:
....
psql -h localhost -p 12432 -U postgres postgres
....

See this link for details on the *crunchy-proxy*:
https://github.com/CrunchyData/crunchy-proxy

You might consider *crunchy-proxy* over pgpool and pgbouncer if
you need load-balancing and smart SQL routing.


=== *pgpool* - pgpool

A pgpool example is provided that will run a pgpool container that
is configured to be used with the master and replica example provided
in the *master-replica* example.  After running
those commands to create a master and replica, you can
create a pgpool container by running the following example command:

....
cd $CCPROOT/examples/docker/pgpool
./run.sh
....

Enter the following command to connect to the pgpool that is
mapped to your local port 12003:
....
psql -h localhost -U testuser -p 12003 userdb
....

You will enter the password of *password* when prompted.  At this point
you can execute both INSERT and SELECT statements on the pgpool connection.
Pgpool will direct INSERT statements to the master and SELECT statements
will be sent round-robin to both master and replica.


=== *badger* - pgbadger

A pgbadger example is provided that will run a HTTP server that
when invoked, will generate a pgbadger report on a given database.

pgbadger reads the log files from a database to product an HTML report
that shows various Postgres statistics and graphs.

To run the example, modify the run-badger.sh script to refer to the
Docker container that you want to run pgbadger against, also referring
to the container's data directory, then run the example as follows:
....
cd $CCPROOT/examples/docker/badger
./run.sh
....

After execution, the container will run and provide a simple HTTP
command you can browse to view the report.  As you run queries against
the database, you can invoke this URL to generate updated reports:
....
curl http://127.0.0.1:14000/api/badgergenerate
....

=== *metrics* - metrics collection

You can collect various Postgres metrics from your database
container by running a crunchy-collect container that points
to your database container.

Metrics collection requires you run the crunchy 'scope' set of containers
that includes:

 * prometheus
 * prometheus push gateway
 * grafana

To start this set of containers, run the following:
....
cd $CCPROOT/examples/docker/metrics
./run.sh
....

These metrics are described in this link:/docs/metrics.asciidoc[document.]

An example has been provided that runs a database container
and also the associated metrics collection container, run the
example as follows:

....
cd $CCPROOT/examples/docker/collect
./run.sh
....

Every 3 minutes the collection container will collect postgres
metrics and push them to the crunchy prometheus database.  You
can graph them using the crunchy grafana container.

=== *vacuum* - vacuum

You can perform a postgres vacuum command by running the crunchy-vacuum
container.  You specify a database to vacuum using environment variables.

An example is shown in the $CCPROOT/examples/docker/vacuum/run.sh script
and can be run as follows:
....
cd $CCPROOT/examples/docker/vacuum
./run.sh
....

This example performs a vacuum on a single table in the master postgres
database.  Vacuum is controlled via the following environment variables:

 * VAC_FULL - when set to true adds the FULL parameter to the VACUUM command
 * VAC_TABLE - when set, allows you to specify a single table to vacuum, when
 not specified, the entire database tables are vacuumed
 * JOB_HOST - required variable is the postgres host we connect to
 * PG_USER - required variable is the postgres user we connect with
 * PG_DATABASE - required variable is the postgres database we connect to
 * PG_PASSWORD - required variable is the postgres user password we connect with
 * PG_PORT - allows you to override the default value of 5432
 * VAC_ANALYZE - when set to true adds the ANALYZE parameter to the VACUUM command
 * VAC_VERBOSE - when set to true adds the VERBOSE parameter to the VACUUM command
 * VAC_FREEZE - when set to true adds the FREEZE parameter to the VACUUM command

=== *custom-setup*- custom setup.sql

You can use your own version of the setup.sql SQL file to customize
the initialization of database data and objects when the container and
database are created.

An example is shown in the $CCPROOT/examples/docker/custom-setup/run.sh script
and can be run as follows:

....
cd $CCPROOT/examples/docker/custom-setup
./run.sh
....

This works by placing a file named, setup.sql, within the /pgconf mounted volume
directory.  Portions of the setup.sql file are required for the crunchy container
to work, see comments within the sample setup.sql file.

=== *pgbouncer* - pgbouncer

The pgbouncer utility can be used to provide a connection pool
to postgres databases.  The crunchy-pgbouncer container also
contains logic that lets it perform a failover from a master
to a slave database.

To test this failover, you first create a running master/slave
cluster as follows:

....
cd $CCPROOT/examples/docker/master-replica
./run.sh
....

An example is shown in the $CCPROOT/examples/docker/pgbouncer/run.sh script
and can be run as follows:

....
cd $CCPROOT/examples/docker/pgbouncer
./run.sh
....

This example configures pgbouncer to provide connection pooling
for the master and pg-replica databases.  It also sets the FAILOVER
environment variable which will cause a failover to be triggered
if the master database can not be reached.

To trigger the failover, stop the master database:

....
docker stop master
....

At this point, the pgbouncer will notice that the master is not reachable
and touch the trigger file on the configured slave database to start
the failover.  The pgbouncer container will then reconfigure
pgbouncer to relabel the slave database into the master database so clients
to pgbouncer will be able to connect to the master as before the failover.

To just log into the database from the pgbouncer connection pool
you would enter the following using the password "password":
....
psql -h localhost -p 12005 -U testuser master
....



=== *sync* - synchronous replication

This example, $CCPROOT/examples/docker/sync, provides a
streaming replication configuration that includes both
synchronous and asynchronous slaves.

To run this example, run the following:

....
cd $CCPROOT/examples/docker/sync
./run.sh
....

You can test the replication status on the master by using the following command
and the password "password":
....
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'table pg_stat_replication'
....

You should see 2 rows, 1 for the async slave and 1 for the sync slave.  The
sync_state column shows values of async or sync.

You can test replication to the slaves by entering some data on
the master like this, and then querying the slaves for that data:
....
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'create table foo (id int)'
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'insert into foo values (1)'
psql -h 127.0.0.1 -p 12002 -U postgres postgres -c 'table foo'
psql -h 127.0.0.1 -p 12003 -U postgres postgres -c 'table foo'
....


=== *pgadmin* - pgadmin4

This example, $CCPROOT/examples/docker/pgadmin4, provides a
container that runs the pgadmin4 web application.

To run this example, run the following:

....
cd $CCPROOT/examples/docker/pgadmin4
./run.sh
....

You should now be able to browse to http://YOURLOCALIP:5050
and log into the web application using a user ID of *admin@admin.org*
and password of *password*.  Replace YOURLOCALIP with whatever
your local IP address happens to be.


=== *pitr* - PITR (point in time recovery)

This example, $CCPROOT/examples/docker/pitr, provides an
example of performing a point in time recovery.

To run this example, run the following to create a
database container:

....
cd $CCPROOT/examples/docker/pitr
./run-master-pitr.sh
....

It takes about 1 minute for the database to become ready
for use after initially starting.

This database is created with the ARCHIVE_MODE and ARCHIVE_TIMEOUT
environment variables set.  See the pitr.asciidoc for more details
on these settings.  Warning:  this example writes the WAL segment
files to the /tmp directory...running it for a long time could
fill up your /tmp!

Next, we will create a base backup of that database using
this:
....
./run-master-pitr-backup.sh
....

At this point, WAL segment files are created every 60 seconds that
contain any database changes.  These segments are stored in
the /tmp/master-data/master-wal directory.

Next, create some data in your database using this command:
....
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('beforechanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'create table pitrtest (id int)'
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('afterchanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('nomorechanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "checkpoint"
....

Next, stop the database to avoid conflicts with the WAL files while
attempting to do a restore from them:
....
docker stop master-pitr
....

The commands above set restore point labels which we can
use to mark the points in the recovery process we want to
reference when creating our restored database.  Points before
and after the test table were made.

Next, lets edit the restore script to use the base backup files
created in the step above.  You can view the backup path name
under /tmp/backups/master-pitr directory.  You will see a value like
*2016-09-21-21-03-29*.  Copy and paste that value into the
run-restore-pitr.sh script in the *BACKUP* environment variable.
....
vi ./run-restore-pitr.sh
....

Next, lets see if we can restore the database before we created the
test table in the last command, we will restore using the backup and
will use the *beforechanges* label as the restore target name in the PITR:
....
./run-restore-pitr.sh
....

The WAL segments are read and applied when restoring from the database
backup.  At this point, you should be able to verify that the
database was restored to the point before creating the test table:
....
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
....

This sql command should show that the pitrtest table does not exist
at this recovery time.

PostgreSQL allows you to pause the recovery process if the target name
or time is specified.  This pause would allow a DBA a chance to review
the recovery time/name and see if this is what they want or expect.  If so,
the DBA can run the following command to resume and complete the recovery:
....
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_xlog_replay_resume()'
....

Until you run the statement above, the database will be left in read-only
mode.

Next, run the script to restore the database
to the *afterchanges* restore point, do this by updating the
RECOVERY_TARGET_NAME to *afterchanges*:
....
vi ./run-restore-pitr.sh
./run-restore-pitr.sh
....


After this restore, you should be able to see the test table:
....
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_xlog_replay_resume()'
....


Lastly, lets recovery using all of the WAL files, this will get the
restored database as current as possible, edit the script
to remove the RECOVERY_TARGET_NAME environment setting completely:
....
./run-restore-pitr.sh
sleep 30
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'create table foo (id int)'
....

At this point, you should be able to create new data in the restored database
and the test table should be present.  When you recover the entire
WAL history, resuming the recovery is not necessary to enable writes.

Other options exist for performing a PITR, see the pitr.asciidoc for
full details.


=== *pgaudit* - pgaudit

This example, $CCPROOT/examples/docker/pgaudit, provides an
example of enabling pgaudit output.  As of release 1.3,
pgaudit is included in the crunchy-postgres container and is
added to the postgres shared library list in the postgresql.conf.

Given the numerous ways pgaudit can be configured, the exact
pgaudit configuration is left to the user to define.  pgaudit
allows you to configure auditing rules either in postgresql.conf
or within your SQL script.

For this test, we place pgaudit statements within a SQL script
and verify that auditing is enabled and working.  If you choose
to configure pgaudit via a postgresql.conf file, then you will
need to define your own custom postgresql.conf file and mount
it to override the default postgresql.conf file.

To run this example, run the following to create a
database container:

....
cd $CCPROOT/examples/docker/pgaudit
./run.sh
....

This starts a database on port 12005 on localhost.  You can then
run the test script as follows:
....
./test-pgaudit.sh
....

This test executes a SQL file which contains pgaudit configuration
statements as well as executes some basic SQL commands.  These
SQL commands will cause pgaudit to create audit log messages in
the pg_log log file created by the database container.

=== *swarm* - docker swarm

This example shows how to run a master and replica database
container on a Docker Swarm (v.1.12) cluster.

First, set up a cluster, the Kubernetes libvirt coreos cluster
example works well, see link:http://kubernetes.io/docs/getting-started-guides/libvirt-coreos/[coreos-libvirt-cluster.]

Next, on each node, create the Swarm using these
link:https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/[Swarm Install instructions.]

Includes the command on the manager node:
....
docker swarm init --advertise-addr 192.168.10.1
....

Then the command on all the worker nodes:
....
 docker swarm join \
     --token SWMTKN-1-65cn5wa1qv76l8l45uvlsbprogyhlprjpn27p1qxjwqmncn37o-015egopg4jhtbmlu04faon82u \
         192.168.10.1.37
....

Before creating Swarm services, for service discovery you need
to define an overlay network to be used by the services you will
create.  Create the network like this:
....
docker network create --driver overlay crunchynet
....

We want to have the master database always placed on
a specific node. This is accomplished using node constraints
as follows:
....
docker node inspect kubernetes-node-1 | grep ID
docker node update --label-add type=master 18yrb7m650umx738rtevojpqy
....

In the above example, the kubernetes-node-1 node with ID 18yrb7m650umx738rtevojpqy has a user defined label of *master* added to it.  The master service
specifies *master* as a constraint when created; this tells Swarm
to place the service on that specific node.  The replica specifies
a constraint of *node.labels.type != master* to have the replica
always placed on a node that is not hosting the master service.


After you set up the Swarm cluster, you can then
run the *$CCPROOT/examples/docker/swarm-service* example as follows
on the *Swarm Manager Node*:

....
cd $CCPROOT/examples/docker/swarm-service
./run.sh
....

You can then find the nodes that are running the master and replica containers
by:
....
docker service ps master
docker service ps replica
....

Given the PostgreSQL replica service is named *replica*, you can scale up
the number of replica containers by running this command:
....
docker service scale replica=2
docker service ls
....

You can verify you have two replicas within PostgreSQL by viewing
the *pg_stat_replication* table, the password is *password*, when
logged into the kubernetes-node-1 host:
....
docker exec -it $(docker ps -q) psql -U postgres -c 'table pg_stat_replication' postgres
....

You should see a row for each replica along with its replication status.


=== *watch* - automated failover

This example shows how to run the crunchy-watch container
to perform an automated failover.  For the example to
work, the host on which you are running needs to allow
read-write access to /run/docker.sock.  The crunchy-watch
container runs as the postgres user, so adjust the
file permissions of /run/docker.sock accordingly.

Run the example as follows (depends on master-replica example
being run prior):
....
cd $CCPROOT/examples/docker/watch
./run.sh
....

This will start the watch container which tests every few seconds
whether the master database is running, if not, it will
trigger a failover (using docker exec) on the replica host.

Test it out by stopping the master:
....
docker stop master
docker logs watch
....

Look at the watch container logs to see it perform the failover.

=== Tips - Send a signal to postgres

First, find the PID of the postmaster:

....
docker exec -it master cat /pgdata/master/postmaster.pid
....

Then, send it the signal to kill it or other signal depending on what you want to do:

....
docker exec -it master kill -SIGTERM 22
....


== Legal Notices

Copyright Â© 2017 Crunchy Data Solutions, Inc.

CRUNCHY DATA SOLUTIONS, INC. PROVIDES THIS GUIDE "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF NON INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.

Crunchy, Crunchy Data Solutions, Inc. and the Crunchy Hippo Logo are trademarks of Crunchy Data Solutions, Inc.
