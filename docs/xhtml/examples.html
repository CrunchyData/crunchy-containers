<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Examples &amp; Use Cases - Crunchy Containers for PostgreSQL</title><link rel="stylesheet" type="text/css" href="docbook-xsl.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /></head><body><div xml:lang="en" class="article" lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="idm140487069282880"></a>Examples &amp; Use Cases - Crunchy Containers for PostgreSQL</h2></div><div><div class="author"><h3 class="author"><span class="firstname">Crunchy Data Solutions, Inc.</span></h3></div></div></div><hr /></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="section"><a href="#_container_examples">1. Container Examples</a></span></dt><dd><dl><dt><span class="section"><a href="#_basic_running_a_single_database_container">1.1. basic - Running a single database container</a></span></dt><dt><span class="section"><a href="#_master_replica_creating_a_master_and_replica_database_cluster">1.2. master-replica - Creating a master and replica database cluster</a></span></dt><dt><span class="section"><a href="#_master_replica_dc_master_and_scaling_replica_example">1.3. master-replica-dc - Master and scaling replica example</a></span></dt><dt><span class="section"><a href="#_master_replica_rc_pvc_master_replica_deployment_using_pvc">1.4. master-replica-rc-pvc - Master/replica deployment using pvc</a></span></dt><dt><span class="section"><a href="#_master_deployment_deploy_a_master_and_replica_deployment">1.5. master-deployment - Deploy a master and replica deployment</a></span></dt><dt><span class="section"><a href="#_master_using_gluster_fs">1.6. master using gluster fs</a></span></dt><dt><span class="section"><a href="#_master_pvc_master_using_pvc">1.7. master-pvc - Master using PVC</a></span></dt><dt><span class="section"><a href="#_backup_performing_a_backup">1.8. backup - Performing a backup</a></span></dt><dt><span class="section"><a href="#_master_restore_restoration_of_backup_example">1.9. master-restore - Restoration of backup example</a></span></dt><dt><span class="section"><a href="#_failover_manual_database_failover">1.10. failover - Manual database failover</a></span></dt><dt><span class="section"><a href="#_pgbackrest_backup_utility">1.11. pgbackrest - Backup Utility</a></span></dt><dt><span class="section"><a href="#_pgbackrest_restore_restoring_backups_made_with_pgbackrest">1.12. pgbackrest restore - Restoring backups made with pgbackrest</a></span></dt><dt><span class="section"><a href="#_pgadmin4_postgresql_web_user_interface">1.13. pgadmin4 - PostgreSQL web user interface</a></span></dt><dt><span class="section"><a href="#_crunchy_proxy_a_smart_proxy_for_postgresql">1.14. crunchy-proxy - A smart proxy for PostgreSQL</a></span></dt><dt><span class="section"><a href="#_custom_config_custom_configuration_files">1.15. custom-config - Custom Configuration Files</a></span></dt><dt><span class="section"><a href="#_custom_config_sync_custom_configuration_files_with_sync_replica">1.16. custom-config sync - Custom Configuration Files with Sync Replica</a></span></dt><dt><span class="section"><a href="#_configmap_database_credentials_from_a_configmap">1.17. configmap - database credentials from a configmap</a></span></dt><dt><span class="section"><a href="#_pgpool_pgpool_pod_example">1.18. pgpool - pgpool pod example</a></span></dt><dt><span class="section"><a href="#_pgbadger_pgbadger_pod">1.19. pgbadger - pgbadger pod</a></span></dt><dt><span class="section"><a href="#_metrics_amp_collect_metrics_collection">1.20. metrics &amp; collect - Metrics collection</a></span></dt><dt><span class="section"><a href="#_vacuum_job_postgresql_vacuum_command">1.21. vacuum-job - PostgreSQL vacuum command</a></span></dt><dt><span class="section"><a href="#_crunchy_dba_cron_scheduler_for_simple_dba_tasks">1.22. crunchy-dba - cron scheduler for simple DBA tasks</a></span></dt><dt><span class="section"><a href="#_custom_setup_custom_setup_sql">1.23. custom-setup - Custom setup.sql</a></span></dt><dt><span class="section"><a href="#_pgbouncer">1.24. pgbouncer</a></span></dt><dt><span class="section"><a href="#_sync_synchronous_replication">1.25. sync - Synchronous replication</a></span></dt><dt><span class="section"><a href="#_statefulsets">1.26. statefulsets</a></span></dt><dt><span class="section"><a href="#_statefulset_using_dynamic_provisioning">1.27. statefulset using dynamic provisioning</a></span></dt><dt><span class="section"><a href="#_secret">1.28. secret</a></span></dt><dt><span class="section"><a href="#_pitr_pitr_point_in_time_recovery">1.29. pitr - PITR (point in time recovery)</a></span></dt><dt><span class="section"><a href="#_pgaudit_pgaudit">1.30. pgaudit - PGAudit</a></span></dt><dt><span class="section"><a href="#_swarm_docker_swarm">1.31. swarm - Docker swarm</a></span></dt><dt><span class="section"><a href="#_watch_automated_failover">1.32. watch - Automated failover</a></span></dt><dt><span class="section"><a href="#_ssl_postgresql_ssl_auth_method">1.33. ssl - PostgreSQL ssl auth method</a></span></dt><dt><span class="section"><a href="#_workshop_openshift_workshop">1.34. workshop - OpenShift Workshop</a></span></dt><dt><span class="section"><a href="#_upgrade_pg_upgrade">1.35. upgrade - pg_upgrade</a></span></dt><dt><span class="section"><a href="#_google_cloud_environment_kubernetes_cluster">1.36. google cloud environment - Kubernetes cluster</a></span></dt></dl></dd><dt><span class="section"><a href="#_docker_tips">2. Docker - Tips</a></span></dt><dt><span class="section"><a href="#_openshift_tips">3. OpenShift - Tips</a></span></dt></dl></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_container_examples"></a>1. Container Examples</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_basic_running_a_single_database_container"></a>1.1. basic - Running a single database container</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker"></a>Docker</h4></div></div></div><p>Create the container with this command:</p><pre class="literallayout">cd $CCPROOT/examples/docker/basic
./run.sh</pre><p>This script will do the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
create a data volume to persist PostgreSQL data files to
</li><li class="listitem">
start up a container named <span class="strong"><strong>basic</strong></span>
</li><li class="listitem">
initialize the database using the passed in environment variables, building a user, database, schema, and table based on the environment variable values.
</li><li class="listitem">
maps the PostgreSQL port of 5432 in the container to your local host port of 12000.
</li></ul></div><p>To start the instance, run the following commands:</p><pre class="literallayout">docker start basic</pre><p>The container creates a default database called <span class="strong"><strong>userdb</strong></span>, a default
user called <span class="strong"><strong>testuser</strong></span> with a default password of <span class="strong"><strong>password</strong></span>, you can
use this to connect from your local host as follows:</p><pre class="literallayout">psql -h localhost -p 12000 -U testuser -W userdb</pre><p>To shut down the instance, run the following commands:</p><pre class="literallayout">docker stop basic</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes"></a>Kubernetes</h4></div></div></div><p>This example starts a single postgres container and service, the most simple
of examples.</p><p>Running the example:</p><pre class="literallayout">examples/kube/basic/run.sh
kubectl get pod basic
kubectl get service basic
kubectl logs basic</pre><p>After the database starts up you can connect to it as follows:</p><pre class="literallayout">psql -h basic -U postgres postgres</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift"></a>OpenShift</h4></div></div></div><p>This OpenShift template will create a single master PostgreSQL instance.</p><p>Running the example:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/basic
./run.sh</pre><p>You can see what passwords were generated by running this command:</p><pre class="literallayout">oc describe pod basic | grep PG</pre><p>You can run the following command to test the database, entering
the value of PG_PASSWORD from above for the password when prompted:</p><pre class="literallayout">psql -h basic.openshift.svc.cluster.local -U testuser userdb</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_master_replica_creating_a_master_and_replica_database_cluster"></a>1.2. master-replica - Creating a master and replica database cluster</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_2"></a>Docker</h4></div></div></div><p>Run the container with this command:</p><pre class="literallayout">cd $CCPROOT/examples/docker/master-replica
./run.sh</pre><p>This script will do the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
create a docker volume using the local driver for the master
</li><li class="listitem">
create a docker volume using the local driver for the replica
</li><li class="listitem">
start up a container named master binding to port 12007
</li><li class="listitem">
start up a container named replica binding to port 12008
</li><li class="listitem">
initialize the database using the passed in environment variables, building a user, database, schema, and table based on the environment variable values.
</li><li class="listitem">
maps the PostgreSQL port of 5432 in the container to your local host port of 12000.
</li></ul></div><p>To start the instance, run the following commands:</p><pre class="literallayout">docker start master replica</pre><p>The container creates a default database called <span class="strong"><strong>userdb</strong></span>, a default
user called <span class="strong"><strong>testuser</strong></span> with a default password of <span class="strong"><strong>password</strong></span>, you can
use this to connect from your local host as follows:</p><pre class="literallayout">psql -h localhost -p 12007 -U testuser -W userdb
psql -h localhost -p 12008 -U testuser -W userdb</pre><p>To shut down the instance, run the following commands:</p><pre class="literallayout">docker stop master replica</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_compose"></a>Docker-Compose</h4></div></div></div><p>This is a <span class="strong"><strong>docker-compose</strong></span> example of deploying master
and read replicas using the crunchy-postgres image from DockerHub.</p><p>To install docker-compose, please follow the instructions located
in the <a class="ulink" href="https://docs.docker.com/compose/install/" target="_top">official Docker documentation</a>.</p><p>You can also access docker-compose by setting up the following Vagrant/VirtualBox
virtualized environment containing CentOS 7, Docker, and docker-compose:
<a class="ulink" href="https://github.com/jasonodonnell/docker-vm" target="_top">https://github.com/jasonodonnell/docker-vm</a></p><p>To deploy this example, run the following commands:</p><pre class="literallayout">cd $CCPROOT/examples/compose/master-replica
docker-compose up</pre><p>Optionally, to deploy more than one replica, run the following:</p><pre class="literallayout">docker-compose up --scale db-replica=3</pre><p>To psql into the created database containers, first identify the ports exposed
on the containers:</p><pre class="literallayout">docker ps</pre><p>Next, using psql, connect to the service:</p><pre class="literallayout">psql -d userdb -h 0.0.0.0 -p &lt;CONTAINER_PORT&gt; -U testuser</pre><p><span class="strong"><strong>Note:</strong></span> See <span class="strong"><strong>PG_PASSWORD</strong></span> in <span class="strong"><strong>docker-compose.yml</strong></span> for the user password.</p><p>To tear down the example, run the following:</p><pre class="literallayout">docker-compose stop
docker-compose rm</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_2"></a>Kubernetes</h4></div></div></div><p>This example starts a master pod, master service, replica pod, and replica
service.  The replica is a replica of the master.  This example uses
emptyDir volumes for persistence.  This example does not allow
you to scale up the replicas.</p><p>Running the example:</p><pre class="literallayout">cd $CCPROOT/examples/kube/master-replica
./run.sh</pre><p>It takes about a minute for the replica to begin replicating with the
master.  To test out replication, see if replication is underway
with this command:</p><pre class="literallayout">psql -h ms-master -U postgres postgres -c 'table pg_stat_replication'</pre><p>If you see a line returned from that query it means the master is replicating
to the replica.  Try creating some data on the master:</p><pre class="literallayout">psql -h ms-master -U postgres postgres -c 'create table foo (id int)'
psql -h ms-master -U postgres postgres -c 'insert into foo values (1)'</pre><p>Then verify that the data is replicated to the replica:</p><pre class="literallayout">psql -h ms-replica -U postgres postgres -c 'table foo'</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_2"></a>OpenShift</h4></div></div></div><p>Run the following command to deploy a master and replica database cluster:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-replica
./run.sh</pre><p>Similarly to the previous example on <span class="strong"><strong>basic</strong></span>, you can view the generated
passwords by running this command:</p><pre class="literallayout">oc describe pod ms-master | grep PG</pre><p>You can then connect to the database instance as follows using the password
shown with the previous command:</p><pre class="literallayout">psql -h ms-master -U testuser -W userdb</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_master_replica_dc_master_and_scaling_replica_example"></a>1.3. master-replica-dc - Master and scaling replica example</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_3"></a>Kubernetes</h4></div></div></div><p>This example starts a master pod, master service, replica pod, and replica
service.  The replica is a replica of the master.  This example uses
emptyDir volumes for persistence.  This example runs the replicas in a
Deployment.  A deployment controller lets you scale up the replicas and
create an initial replica set.</p><p>Running the example:</p><pre class="literallayout">examples/kube/master-replica-dc/run.sh</pre><p>You can insert data in the master and make sure it replicates to
the replicas using the commands from Example 2 above.  Replace
<span class="strong"><strong>master</strong></span> with the <span class="strong"><strong>master-dc</strong></span> name and <span class="strong"><strong>replica</strong></span> with <span class="strong"><strong>replica-dc</strong></span>.</p><p>This example creates 2 replicas when it initially starts.  To scale
up the number of replicas, run this command:</p><pre class="literallayout">kubectl get deployment
kubectl scale --current-replicas=2 --replicas=3 deployment/replica-dc
kubectl get deployment
kubectl get pod</pre><p>You can verify that you now have 3 replicas by running this query
on the master:</p><pre class="literallayout">psql -h master-dc -U postgres postgres -c 'table pg_stat_replication'</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_3"></a>OpenShift</h4></div></div></div><p>This example is similar to the previous examples but
builds a master pod, and a single replica that can be scaled up
using a replication controller.   The master is implemented as
a single pod since it can not be scaled like read-only replicas.</p><p>Running the example:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-replica-dc
./run.sh</pre><p>Connect to the PostgreSQL instances with the following:</p><pre class="literallayout">psql -h master-dc.pgproject.svc.cluster.local -U testuser userdb
psql -h replica-dc.pgproject.svc.cluster.local -U testuser userdb</pre><p>Here is an example of increasing or scaling up the Postgres <span class="emphasis"><em>replica</em></span> pods to 2:</p><pre class="literallayout">oc scale rc replica-dc-1 --replicas=2</pre><p>To check the <span class="strong"><strong>master</strong></span> default password, enter the following command and look for
the <span class="strong"><strong>PG_MASTER_USER</strong></span> and <span class="strong"><strong>PG_MASTER_PASSWORD</strong></span> variables:</p><pre class="literallayout">oc describe pod master-dc | grep PG</pre><p>Enter the following commands to verify the PostgreSQL replication is working, using
the password for master found with the previous command.</p><pre class="literallayout">psql -c 'table pg_stat_replication' -h master-dc.pgproject.svc.cluster.local -U master postgres
psql -h replica-dc.pgproject.svc.cluster.local -U master postgres</pre><p>You can see that the replica service is load balancing between
multiple replicas by running a command as follows, run the command
multiple times and the ip address should alternate between
the replicas:</p><pre class="literallayout">psql -h replica-dc -U postgres postgres -c 'select inet_server_addr()'</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_master_replica_rc_pvc_master_replica_deployment_using_pvc"></a>1.4. master-replica-rc-pvc - Master/replica deployment using pvc</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_4"></a>OpenShift</h4></div></div></div><p>This example uses a pvc based volume for the master and the replicas.  In
some scenarios, customers might want to have all the Postgres
instances using NFS volumes for persistence.</p><p>To run the example, follow these steps:</p><p>As the project user, create the master replica deployment:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-replica-rc-pvc
./run.sh</pre><p>Note:  The <span class="strong"><strong>master-replica.json</strong></span> file creates the master and replica deployment,
creating pods and services where the replica is controlled by a Replication Controller,
allowing you to scale up the replicas.</p><p>If you examine your NFS directory, you will see postgres data directories
created and used by your master and replica pods.</p><p>Next, add some test data to the master:</p><pre class="literallayout">psql -c 'create table testtable (id int)' -U master -h m-s-rc-pvc-master postgres
psql -c 'insert into testtable values (123)' -U master -h m-s-rc-pvc-master postgres</pre><p>Next, add a new replica:</p><pre class="literallayout">oc scale rc m-s-rc-pvc-replica-1 --replicas=2</pre><p>At this point, you should see the new NFS directory created by the new
replica pod, and you should also be able to test that replication is
working on the new replica:</p><pre class="literallayout">psql -c 'table testtable' -U master -h m-s-rc-pvc-replica postgres</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_master_deployment_deploy_a_master_and_replica_deployment"></a>1.5. master-deployment - Deploy a master and replica deployment</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_4"></a>Kubernetes</h4></div></div></div><p>Starting in release 1.2.8, the postgres container can accept
an environment variable named PGDATA_PATH_OVERRIDE.  If set,
the /pgdata/subdir path will use a path subdir name of your
choosing instead of the default which is the hostname of the container.</p><p>This example shows how a Deployment of a master postgres is
supported.  A pod is a deployment uses a hostname generated by
Kubernetes, so if you want to restart the master pod, you will
get a different hostname as defined by the Deployment.  For
finding the /pgdata that pertains to the pod, you will need
to specify a /pgdata/subdir name that never changes, and that
is the purpose of the PGDATA_PATH_OVERRIDE env var.</p><p>Start the example as follows:</p><pre class="literallayout">cd $CCPROOT/examples/kube/master-deployment
./run.sh</pre><p>This will create the following in your Kube environment:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
create a master-dc service, uses a PVC to persist postgres data
</li><li class="listitem">
create a replica-dc service, uses emptyDir persistence
</li><li class="listitem">
create a master-dc Deployment of replica count 1 for the master
   postgres database pod
</li><li class="listitem">
create a replica-dc Deployment of replica count 1 for the replica
</li><li class="listitem">
create a replica2-dc Deployment of replica count 1 for the 2nd replica
</li><li class="listitem">
create a ConfigMap to hold a custom postgresql.conf, setup.sql, and
   pg_hba.conf files
</li><li class="listitem">
create secrets for the master user, superuser, and normal user to
   hold the passwords
</li><li class="listitem">
create a volume mount for /pgbackrest and /pgwal
</li></ul></div><p>The persisted master postgres data is found under /pgdata/master-dc.
If you delete the master pod, the Deployment will create another
pod for the master, and will be able to start up immediately since
we are using the same /pgdata/master-dc data directory.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_5"></a>OpenShift</h4></div></div></div><p>Start by running the example:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-deployment
./run.sh</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_master_using_gluster_fs"></a>1.6. master using gluster fs</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_5"></a>Kubernetes</h4></div></div></div><p>This example deploys a master database container that uses
a gluster file system as the persistent volume.</p><p>Set up gluster according to
<a class="ulink" href="https://wiki.centos.org/SpecialInterestGroup/Storage/gluster-Quickstart" target="_top">https://wiki.centos.org/SpecialInterestGroup/Storage/gluster-Quickstart</a></p><p>Start the example as follows:</p><pre class="literallayout">cd $CCPROOT/examples/kube/gluster
./run.sh</pre><p>This will start a container and service for the master database.</p><p>You can access the master database as follows:</p><pre class="literallayout">psql -h master-gluster -U postgres postgres</pre><p>This example has a mount point of /mnt/gluster which is mapped
to the gluster fs at yourhost:/gv0</p><p><span class="strong"><strong>Tip</strong></span></p><p>Create a static route from your host to 10.0.0.0/16 if you
want to test the user interfaces of the metrics tools.</p><p>On my host, 114, and my bridge, br1, this worked for me:</p><pre class="literallayout"> ip route add 10.0.0.0/16 via 192.168.0.114 dev br1</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_master_pvc_master_using_pvc"></a>1.7. master-pvc - Master using PVC</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_6"></a>OpenShift</h4></div></div></div><p>This example will create a single master postgres pod that is using
an PVC based volume to store the postgres data files.</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-pvc
./run.sh</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_backup_performing_a_backup"></a>1.8. backup - Performing a backup</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_3"></a>Docker</h4></div></div></div><p>In order to run this backup script, you first need to edit
run.sh to specify your host IP address you are running
on.  The script assumes you are going to backup the <span class="strong"><strong>basic</strong></span>
container created in the first example, so you need to ensure
that container is running.</p><p>Run the backup with this command:</p><pre class="literallayout">cd $CCPROOT/examples/docker/backup
./run.sh</pre><p>This script will do the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
start up a backup container named basicbackup
</li><li class="listitem">
run pg_basebackup on the container named master
</li><li class="listitem">
store the backup in /tmp/backups/master directory
</li><li class="listitem">
exit after the backup
</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_6"></a>Kubernetes</h4></div></div></div><p>This example depends on the basic example being run prior to
this example!</p><p>This example performs a database backup on the basic database.
The backup is stored in the /nfsfileshare backup path which is also
a dependency.  See the installation docs on how to set up the NFS
server on this host.</p><p>Running the example:</p><pre class="literallayout">examples/kube/backup-job/run.sh</pre><p>Things to point out with this example include its use of persistent
volumes and volume claims to store the backup data files to
an NFS server.</p><p>You can view the persistent volume information as follows:</p><pre class="literallayout">kubectl get pvc
kubectl get pv</pre><p>The Kube Job type executes a pod and then the pod exits.  You can
view the Job status using this command:</p><pre class="literallayout">kubectl get job</pre><p>While the backup pod is running, you can view the pod as follows:</p><pre class="literallayout">kubectl get pod</pre><p>You should find the backup archive in this location:</p><pre class="literallayout">ls /nfsfileshare/basic</pre><p><span class="strong"><strong>Tip</strong></span></p><p>You can view the backup pod log using the <span class="strong"><strong>docker logs</strong></span> command
on the exited container. Use <span class="strong"><strong>docker ps -a | grep backup</strong></span> to
locate the container.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_7"></a>OpenShift</h4></div></div></div><p>This example assumes you have a database pod running called <span class="strong"><strong>basic</strong></span>
as created by the <span class="strong"><strong>basic</strong></span> example and that you have configured NFS as described
in the <a class="ulink" href="install.adoc" target="_top">installation documentation</a>.</p><p>You can perform a database backup by executing the following
step:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/backup-job
./run.sh</pre><p>A successful backup will perform pg_basebackup on the pg-master and store
the backup in the NFS mounted volume under a directory named pg-master, each
backup will be stored in a subdirectory with a timestamp as the name.  This
allows any number of backups to be kept.</p><p>The <span class="strong"><strong>examples/openshift/crunchy-pv-backup.json</strong></span> specifies a <span class="strong"><strong>persistentVolumeReclaimPolicy</strong></span> of <span class="strong"><strong>Retain</strong></span> to tell OpenShift
that we want to keep the volume contents after the removal of the PV.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_master_restore_restoration_of_backup_example"></a>1.9. master-restore - Restoration of backup example</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_4"></a>Docker</h4></div></div></div><p>In order to run this backup script, you first need to edit
run.sh to specify your host IP address you are running
on.  The script assumes you are going to backup the container
created in Example 2.</p><p>Run the backup with this command:</p><pre class="literallayout">cd $CCPROOT/examples/docker/restore
./run.sh</pre><p>This script will do the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
start up a container named master-restore
</li><li class="listitem">
copy the backup files from the previous backup example into /pgdata
</li><li class="listitem">
start up the container using the backup files
</li><li class="listitem">
maps the PostgreSQL port of 5432 in the container to your local host port of 12001 as to not conflict with the master running in the previous example.
</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_7"></a>Kubernetes</h4></div></div></div><p>This example assumes you have run the backup-job example prior
to this example!</p><p>You will need to find a backup you want to
use for running this example, you will need the timestamped directory
path under /nfsfileshare/basic/.  Edit the master-restore.json
file and update the BACKUP_PATH setting to specify the
NFS backup path you want to restore with, example:</p><pre class="literallayout">"name": "BACKUP_PATH",
"value": "basic/2016-05-27-14-35-33"</pre><p>This example runs a postgres container passing in the backup location.
The startup of the container will use rsync to copy the backup data
to this new container, and then launch postgres which will use the
backup data to startup with.</p><p>Running the example:</p><pre class="literallayout">examples/kube/master-restore/run.sh</pre><p>Test the restored database as follows:</p><pre class="literallayout">psql -h restored-master -U postgres postgres</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_8"></a>OpenShift</h4></div></div></div><p>This is an example of restoring a database pod using
an existing backup archive located on an NFS volume.</p><p>First, locate the database backup you want to restore, for example:</p><pre class="literallayout">/nfsfileshare/pg-master/2016-01-29:22:34:20</pre><p>Then create the pod:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-restore
./run.sh</pre><p>When the database pod starts, it will copy the backup files
to the database directory inside the pod and start up postgres as
usual.</p><p>The restore only takes place if:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
the /pgdata directory is empty
</li><li class="listitem">
the /backups directory contains a valid postgresql.conf file
</li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_failover_manual_database_failover"></a>1.10. failover - Manual database failover</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_9"></a>OpenShift</h4></div></div></div><p>An example of performing a database failover is described
in the following steps:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
create a master and replica replication
</li></ul></div><pre class="literallayout">cd $CCPROOT/examples/openshift/master-replica-dc
./run.sh</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
scale up the number of replicas to 2
</li></ul></div><pre class="literallayout">oc scale rc replica-dc-1 --replicas=2</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
delete the master pod
</li></ul></div><pre class="literallayout">oc delete pod master-dc</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
exec into a replica and create a trigger file to being
  the recovery process, effectively turning the replica into a master
</li></ul></div><pre class="literallayout">oc exec -it replica-dc-1-lt5a5
touch /tmp/pg-failover-trigger</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
change the label on the replica to master-dc instead of replica-dc
</li></ul></div><pre class="literallayout">oc edit pod/replica-dc-1-lt5a5
original line: labels/name: replica-dc
updated line: labels/name: master-dc</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
or alternatively:
</li></ul></div><pre class="literallayout">oc label --overwrite=true pod replica-dc-1-lt5a5 name=master-dc</pre><p>You can test the failover by creating some data on the master
and then test to see if the replicas have the replicated data.</p><pre class="literallayout">psql -c 'create table foo (id int)' -U master -h master-dc postgres
psql -c 'table foo' -U master -h replica-dc postgres</pre><p>After a failover, you would most likely want to create a database
backup and be prepared to recreate your cluster from that backup.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pgbackrest_backup_utility"></a>1.11. pgbackrest - Backup Utility</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_8"></a>Kubernetes</h4></div></div></div><p>Starting in release 1.2.5, the pgbackrest utility has been
added to the crunchy-postgres container.  See the
<a class="ulink" href="backrest.adoc" target="_top">pgbackrest Documentation</a> for details
on how this feature works within the container suite.</p><p>Start the example as follows:</p><pre class="literallayout">cd $CCPROOT/examples/kube/backrest
./run.sh</pre><p>This will create the following in your Kube environment:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
A configMap named backrestconf which contains the pgbackrest.conf file
</li><li class="listitem">
master-backrest pod with pgbackrest archive enabled. An initial stanza db will be created on initialization
</li><li class="listitem">
master-backrest service
</li></ul></div><p>The crunchy-pvc will be used for /pgdata, and crunchy-pvc2 for the /backrestrepo. Examine the /backrestrepo location to view the archive directory and ensure WAL archiving is working. See <a class="ulink" href="backrest.adoc" target="_top">pgbackrest Documentation</a> for steps to backup and restore using pgbackrest.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_10"></a>OpenShift</h4></div></div></div><p>This example shows how to enable pgbackrest as the archiver
within the crunchy-postgres container.
See the <a class="ulink" href="backrest.adoc" target="_top">pgbackrest documentation</a> for details and background.</p><p>Start by running the example database container:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/backrest
./run.sh</pre><p>This will create the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
PV/PVC for /pgconf and /backrestrepo volumes
</li><li class="listitem">
master database pod
</li><li class="listitem">
master service
</li></ul></div><p>The run.sh script copies the pgbackrest.conf configuration file
to /nfsfileshare/pgconf which is our NFS file path.</p><p>The archive files are written to the NFS path of /nfsfileshare/backrestrepo.</p><p>The presence of /pgconf/pgbackrest.conf is what is used to
determine whether pgbackrest will be used as the archive command or not.
You will need to specify the ARCHIVE_TIMEOUT environment variable
as well to use this.</p><p>After you run the example, you should see archive files
being written to the /backrestrepo volume (/nfsfileshare/backrestrepo).</p><p>You can create a backup using backrest using this command within
the container:</p><pre class="literallayout">pgbackrest --stanza=db backup --db-path=/pgdata/master-backrest/ --log-path=/tmp --repo-path=/backrestrepo -conf=/pgconf/pgbackrest.conf</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pgbackrest_restore_restoring_backups_made_with_pgbackrest"></a>1.12. pgbackrest restore - Restoring backups made with pgbackrest</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_amp_openshift"></a>Kubernetes &amp; OpenShift</h4></div></div></div><p>This assumes you have run the pgbackrest example above. There are two options to choose from when performing a restore, DELTA and FULL. A FULL is the default; a DELTA will only occur if the environment variable DELTA is specified in the restore-job spec. Consult the pgbackrest user guide to determine which is best suited to run.</p><p>Steps for FULL restore</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
Delete the master-backrest pod, if still running
</li><li class="listitem">
Empty the PGDATA directory (remove all files)
</li><li class="listitem">
Navigate to the backrest-restore examples directory. Execute the full-restore.sh script.
</li><li class="listitem">
Check the restore logs (db-restore.log) in the /backrestrepo mountpoint for success. You can also view the logs of the completed job pod with kubectl get pod -a
</li><li class="listitem">
Re-create the master-backrest pod in the backrest examples directory. The database will recover.
</li></ul></div><p>Steps for DELTA restore</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
Delete the master-backrest pod, if still running
</li><li class="listitem">
rm postmaster.pid from PGDATA.
</li><li class="listitem">
Navigate to the backrest-restore examples directory. Execute the delta-restore.sh script.
</li><li class="listitem">
Check the restore logs (db-restore.log) in the /backrestrepo mountpoint for success. You can also view the logs of the completed job pod with kubectl get pod -a
</li><li class="listitem">
Re-create the master-backrest pod in the backrest examples directory. The database will recover only files that have changed from the last backup.
</li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pgadmin4_postgresql_web_user_interface"></a>1.13. pgadmin4 - PostgreSQL web user interface</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_5"></a>Docker</h4></div></div></div><p>This example, $CCPROOT/examples/docker/pgadmin4, provides a
container that runs the pgadmin4 web application.</p><p>To run this example, run the following:</p><pre class="literallayout">cd $CCPROOT/examples/docker/pgadmin4
./run.sh</pre><p>You should now be able to browse to <a class="ulink" href="http://YOURLOCALIP:5050" target="_top">http://YOURLOCALIP:5050</a>
and log into the web application using a user ID of <span class="strong"><strong>admin@admin.org</strong></span>
and password of <span class="strong"><strong>password</strong></span>.  Replace YOURLOCALIP with whatever
your local IP address happens to be.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_9"></a>Kubernetes</h4></div></div></div><p>This example deploys the pgadmin4 (beta4) web user interface
for Postgresql.</p><p>Start the container as follows:</p><pre class="literallayout">cd $CCPROOT/examples/kube/pgadmin4
./run.sh</pre><p>This will start a container and service for pgadmin4.  You can browse
the user interface at <a class="ulink" href="http://pgadmin4.default.svc.cluster.local:5050" target="_top">http://pgadmin4.default.svc.cluster.local:5050</a></p><p>See the pgadmin4 documentation for more details at <a class="ulink" href="http://pgadmin.org" target="_top">http://pgadmin.org</a></p><p>The example uses pgadmin4 configuration files which are mounted
at an NFS mount point, this NFS data directory is mounted into
the container and used by the pgadmin4 application to persist
metadata.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_11"></a>OpenShift</h4></div></div></div><p>This example, examples/openshift/pgadmin4, provides a
container that runs the pgadmin4 web application.</p><p>To run this example, run the following:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/pgadmin4
./run.sh</pre><p>This script creates the <span class="strong"><strong>pgadmin4</strong></span> pod and service, it will
expose port 5050.</p><p>You should now be able to browse to <a class="ulink" href="http://pgadmin4.openshift.svc.cluster.local:5050" target="_top">http://pgadmin4.openshift.svc.cluster.local:5050</a>
and log into the web application using a user ID of <span class="strong"><strong>admin@admin.org</strong></span>
and password of <span class="strong"><strong>password</strong></span>.  Replace YOURLOCALIP with whatever
your local IP address happens to be.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_crunchy_proxy_a_smart_proxy_for_postgresql"></a>1.14. crunchy-proxy - A smart proxy for PostgreSQL</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_6"></a>Docker</h4></div></div></div><p>A <span class="strong"><strong>crunchy-proxy</strong></span> example is provided that will run a  container that
is configured to be used with the master and replica example provided
in the <span class="strong"><strong>master-replica</strong></span> example.</p><p>You can create the proxy by running:</p><pre class="literallayout">cd $CCPROOT/examples/docker/crunchy-proxy
./run.sh</pre><p>This proxy will listen on localhost:12432.  You can access the
<span class="strong"><strong>master-replica</strong></span> cluster by:</p><pre class="literallayout">psql -h localhost -p 12432 -U postgres postgres</pre><p>See this link for details on the <span class="strong"><strong>crunchy-proxy</strong></span>:
<a class="ulink" href="https://github.com/CrunchyData/crunchy-proxy" target="_top">https://github.com/CrunchyData/crunchy-proxy</a></p><p>You might consider <span class="strong"><strong>crunchy-proxy</strong></span> over pgpool and pgbouncer if
you need load-balancing and smart SQL routing.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_10"></a>Kubernetes</h4></div></div></div><p>This example assumes you have run the master-replica example prior
to this example!</p><p>This example runs a crunchy-proxy pod that creates a special purpose
proxy to a postgres cluster (master and replica).</p><p><span class="strong"><strong>crunchy-proxy</strong></span> offers a high performance alternative to
pgbouncer and pgpool.</p><p>The proxy example copies a configuration file to the PV_PATH
and starts up the <span class="strong"><strong>crunchy-proxy</strong></span> within a Deployment.</p><p>If you run the example in minikube, you will need to manually
copy the crunchy-proxy-config.json file to a file on
the minikube named <span class="strong"><strong>/data/config.json</strong></span>.</p><p>The proxy reads the configuration file from a <span class="strong"><strong>/config</strong></span> volume
mount and begins execution.</p><p>Start by running the proxy container:</p><pre class="literallayout">cd $CCPROOT/examples/kube/crunchy-proxy
./run.sh</pre><p>The proxy will listen on port 5432 as specified in the
configuration file.  The example creates a Service named
<span class="strong"><strong>crunchy-proxy</strong></span> that you can use to access the configured
PostgreSQL backend containers from the <span class="strong"><strong>master-replica</strong></span> example.</p><p>See the following link for more information on the <span class="strong"><strong>crunchy-proxy</strong></span>:</p><p><a class="ulink" href="https://github.com/CrunchyData/crunchy-proxy" target="_top">https://github.com/CrunchyData/crunchy-proxy</a></p><p>Test the proxy by running psql commands via the proxy connection:</p><pre class="literallayout">psql -h crunchy-proxy -U postgres postgres</pre><p>SQL "reads" will be sent to the PostgreSQL replica database if your
SQL includes the <span class="strong"><strong>crunchy-proxy</strong></span> read annotation.  SQL statements
that do not include the read annotation will be sent to the master
database container within the PostgreSQL cluster.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_12"></a>OpenShift</h4></div></div></div><p>This example shows how to use the <span class="strong"><strong>crunchy-proxy</strong></span> to
act as a smart proxy to a PostgreSQL cluster.  The example
depends upon the <span class="strong"><strong>master-replica</strong></span> example being run prior.</p><p><span class="strong"><strong>crunchy-proxy</strong></span> offers a high performance alternative to
pgbouncer and pgpool.</p><p>The proxy example copies a configuration file to the PV_PATH
and starts up the <span class="strong"><strong>crunchy-proxy</strong></span> within a Deployment.</p><p>The proxy reads the configuration file from a <span class="strong"><strong>/config</strong></span> volume
mount and begins execution.</p><p>Start by running the proxy container:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/crunchy-proxy
./run.sh</pre><p>The proxy will listen on port 5432 as specified in the
configuration file.  The example creates a Service named
<span class="strong"><strong>crunchy-proxy</strong></span> that you can use to access the configured
PostgreSQL backend containers from the <span class="strong"><strong>master-replica</strong></span> example.</p><p>See the following link for more information on the <span class="strong"><strong>crunchy-proxy</strong></span>:</p><p><a class="ulink" href="https://github.com/CrunchyData/crunchy-proxy" target="_top">https://github.com/CrunchyData/crunchy-proxy</a></p><p>Test the proxy by running psql commands via the proxy connection:</p><pre class="literallayout">psql -h crunchy-proxy -U postgres postgres</pre><p>SQL "reads" will be sent to the PostgreSQL replica database if your
SQL includes the <span class="strong"><strong>crunchy-proxy</strong></span> read annotation.  SQL statements
that do not include the read annotation will be sent to the master
database container within the PostgreSQL cluster.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_custom_config_custom_configuration_files"></a>1.15. custom-config - Custom Configuration Files</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_13"></a>OpenShift</h4></div></div></div><p>This example shows how you can use your own customized version of setup.sql
when creating a postgres database container.</p><p>If you mount a /pgconf volume, crunchy-postgres will look at that directory
for postgresql.conf, pg_hba.conf, and setup.sql.  If it finds one of them it
will use that file instead of the default files.  Currently, if you specify a postgresql.conf
file, you also need to specify a pg_hba.conf file.</p><p>The example shows how a custom setup.sql file can be used.
Run it as follows:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/custom-config
./run.sh</pre><p>This will start a database container that will use an NFS mounted /pgconf
directory that will container the custom setup.sql file found in the example
directory.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_custom_config_sync_custom_configuration_files_with_sync_replica"></a>1.16. custom-config sync - Custom Configuration Files with Sync Replica</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_14"></a>OpenShift</h4></div></div></div><p>This example shows how you can use your own customized version of postgresql.conf
and pg_hba.conf to override the default configuration.  It also specifies
a sync replica in the postgresql.conf and starts up a sync replica.</p><p>If you mount a /pgconf volume, crunchy-postgres will look at that directory
for postgresql.conf, pg_hba.conf, and setup.sql.  If it finds one of them it
will use that file instead of the default files.  Currently, if you specify a postgresql.conf
file, you also need to specify a pg_hba.conf file.</p><p>Run it as follows:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/custom-config-sync
./run.sh</pre><p>This will start a <span class="strong"><strong>csmaster</strong></span> container that will use the custom
config files when the database is running.  It will also create
a sync replica named <span class="strong"><strong>cssyncreplica</strong></span>, this replica is
connected to the master via streaming replication.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_configmap_database_credentials_from_a_configmap"></a>1.17. configmap - database credentials from a configmap</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_15"></a>OpenShift</h4></div></div></div><p>This example shows how to use a configmap to store the
postgresql.conf and pg_hba.conf files to be used when
overriding the default configuration within the container.</p><p>Start by running the database container:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/configmap
./run.sh</pre><p>The files, pg_hba.conf and postgresql.conf, in the
example directory are used to create a configmap object
within OpenShift.  Within the run.sh script, the configmap
is created, and notice within the configmap.json file
how the /pgconf mount is related to the configmap.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pgpool_pgpool_pod_example"></a>1.18. pgpool - pgpool pod example</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_7"></a>Docker</h4></div></div></div><p>A pgpool example is provided that will run a pgpool container that
is configured to be used with the master and replica example provided
in the <span class="strong"><strong>master-replica</strong></span> example.  After running
those commands to create a master and replica, you can
create a pgpool container by running the following example command:</p><pre class="literallayout">cd $CCPROOT/examples/docker/pgpool
./run.sh</pre><p>Enter the following command to connect to the pgpool that is
mapped to your local port 12003:</p><pre class="literallayout">psql -h localhost -U testuser -p 12003 userdb</pre><p>You will enter the password of <span class="strong"><strong>password</strong></span> when prompted.  At this point
you can execute both INSERT and SELECT statements on the pgpool connection.
Pgpool will direct INSERT statements to the master and SELECT statements
will be sent round-robin to both master and replica.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_11"></a>Kubernetes</h4></div></div></div><p>This example assumes you have run the master-replica example prior
to this example!</p><p>This example runs a pgpool pod that creates a special purpose
proxy to a postgres cluster (master and replica).</p><p>Running the example:</p><pre class="literallayout">examples/kube/pgpool/run.sh</pre><p>The example is configured to allow the <span class="strong"><strong>testuser</strong></span> to connect
to the <span class="strong"><strong>userdb</strong></span> database as follows:</p><pre class="literallayout">psql -h pgpool -U testuser userdb</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_16"></a>OpenShift</h4></div></div></div><p>You can create a pgpool service that will work with the
master and replica created in the previous example.</p><p>You will need to edit the pgpool-rc.json and supply the
testuser password that was generated when you created
the master replica pods, then run the following command
to deploy the pgpool service:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/pgpool
./run.sh</pre><p>Next, you can access the master replica cluster via the pgpool
service by entering the following command:</p><pre class="literallayout">psql -h pgpool -U testuser userdb
psql -h pgpool -U testuser postgres</pre><p>When prompted, enter the password for the PG_USERNAME testuser
that was set for the pg-master pod, typically it is <span class="strong"><strong>password</strong></span>.</p><p>At this point, you can enter SELECT and INSERT statements and
pgpool will proxy the SQL commands to the master or replica(s)
depending on the type of SQL command.  Writes will always
be sent to the master, and reads will be sent (round-robin)
to the replica(s).</p><p>You can view the nodes that pgpool is configured for by
running:</p><pre class="literallayout">psql -h pgpool -U testuser userdb -c 'show pool_nodes'</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pgbadger_pgbadger_pod"></a>1.19. pgbadger - pgbadger pod</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_8"></a>Docker</h4></div></div></div><p>A pgbadger example is provided that will run a HTTP server that
when invoked, will generate a pgbadger report on a given database.</p><p>pgbadger reads the log files from a database to product an HTML report
that shows various Postgres statistics and graphs.</p><p>To run the example, modify the run-badger.sh script to refer to the
Docker container that you want to run pgbadger against, also referring
to the container’s data directory, then run the example as follows:</p><pre class="literallayout">cd $CCPROOT/examples/docker/badger
./run.sh</pre><p>After execution, the container will run and provide a simple HTTP
command you can browse to view the report.  As you run queries against
the database, you can invoke this URL to generate updated reports:</p><pre class="literallayout">curl http://127.0.0.1:14000/api/badgergenerate</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_12"></a>Kubernetes</h4></div></div></div><p>This example runs a pod that includes a database container and
a pgbadger container. A service is also created for the pod.</p><p>Running the example:</p><pre class="literallayout">examples/kube/badger/run.sh</pre><p>You can access pgbadger at:</p><pre class="literallayout">curl http://badger:10000/api/badgergenerate</pre><p><span class="strong"><strong>Tip</strong></span></p><p>You can view the database container logs using this command:</p><pre class="literallayout">kubectl logs -c server badger</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_17"></a>OpenShift</h4></div></div></div><p>This example creates a pod that contains a database container and
a pgbadger container.</p><p><span class="strong"><strong>pgbadger</strong></span> is then served up on port 10000.  Each time you do a
GET on <a class="ulink" href="http://pg-master:10000/api/badgergenerate" target="_top">http://pg-master:10000/api/badgergenerate</a>
it will run pgbadger against the database log files running in the
pg-master container.</p><p>golang is required to build the pgbadger container, on RH 7.2, golang
is found in the <span class="emphasis"><em>server optional</em></span> repository and needs to be enabled
to install.</p><p>To run the example:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/badger
./run.sh</pre><p>try the following command to see the generated HTML output:</p><pre class="literallayout">curl http://badger-example:10000/api/badgergenerate</pre><p>You can view this output in a browser if you allow port forwarding
from your container to your server host using a command like
this:</p><pre class="literallayout">socat tcp-listen:10001,reuseaddr,fork tcp:pg-master:10000</pre><p>This command maps port 10000 of the service/container to port
10001 of the local server.  You can now use your browser to
see the badger report.</p><p>This is a short-cut way to expose a service to the external world,
OpenShift would normally configure a router whereby you could
<span class="emphasis"><em>expose</em></span> the service in an OpenShift way.  Here are the docs
on installing OpenShift on a router:</p><pre class="literallayout">https://docs.openshift.com/enterprise/3.0/install_config/install/deploy_router.html</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_metrics_amp_collect_metrics_collection"></a>1.20. metrics &amp; collect - Metrics collection</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_9"></a>Docker</h4></div></div></div><p>You can collect various Postgres metrics from your database
container by running a crunchy-collect container that points
to your database container.</p><p>Metrics collection requires you run the crunchy <span class="emphasis"><em>scope</em></span> set of containers
that includes:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
Prometheus
</li><li class="listitem">
Prometheus push gateway
</li><li class="listitem">
Grafana
</li></ul></div><p>To start this set of containers, run the following:</p><pre class="literallayout">cd $CCPROOT/examples/docker/metrics
./run.sh</pre><p>These metrics are described in this <a class="ulink" href="metrics.adoc" target="_top">document.</a></p><p>An example has been provided that runs a database container
and also the associated metrics collection container, run the
example as follows:</p><pre class="literallayout">cd $CCPROOT/examples/docker/collect
./run.sh</pre><p>Every 3 minutes the collection container will collect postgres
metrics and push them to the crunchy-prometheus database.  You
can graph them using the crunchy-grafana container.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_13"></a>Kubernetes</h4></div></div></div><p>This example starts up Prometheus, Grafana, and Prometheus gateway.</p><p>It is required to view or capture metrics collected by crunchy-collect.</p><p>Running the example:</p><pre class="literallayout">examples/kube/metrics/run.sh</pre><p>This will start up 3 containers and services:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
Prometheus (<a class="ulink" href="http://crunchy-prometheus:9090" target="_top">http://crunchy-prometheus:9090</a>)
</li><li class="listitem">
Prometheus gateway (<a class="ulink" href="http://crunchy-promgateway:9091" target="_top">http://crunchy-promgateway:9091</a>)
</li><li class="listitem">
Grafana (<a class="ulink" href="http://crunchy-grafana:3000" target="_top">http://crunchy-grafana:3000</a>)
</li></ul></div><p>If you want your metrics and dashboards to persist to NFS, run
this script:</p><pre class="literallayout">examples/kube/metrics/run-pvc.sh</pre><p>In the /docs/ folder of the GitHub repository, check out the <a class="ulink" href="metrics.adoc" target="_top">metrics documentation</a>
for details on the exact metrics being collected.</p><p>This example runs a pod that includes a database container and
a metrics collection container. A service is also created for the pod.</p><p>Running the example:</p><pre class="literallayout">examples/kube/collect/run.sh</pre><p>You can view the collect container logs using this command:</p><pre class="literallayout">kubectl logs -c collect master-collect</pre><p>You can access the database or drive load against it using
this command:</p><pre class="literallayout">psql -h master-collect -U postgres postgres</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_18"></a>OpenShift</h4></div></div></div><p>This example shows how Postgres metrics can be collected
and stored in Prometheus and graphed with Grafana.</p><p>First, create the crunchy-metrics pod which contains
the Prometheus data store and the Grafana graphing web application:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/metrics
./run.sh</pre><p>At this point, you can view the Prometheus web console at
crunchy-metrics:9090, the Prometheus push gateway at crunchy-metrics:9091,
and the Grafana web app at crunchy-metrics:3000.</p><p>Next, start a Postgres pod that has the crunchy-collect container
as follows:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/collect
./run.sh</pre><p>At this point, metrics will be collected every 3 minutes and pushed
to Prometheus.  You can build graphs off the metrics using Grafana.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_vacuum_job_postgresql_vacuum_command"></a>1.21. vacuum-job - PostgreSQL vacuum command</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_10"></a>Docker</h4></div></div></div><p>You can perform a Postgres vacuum command by running the crunchy-vacuum
container.  You specify a database to vacuum using environment variables.</p><p>An example is shown in the $CCPROOT/examples/docker/vacuum/run.sh script
and can be run as follows:</p><pre class="literallayout">cd $CCPROOT/examples/docker/vacuum
./run.sh</pre><p>This example performs a vacuum on a single table in the master Postgres
database.  Vacuum is controlled via the following environment variables:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
VAC_FULL - when set to true adds the FULL parameter to the VACUUM command
</li><li class="listitem">
VAC_TABLE - when set, allows you to specify a single table to vacuum, when
 not specified, the entire database tables are vacuumed
</li><li class="listitem">
JOB_HOST - required variable is the Postgres host we connect to
</li><li class="listitem">
PG_USER - required variable is the Postgres user we connect with
</li><li class="listitem">
PG_DATABASE - required variable is the Postgres database we connect to
</li><li class="listitem">
PG_PASSWORD - required variable is the Postgres user password we connect with
</li><li class="listitem">
PG_PORT - allows you to override the default value of 5432
</li><li class="listitem">
VAC_ANALYZE - when set to true adds the ANALYZE parameter to the VACUUM command
</li><li class="listitem">
VAC_VERBOSE - when set to true adds the VERBOSE parameter to the VACUUM command
</li><li class="listitem">
VAC_FREEZE - when set to true adds the FREEZE parameter to the VACUUM command
</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_14"></a>Kubernetes</h4></div></div></div><p>This example assumes you have run the basic example prior
to this example!</p><p>This example runs a Job which performs a SQL VACUUM on a particular
table (testtable) in the basic database instance.</p><p>Running the example:</p><pre class="literallayout">examples/kube/vacuum-job/run.sh</pre><p>Verify the job completed:</p><pre class="literallayout">kubectl get job</pre><p>Look at the docker log of the vacuum job’s pod:</p><pre class="literallayout">docker logs $(docker ps -a | grep crunchy-vacuum | cut -f 1 -d' ')</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_19"></a>OpenShift</h4></div></div></div><p>This example shows how you can run a vacuum job against
a Postgres database container.</p><p>The crunchy-vacuum container image exists to allow a DBA
a way to run a job either one-off or scheduled to perform
a variety of vacuum operations.</p><p>To run the vacuum a single time, an example is included
as follows from the examples/openshift directory:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-replica
./run.sh
cd ../vacuum-job
./run.sh</pre><p>This will start a vacuum container that runs as a Kube Job type.  It
will run once.  The crunchy-vacuum image is executed, passed in
the Postgres connection parameters to the single-master Postgres
container.  The type of vacuum performed is dictated by the
environment variables passed into the job. The complete set
of environment variables read by the vacuum job include:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
VAC_FULL - when set to true adds the FULL parameter to the VACUUM command
</li><li class="listitem">
VAC_TABLE - when set, allows you to specify a single table to vacuum, when
 not specified, the entire database tables are vacuumed
</li><li class="listitem">
JOB_HOST - required variable is the Postgres host we connect to
</li><li class="listitem">
PG_USER - required variable is the Postgres user we connect with
</li><li class="listitem">
PG_DATABASE - required variable is the Postgres database we connect to
</li><li class="listitem">
PG_PASSWORD - required variable is the Postgres user password we connect with
</li><li class="listitem">
PG_PORT - allows you to override the default value of 5432
</li><li class="listitem">
VAC_ANALYZE - when set to true adds the ANALYZE parameter to the VACUUM command
</li><li class="listitem">
VAC_VERBOSE - when set to true adds the VERBOSE parameter to the VACUUM command
</li><li class="listitem">
VAC_FREEZE - when set to true adds the FREEZE parameter to the VACUUM command
</li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_crunchy_dba_cron_scheduler_for_simple_dba_tasks"></a>1.22. crunchy-dba - cron scheduler for simple DBA tasks</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_15"></a>Kubernetes</h4></div></div></div><p>The crunchy-dba container implements a cron scheduler. The purpose of the crunchy-dba
container is to offer a way to perform simple DBA tasks that occur on some form of
schedule such as backup jobs or running a vacuum on a single Postgres database container.
Both of these examples are provided as scripts.</p><p>You can either run the crunchy-dba container as a single pod or include the container
within a database pod.</p><p>The crunchy-dba container makes use of a Service Account to perform the startup of
scheduled jobs. The Kube Job type is used to execute the scheduled jobs with a Restart
policy of Never.</p><p>The script to schedule vacuum on a regular schedule is executed through the following
commands:</p><pre class="literallayout">cd $CCPROOT/examples/kube/dba
./run-vac.sh</pre><p>To run the script for scheduled backups, run the following in the same directory:</p><pre class="literallayout">./run-backup.sh</pre><p>Individual parameters for both can be modified within their respective JSON files;
please see link:containers.adoc for a full list of what can be modified.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_custom_setup_custom_setup_sql"></a>1.23. custom-setup - Custom setup.sql</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_11"></a>Docker</h4></div></div></div><p>You can use your own version of the setup.sql SQL file to customize
the initialization of database data and objects when the container and
database are created.</p><p>An example is shown in the $CCPROOT/examples/docker/custom-setup/run.sh script
and can be run as follows:</p><pre class="literallayout">cd $CCPROOT/examples/docker/custom-setup
./run.sh</pre><p>This works by placing a file named, setup.sql, within the /pgconf mounted volume
directory.  Portions of the setup.sql file are required for the crunchy container
to work, see comments within the sample setup.sql file.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pgbouncer"></a>1.24. pgbouncer</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_12"></a>Docker</h4></div></div></div><p>The pgbouncer utility can be used to provide a connection pool
to Postgres databases.  The crunchy-pgbouncer container also
contains logic that lets it perform a failover from a master
to a replica database.</p><p>To test this failover, you first create a running master/replica
cluster as follows:</p><pre class="literallayout">cd $CCPROOT/examples/docker/master-replica
./run.sh</pre><p>An example is shown in the $CCPROOT/examples/docker/pgbouncer/run.sh script
and can be run as follows:</p><pre class="literallayout">cd $CCPROOT/examples/docker/pgbouncer
./run.sh</pre><p>This example configures pgbouncer to provide connection pooling
for the master and pg-replica databases.  It also sets the FAILOVER
environment variable which will cause a failover to be triggered
if the master database can not be reached.</p><p>To trigger the failover, stop the master database:</p><pre class="literallayout">docker stop master</pre><p>At this point, the pgbouncer will notice that the master is not reachable
and touch the trigger file on the configured replica database to start
the failover.  The pgbouncer container will then reconfigure
pgbouncer to relabel the replica database into the master database so clients
to pgbouncer will be able to connect to the master as before the failover.</p><p>To just log into the database from the pgbouncer connection pool
you would enter the following using the password "password":</p><pre class="literallayout">psql -h localhost -p 12005 -U testuser master</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_16"></a>Kubernetes</h4></div></div></div><p>This example assumes you have run the master-replica example prior
to this example!</p><p>This example runs a crunchy-pgbouncer container to look for the
master within a Postgres cluster, if it can not find the master it
will proceed to cause a failover to a replica.  It will also configure
a pgbouncer container that sets up a connection pool to the
configured master and replica.</p><p>Running the example:</p><pre class="literallayout">examples/kube/pgbouncer/run.sh</pre><p>Connect to the <span class="strong"><strong>master</strong></span> and <span class="strong"><strong>replica</strong></span> databases as follows:</p><pre class="literallayout">psql -h pgbouncer -U postgres master
psql -h pgbouncer -U postgres replica</pre><p>The names <span class="strong"><strong>master</strong></span> and <span class="strong"><strong>replica</strong></span> are pgbouncer configured names
and don’t necessarily have to match the database name in the
actual Postgres instance.</p><p>View the pgbouncer log as follows:</p><pre class="literallayout">kubectl log pgbouncer</pre><p>Next, test the failover capability within the crunchy-watch
container using the following:</p><pre class="literallayout">kubectl delete pod master</pre><p>Take another look at the pgbouncer log and you will see it trigger
the failover to the replica pod.  After this failover
you should be able to execute the command:</p><pre class="literallayout">psql -h pgbouncer -U postgres master</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_20"></a>OpenShift</h4></div></div></div><p>This example shows how you can use the crunchy-pgbouncer container
when running under OpenShift.</p><p>The example assumes you have run the master/replica example
found here:</p><pre class="literallayout">$CCPROOT/examples/openshift/master-replica-dc
./run.sh</pre><p>Then you would start up the pgbouncer container using the following
example:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/pgbouncer
./run.sh</pre><p>The example assumes you have an NFS share path of /nfsfileshare/!  NFS
is required to mount the pgbouncer configuration files which are
then mounted to /pgconf in the crunchy-pgbouncer container.</p><p>If you mount a /pgconf volume, crunchy-postgres will look at that directory
for postgresql.conf, pg_hba.conf, and setup.sql.  If it finds one of them it
will use that file instead of the default files.</p><p>Test the example by killing off the master database container as
follows:</p><pre class="literallayout">oc delete pod master-dc</pre><p>Then watch the pgbouncer log as follows to confirm it detects the loss
of the master:</p><pre class="literallayout">oc logs pgbouncer</pre><p>After the failover is completed, you should be able to access
the new master using the master service as follows:</p><pre class="literallayout">psql -h master-dc.openshift.svc.cluster.local -U master postgres</pre><p>and access the replica as follows:</p><pre class="literallayout">psql -h replica-dc.openshift.svc.cluster.local -U master postgres</pre><p>or via the pgbouncer proxy as follows:</p><pre class="literallayout">psql -h pgbouncer.openshift.svc.cluster.local  -U master master</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_sync_synchronous_replication"></a>1.25. sync - Synchronous replication</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_13"></a>Docker</h4></div></div></div><p>This example, $CCPROOT/examples/docker/sync, provides a
streaming replication configuration that includes both
synchronous and asynchronous replicas.</p><p>To run this example, run the following:</p><pre class="literallayout">cd $CCPROOT/examples/docker/sync
./run.sh</pre><p>You can test the replication status on the master by using the following command
and the password "password":</p><pre class="literallayout">psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'table pg_stat_replication'</pre><p>You should see 2 rows, 1 for the async replica and 1 for the sync replica.  The
sync_state column shows values of async or sync.</p><p>You can test replication to the replicas by entering some data on
the master like this, and then querying the replicas for that data:</p><pre class="literallayout">psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'create table foo (id int)'
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'insert into foo values (1)'
psql -h 127.0.0.1 -p 12002 -U postgres postgres -c 'table foo'
psql -h 127.0.0.1 -p 12003 -U postgres postgres -c 'table foo'</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_17"></a>Kubernetes</h4></div></div></div><p>This example deploys a PostgreSQL cluster with a master,
a synchronous replica, and an asynchronous replica.  The
two replicas share the same Service.</p><p>Running the example:</p><pre class="literallayout">examples/kube/sync/run.sh</pre><p>Connect to the <span class="strong"><strong>mastersync</strong></span> and <span class="strong"><strong>replicasync</strong></span> databases as follows:</p><pre class="literallayout">psql -h mastersync -U postgres postgres -c 'create table mister (id int)'
psql -h mastersync -U postgres postgres -c 'insert into mister values (1)'
psql -h mastersync -U postgres postgres -c 'table pg_stat_replication'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from mister'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from mister'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from mister'</pre><p>This set of queries will show you the IP address of the Postgres replica
container, notice it changes because of the round-robin Service proxy
we are using for both replicas.  The example queries also show that both
replicas are replicating from the master.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_21"></a>OpenShift</h4></div></div></div><p>This example deploys a PostgreSQL cluster with a master,
a synchrounous replica, and an asynchronous replica.  The
two replicas share the same Service.</p><p>Running the example:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/sync
./run.sh</pre><p>Connect to the <span class="strong"><strong>master</strong></span> and <span class="strong"><strong>replica</strong></span> databases as follows:</p><pre class="literallayout">psql -h master -U postgres postgres -c 'create table mister (id int)'
psql -h master -U postgres postgres -c 'insert into mister values (1)'
psql -h master -U postgres postgres -c 'table pg_stat_replication'
psql -h replica -U postgres postgres -c 'select inet_server_addr(), * from mister'
psql -h replica -U postgres postgres -c 'select inet_server_addr(), * from mister'
psql -h replica -U postgres postgres -c 'select inet_server_addr(), * from mister'</pre><p>This set of queries will show you the IP address of the Postgres replica
container, notice it changes because of the round-robin Service proxy
we are using for both replicas.  The example queries also show that both
replicas are replicating from the master.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_statefulsets"></a>1.26. statefulsets</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_18"></a>Kubernetes</h4></div></div></div><p>This example deploys a statefulset named <span class="strong"><strong>pgset</strong></span>.  The statefulset
is a new feature in Kubernetes as of version 1.5.  Statefulsets have
replaced PetSets going forward.</p><p>This example creates 2 Postgres containers to form the set.  At
startup, each container will examine its hostname to determine
if it is the first container within the set of containers.</p><p>The first container is determined by the hostname suffix assigned
by Kube to the pod.  This is an ordinal value starting with <span class="strong"><strong>0</strong></span>.</p><p>If a container sees that it has an ordinal value of <span class="strong"><strong>0</strong></span>, it will
update the container labels to add a new label of:</p><pre class="literallayout">name=$PG_MASTER_HOST</pre><p>In this example, PG_MASTER_HOST is specified as <span class="strong"><strong>pgset-master</strong></span>.</p><p>By default, the containers specify a value of <span class="strong"><strong>name=pgset-replica</strong></span></p><p>There are 2 services that end user applications will use to
access the PostgreSQL cluster, one service (pgset-master) routes to the master
container and the other (pgset-replica) to the replica containers.</p><pre class="literallayout">$ kubectl get service
NAME            CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes      10.96.0.1       &lt;none&gt;        443/TCP    22h
pgset           None            &lt;none&gt;        5432/TCP   1h
pgset-master    10.97.168.138   &lt;none&gt;        5432/TCP   1h
pgset-replica   10.97.218.221   &lt;none&gt;        5432/TCP   1h</pre><p>Start the example as follows:</p><pre class="literallayout">cd $CCPROOT/examples/kube/statefulset
./run.sh</pre><p>You can access the master database as follows:</p><pre class="literallayout">psql -h pgset-master -U postgres postgres</pre><p>You can access the replica databases as follows:</p><pre class="literallayout">psql -h pgset-replica -U postgres postgres</pre><p>You can scale the number of containers using this command, this will
essentially create an additional replica databse:</p><pre class="literallayout">kubectl scale pgset --replica=3</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_22"></a>OpenShift</h4></div></div></div><p>This example shows how to use a StatefulSet (available
in OpenShift Origin 3.5) to create a PostgreSQL cluster.</p><p>Build the example by:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/statefulset
./run.sh</pre><p>This will create a statefulset named pgset, which will create
2 pods, pgset-0 and pgset-1:</p><pre class="literallayout">oc get statefulset
oc get pod</pre><p>A service is created for the master and another service for the replica:</p><pre class="literallayout">oc get service</pre><p>The statefulset ordinal value of 0 is used to determine which pod
will act as the PostgreSQL master, all other ordinal values will
assume the replica role.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_statefulset_using_dynamic_provisioning"></a>1.27. statefulset using dynamic provisioning</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_19"></a>Kubernetes</h4></div></div></div><p>The example in <span class="strong"><strong>examples/statefulset-dyn</strong></span> is almost an exact copy of the
previous statefulset example, however, this example uses
Dynamic Storage Provisioning to automatically create Persistent
Volume Claims based on StorageClasses.  This Kube feature is
available on Google Container Engine which this example was
tested upon.</p><p>You can run the example as follows:</p><pre class="literallayout">cd $CCPROOT/examples/kube/statefulset-dyn
./run.sh</pre><p>This will create a StorageClass named <span class="strong"><strong>slow</strong></span> which you can view using:</p><pre class="literallayout">kubectl get storageclass
NAME      TYPE
slow      kubernetes.io/gce-pd</pre><p>The example causes Kube to create the required PVCs automatically:</p><pre class="literallayout">kubectl get pvc
NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
pgdata-pgset-0   Bound     pvc-06334f6f-371b-11e7-9bda-42010a8000e9   1Gi        RWX           slow           5m
pgdata-pgset-1   Bound     pvc-063795b3-371b-11e7-9bda-42010a8000e9   1Gi        RWX           slow           5m</pre><p>More information on dynamic storage provisioning can be found here:
<a class="ulink" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_top">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a></p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_secret"></a>1.28. secret</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_23"></a>OpenShift</h4></div></div></div><p>You can use Kubernetes Secrets to set and maintain your database
credentials.  Secrets requires you base64 encode your user and password
values as follows:</p><pre class="literallayout">echo -n 'myuserid' | base64</pre><p>You will paste these values into  your JSON secrets files for values.</p><p>This example allows you to set the PostgreSQL passwords
using Kube Secrets.</p><p>The secret uses a base64 encoded string to represent the
values to be read by the container during initialization.  The
encoded password value is <span class="strong"><strong>password</strong></span>.  Run the example
as follows:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/secret
./run.sh</pre><p>The secrets are mounted in the <span class="strong"><strong>/pguser</strong></span>, <span class="strong"><strong>/pgmaster</strong></span>, <span class="strong"><strong>/pgroot</strong></span> volumes within the
container and read during initialization.  The container
scripts create a Postgres user with those values, and sets the passwords
for the master user and Postgres superuser using the mounted secret volumes.</p><p>When using secrets, you do NOT have to specify the following
env vars if you specify all three secrets volumes:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
PG_USER
</li><li class="listitem">
PG_PASSWORD
</li><li class="listitem">
PG_ROOT_PASSWORD
</li><li class="listitem">
PG_MASTER_USER
</li><li class="listitem">
PG_MASTER_PASSWORD
</li></ul></div><p>You can test the container as follows, in all cases, the password is <span class="strong"><strong>password</strong></span>:</p><pre class="literallayout">psql -h secret-pg -U pguser1 postgres
psql -h secret-pg -U postgres postgres
psql -h secret-pg -U master postgres</pre><p>Secrets requires you base64 encode your user and password
values as follows:</p><pre class="literallayout">echo -n 'myuserid' | base64</pre><p>You will paste these values into  your JSON secrets files for values.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pitr_pitr_point_in_time_recovery"></a>1.29. pitr - PITR (point in time recovery)</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_14"></a>Docker</h4></div></div></div><p>This example, $CCPROOT/examples/docker/pitr, provides an
example of performing a point in time recovery.</p><p>To run this example, run the following to create a
database container:</p><pre class="literallayout">cd $CCPROOT/examples/docker/pitr
./run-master-pitr.sh</pre><p>It takes about 1 minute for the database to become ready
for use after initially starting.</p><p>This database is created with the ARCHIVE_MODE and ARCHIVE_TIMEOUT
environment variables set.  See the <a class="ulink" href="pitr.adoc" target="_top">PITR documentation</a>for more details
on these settings.  Warning:  this example writes the WAL segment
files to the /tmp directory…running it for a long time could
fill up your /tmp!</p><p>Next, we will create a base backup of that database using
this:</p><pre class="literallayout">./run-master-pitr-backup.sh</pre><p>At this point, WAL segment files are created every 60 seconds that
contain any database changes.  These segments are stored in
the /tmp/master-data/master-wal directory.</p><p>Next, create some data in your database using this command:</p><pre class="literallayout">psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('beforechanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'create table pitrtest (id int)'
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('afterchanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('nomorechanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "checkpoint"</pre><p>Next, stop the database to avoid conflicts with the WAL files while
attempting to do a restore from them:</p><pre class="literallayout">docker stop master-pitr</pre><p>The commands above set restore point labels which we can
use to mark the points in the recovery process we want to
reference when creating our restored database.  Points before
and after the test table were made.</p><p>Next, lets edit the restore script to use the base backup files
created in the step above.  You can view the backup path name
under the /tmp/backups/master-pitr-backups/ directory. You will see
another directory inside of this path with a name similar to
<span class="strong"><strong>2016-09-21-21-03-29</strong></span>.  Copy and paste that value into the
run-restore-pitr.sh script in the <span class="strong"><strong>BACKUP</strong></span> environment variable.</p><p>In order to restore the database before we created test table in the
last command, you’ll need uncomment to the RECOVERY_TARGET_NAME label
<span class="strong"><strong>-e RECOVERY_TARGET_NAME=beforechanges</strong></span> to define the restore target name.
After that, run the script.</p><pre class="literallayout">vi ./run-restore-pitr.sh
./run-restore-pitr.sh</pre><p>The WAL segments are read and applied when restoring from the database
backup.  At this point, you should be able to verify that the
database was restored to the point before creating the test table:</p><pre class="literallayout">psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'</pre><p>This SQL command should show that the pitrtest table does not exist
at this recovery time. The output should be similar to:</p><p>PostgreSQL allows you to pause the recovery process if the target name
or time is specified.  This pause would allow a DBA a chance to review
the recovery time/name and see if this is what they want or expect.  If so,
the DBA can run the following command to resume and complete the recovery:</p><pre class="literallayout">psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_xlog_replay_resume()'</pre><p>Until you run the statement above, the database will be left in read-only
mode.</p><p>Next, run the script to restore the database
to the <span class="strong"><strong>afterchanges</strong></span> restore point, do this by updating the
RECOVERY_TARGET_NAME to <span class="strong"><strong>afterchanges</strong></span>:</p><pre class="literallayout">vi ./run-restore-pitr.sh
./run-restore-pitr.sh</pre><p>After this restore, you should be able to see the test table:</p><pre class="literallayout">psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_xlog_replay_resume()'</pre><p>Lastly, lets start a recovery using all of the WAL files. This will get the
restored database as current as possible. To do so, edit the script
to remove the RECOVERY_TARGET_NAME environment setting completely:</p><pre class="literallayout">./run-restore-pitr.sh
sleep 30
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'create table foo (id int)'</pre><p>At this point, you should be able to create new data in the restored database
and the test table should be present.  When you recover the entire
WAL history, resuming the recovery is not necessary to enable writes.</p><p>Other options exist for performing a PITR. See the <a class="ulink" href="pitr.adoc" target="_top">PITR documentation</a> for
full details.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_20"></a>Kubernetes</h4></div></div></div><p>This example is identical to the OpenShift PITR example; please see below for
details on how the PITR example works.</p><p>The only differences are the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
paths are <span class="strong"><strong>examples/kube/pitr</strong></span>
</li><li class="listitem">
JSON and scripts are modifed to work with Kube
</li><li class="listitem">
<span class="strong"><strong>kubectl</strong></span> commands are used instead of <span class="strong"><strong>oc</strong></span> commands
</li><li class="listitem">
database services resolve to <span class="strong"><strong>default.svc.cluster.local</strong></span> instead
   of <span class="strong"><strong>openshift.svc.cluster.local</strong></span>
</li></ul></div><p>See <a class="ulink" href="pitr.adoc" target="_top">PITR Documentation</a> for details on PITR concepts and how PITR is implemented
within the Suite.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_24"></a>OpenShift</h4></div></div></div><p>This is a complex example.  For details on how PITR is implemented
within the Suite, see the <a class="ulink" href="pitr.adoc" target="_top">PITR Documentation</a> for details and background.</p><p>This example, $CCPROOT/examples/openshift/pitr, provides an
example of performing a PITR using OpenShift.</p><p>Lets start by running the example database container:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/pitr
./run-master-pitr.sh</pre><p>This step will create a database container, <span class="strong"><strong>master-pitr</strong></span>.  This
container is configured to continuously write WAL segment files
to a mounted volume (/pgwal).</p><p>After you start the database, you will create a base backup
using this command:</p><pre class="literallayout">./run-master-pitr-backup.sh</pre><p>This will create a backup and write the backup files to a persistent
volume (/pgbackup).</p><p>Next, lets create some recovery targets within the database, run
the SQL commands against the <span class="strong"><strong>master-pitr</strong></span> database as follows:</p><pre class="literallayout">./run-sql.sh</pre><p>This will create recovery targets named <span class="strong"><strong>beforechanges</strong></span>, <span class="strong"><strong>afterchanges</strong></span>, and
<span class="strong"><strong>nomorechanges</strong></span>.  It will create a table, <span class="strong"><strong>pitrtest</strong></span>, between
the <span class="strong"><strong>beforechanges</strong></span> and <span class="strong"><strong>afterchanges</strong></span> targets.  It will also run a SQL
CHECKPOINT to flush out the changes to WAL segments.</p><p>Next, now that we have a base backup and a set of WAL files containing
our database changes, we can shut down the <span class="strong"><strong>master-pitr</strong></span> database
to simulate a database failure.  Do this by running the following:</p><pre class="literallayout">oc delete pod master-pitr</pre><p>Next, we will create 3 different restored database containers based
upon the base backup and the saved WAL files.</p><p>First, we restore prior to the <span class="strong"><strong>beforechanges</strong></span> recovery target.  This
recovery point is <span class="strong"><strong>before</strong></span> the <span class="strong"><strong>pitrtest</strong></span> table is created.</p><p>Edit the master-pitr-restore.json file, and edit the environment
variable to indicate we want to use the <span class="strong"><strong>beforechanges</strong></span> recovery
point:</p><pre class="literallayout">}, {
"name": "RECOVERY_TARGET_NAME",
"value": "beforechanges"
}, {</pre><p>Then run the following to create the restored database container:</p><pre class="literallayout">./run-restore-pitr.sh</pre><p>After the database has restored, you should be able to perform
a test to see if the recovery worked as expected:</p><pre class="literallayout">psql -h master-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'table pitrtest'
psql -h master-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'create table foo (id int)'
psql -h master-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'select pg_xlog_replay_resume()'
psql -h master-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'create table foo (id int)'</pre><p>The output of these command should show that the <span class="strong"><strong>pitrtest</strong></span> table is not
present.  It should also show that you can not create a new table
because the database is paused in recovery mode.  Lastly, if you
execute a <span class="strong"><strong>resume</strong></span> command, it will show that you can now create
a table as the database has fully recovered.</p><p>You can also test that if <span class="strong"><strong>afterchanges</strong></span> is specified, that the
<span class="strong"><strong>pitrtest</strong></span> table is present but that the database is still in recovery
mode.</p><p>Lastly, you can test a full recovery using <span class="strong"><strong>all</strong></span> of the WAL files, if
you remove the <span class="strong"><strong>RECOVERY_TARGET_NAME</strong></span> environment variable completely.</p><p>The NFS portions of this example depend upon an NFS file
system with the following path configurations be present:</p><pre class="literallayout">/nfsfileshare
/nfsfileshare/backups
/nfsfileshare/WAL</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_pgaudit_pgaudit"></a>1.30. pgaudit - PGAudit</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_15"></a>Docker</h4></div></div></div><p>This example, $CCPROOT/examples/docker/pgaudit, provides an
example of enabling pgaudit output.  As of release 1.3,
pgaudit is included in the crunchy-postgres container and is
added to the Postgres shared library list in the postgresql.conf.</p><p>Given the numerous ways pgaudit can be configured, the exact
pgaudit configuration is left to the user to define.  pgaudit
allows you to configure auditing rules either in postgresql.conf
or within your SQL script.</p><p>For this test, we place pgaudit statements within a SQL script
and verify that auditing is enabled and working.  If you choose
to configure pgaudit via a postgresql.conf file, then you will
need to define your own custom postgresql.conf file and mount
it to override the default postgresql.conf file.</p><p>To run this example, run the following to create a
database container:</p><pre class="literallayout">cd $CCPROOT/examples/docker/pgaudit
./run.sh</pre><p>This starts a database on port 12005 on localhost.  You can then
run the test script as follows:</p><pre class="literallayout">./test-pgaudit.sh</pre><p>This test executes a SQL file which contains pgaudit configuration
statements as well as executes some basic SQL commands.  These
SQL commands will cause pgaudit to create audit log messages in
the pg_log log file created by the database container.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_swarm_docker_swarm"></a>1.31. swarm - Docker swarm</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_16"></a>Docker</h4></div></div></div><p>This example shows how to run a master and replica database
container on a Docker Swarm (v.1.12) cluster.</p><p>First, set up a cluster. The Kubernetes libvirt coreos cluster
example works well; see <a class="ulink" href="http://kubernetes.io/docs/getting-started-guides/libvirt-coreos/" target="_top">coreos-libvirt-cluster.</a></p><p>Next, on each node, create the Swarm using these
<a class="ulink" href="https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/" target="_top">Swarm Install instructions.</a></p><p>Includes the command on the manager node:</p><pre class="literallayout">docker swarm init --advertise-addr 192.168.10.1</pre><p>Then the command on all the worker nodes:</p><pre class="literallayout"> docker swarm join \
     --token SWMTKN-1-65cn5wa1qv76l8l45uvlsbprogyhlprjpn27p1qxjwqmncn37o-015egopg4jhtbmlu04faon82u \
         192.168.10.1.37</pre><p>Before creating Swarm services, for service discovery you need
to define an overlay network to be used by the services you will
create.  Create the network like this:</p><pre class="literallayout">docker network create --driver overlay crunchynet</pre><p>We want to have the master database always placed on
a specific node. This is accomplished using node constraints
as follows:</p><pre class="literallayout">docker node inspect kubernetes-node-1 | grep ID
docker node update --label-add type=master 18yrb7m650umx738rtevojpqy</pre><p>In the above example, the kubernetes-node-1 node with ID 18yrb7m650umx738rtevojpqy has a user defined label of <span class="strong"><strong>master</strong></span> added to it.  The master service
specifies <span class="strong"><strong>master</strong></span> as a constraint when created; this tells Swarm
to place the service on that specific node.  The replica specifies
a constraint of <span class="strong"><strong>node.labels.type != master</strong></span> to have the replica
always placed on a node that is not hosting the master service.</p><p>After you set up the Swarm cluster, you can then
run the <span class="strong"><strong>$CCPROOT/examples/docker/swarm-service</strong></span> example as follows
on the <span class="strong"><strong>Swarm Manager Node</strong></span>:</p><pre class="literallayout">cd $CCPROOT/examples/docker/swarm-service
./run.sh</pre><p>You can then find the nodes that are running the master and replica containers
by:</p><pre class="literallayout">docker service ps master
docker service ps replica</pre><p>Given the PostgreSQL replica service is named <span class="strong"><strong>replica</strong></span>, you can scale up
the number of replica containers by running this command:</p><pre class="literallayout">docker service scale replica=2
docker service ls</pre><p>You can verify you have two replicas within PostgreSQL by viewing
the <span class="strong"><strong>pg_stat_replication</strong></span> table, the password is <span class="strong"><strong>password</strong></span>, when
logged into the kubernetes-node-1 host:</p><pre class="literallayout">docker exec -it $(docker ps -q) psql -U postgres -c 'table pg_stat_replication' postgres</pre><p>You should see a row for each replica along with its replication status.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_watch_automated_failover"></a>1.32. watch - Automated failover</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_docker_17"></a>Docker</h4></div></div></div><p>This example shows how to run the crunchy-watch container
to perform an automated failover.  For the example to
work, the host on which you are running needs to allow
read-write access to /run/docker.sock.  The crunchy-watch
container runs as the <span class="strong"><strong>postgres</strong></span> user, so adjust the
file permissions of /run/docker.sock accordingly.</p><p>Run the example as follows (depends on master-replica example
being run prior):</p><pre class="literallayout">cd $CCPROOT/examples/docker/watch
./run.sh</pre><p>This will start the watch container which tests every few seconds
whether the master database is running, if not, it will
trigger a failover (using docker exec) on the replica host.</p><p>Test it out by stopping the master:</p><pre class="literallayout">docker stop master
docker logs watch</pre><p>Look at the watch container logs to see it perform the failover.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_21"></a>Kubernetes</h4></div></div></div><p>This example assumes you have run the master-replica example prior
to this example!</p><p>This example runs a crunchy-watch container to look for the
master within a Postgres cluster, if it can not find the master it
will proceed to cause a failover to a replica.</p><p>Running the example:</p><pre class="literallayout">examples/kube/watch/run.sh</pre><p>Check out the log of the watch container as follows:</p><pre class="literallayout">kubectl log watch</pre><p>Then trigger a failover using this command:</p><pre class="literallayout">kubectl delete pod master</pre><p>Resume watching the watch container’s log and verify that it
detects the master is not reachable and performs a failover
on the replica.</p><p>A final test is to see if the old replica is now a fully functioning
master by inserting some test data into it as follows:</p><pre class="literallayout">psql -h master -U postgres postgres -c 'create table failtest (id int)'</pre><p>The above command still works because the watch container has
changed the labels of the replica to make it a master, so the master
service will still work and route now to the new master even though
the pod is named replica.</p><p><span class="strong"><strong>Tip</strong></span></p><p>You can view the labels on a pod with this command:</p><pre class="literallayout">kubectl describe pod replica | grep Label</pre></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_25"></a>OpenShift</h4></div></div></div><p>This example shows how a form of automated failover can be
configured for a master and replica deployment.</p><p>First, create a master and a replica, in this case the replica lives in a
Deployment which can scale up:</p><pre class="literallayout">cd $CCPROOT/examples/openshift/master-replica-dc
./run.sh</pre><p>Next, create an OpenShift service account which is used by the crunchy-watch
container to perform the failover, also set policies that allow the
service account the ability to edit resources within the OpenShift and
default projects :</p><pre class="literallayout">cd $CCPROOT/examples/openshift/watch
oc create -f watch-sa.json
oc policy add-role-to-group edit system:serviceaccounts -n openshift
oc policy add-role-to-group edit system:serviceaccounts -n default</pre><p>Next, create the container that will <span class="emphasis"><em>watch</em></span> the Postgresql cluster:</p><pre class="literallayout">./run.sh</pre><p>At this point, the watcher will sleep every 20 seconds (configurable) to
see if the master is responding.  If the master doesn’t respond, the watcher
will perform the following logic:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
log into OpenShift using the service account
</li><li class="listitem">
set its current project
</li><li class="listitem">
find the first replica pod
</li><li class="listitem">
delete the master service saving off the master service definition
</li><li class="listitem">
create the trigger file on the first replica pod
</li><li class="listitem">
wait 20 seconds for the failover to complete on the replica pod
</li><li class="listitem">
edit the replica pod’s label to match that of the master
</li><li class="listitem">
recreate the master service using the stored service definition
</li><li class="listitem">
loop through the other remaining replica and delete its pod
</li></ul></div><p>At this point, clients when access the master’s service will actually
be accessing the new master.  Also, OpenShift will recreate the number
of replicas to its original configuration which each replica pointed to the
new master.  Replication from the master to the new replicas will be
started as each new replica is started by OpenShift.</p><p>To test it out, delete the master pod and view the watch pod log:</p><pre class="literallayout">oc delete pod master-dc
oc logs watch
oc get pod</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_ssl_postgresql_ssl_auth_method"></a>1.33. ssl - PostgreSQL ssl auth method</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_26"></a>OpenShift</h4></div></div></div><p>This example shows how you can configure Postgres to use ssl for
client authentication.</p><p>The example is found at:</p><pre class="literallayout">./examples/openshift/customer-config-ssl</pre><p>The example requires SSL keys to be created, the example script
<span class="strong"><strong>keys.sh</strong></span> is required to be executed to create the required
server and client certificates.  This script also creates
a client key configuration you can use to test with.</p><p>The example requires an NFS volume, /pgconf, be mounted into which
the Postgres configuration files and keys are copied to.  Permissions
of the keys are important as well, they will need to be owned
by either the <span class="strong"><strong>root</strong></span> or <span class="strong"><strong>postgres</strong></span> user.  The <span class="strong"><strong>run.sh</strong></span> script
copies the required files and sets these permissions when executing the example.</p><p>The <span class="strong"><strong>keys.sh</strong></span> script creates a client cert with the <span class="strong"><strong>testuser</strong></span> specified
as the CN.  The <span class="strong"><strong>testuser</strong></span> Postgres user is created by the <span class="strong"><strong>setup.sql</strong></span>
configuration script as normal.  It is with the <span class="strong"><strong>testuser</strong></span> role that
you will test with.</p><p>Run the Postgres example as follows:</p><pre class="literallayout">./run.sh</pre><p>A required step to make this example work is to define
in your <span class="strong"><strong>/etc/hosts</strong></span> file an entry that maps <span class="strong"><strong>server.crunchydata.com</strong></span>
to the example’s service IP address, this is because we generate
a server certificate with the server name of <span class="strong"><strong>server.crunchyhdata.com</strong></span>.</p><p>For example, if your service has an address as follows:</p><pre class="literallayout"> oc get service
NAME                CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE
custom-config-ssl   172.30.211.108   &lt;none&gt;        5432/TCP</pre><p>Then your <span class="strong"><strong>/etc/hosts</strong></span> file needs an entry like this:</p><pre class="literallayout">172.30.211.108 server.crunchydata.com</pre><p>For a production Openshift installation, you’ll likely want DNS
names to resolve to the Postgres Service name and generate
server certificates using the DNS names instead of an example
name like <span class="strong"><strong>server.crunchydata.com</strong></span>.</p><p>Once the container starts up, you can test the SSL connection
as follows:</p><pre class="literallayout">psql -h server.crunchydata.com -U testuser userdb</pre><p>You should see a connection that looks like the following:</p><pre class="literallayout">psql (9.6.3)
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

userdb=&gt;</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_workshop_openshift_workshop"></a>1.34. workshop - OpenShift Workshop</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_openshift_27"></a>OpenShift</h4></div></div></div><p>This example, $CCPROOT/examples/openshift/workshop, provides an
example of using OpenShift Templates to build pods, routes, services, etc.</p><p>You use the <span class="strong"><strong>oc new-app</strong></span> command to create objects from the
JSON templates.  This is an alternative way to create OpenShift objects
instead of using <span class="strong"><strong>oc create</strong></span>.</p><p>This example is used within a joint Redhat-Crunchy workshop that is
given at various conferences to demonstrate OpenShift and Crunchy Containers
working together.  Thanks to Steven Pousty from Redhat for this
example!</p><p>See the README file within the workshop directory for instructions
on running the example.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_upgrade_pg_upgrade"></a>1.35. upgrade - pg_upgrade</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_22"></a>Kubernetes</h4></div></div></div><p>Starting in release 1.3.1, the upgrade container will let
you perform a pg_upgrade on a 9.5 database converting its data
to a 9.6 version.</p><p>This example assumes you have run <span class="strong"><strong>master-pvc</strong></span> using a 9.5 image
such as <span class="strong"><strong>centos7-9.5-1.5</strong></span> prior to running this upgrade.</p><p>Prior to starting this example, shut down the <span class="strong"><strong>master-pvc</strong></span> database
using the <span class="strong"><strong>examples/kube/master-pvc/cleanup.sh</strong></span> script.</p><p>Prior to running this example, make sure your CCP_IMAGE_TAG
environment variable is using a 9.6 image such as <span class="strong"><strong>centos7-9.6-1.5</strong></span>.</p><p>Start the upgrade as follows:</p><pre class="literallayout">cd $CCPROOT/examples/kube/upgrade
./run.sh</pre><p>This will create the following in your Kube environment:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
a Kube Job running the <span class="strong"><strong>crunchy-upgrade</strong></span> container
</li><li class="listitem">
a new data directory name <span class="strong"><strong>master-upgrade</strong></span> found in the <span class="strong"><strong>pgnewdata</strong></span>
 PVC
</li></ul></div><p>If successful, the Job will end with a Successful status, verify
the results of the Job by examining the Job’s pod log:</p><pre class="literallayout">kubectl get pod -a -l job-name=upgrade-job
kubectl logs -l job-name=upgrade-job</pre><p>You can verify the upgraded database by running the
<span class="strong"><strong>examples/kube/master-upgrade</strong></span>
example, this example will mount the newly created and upgraded
database files.  Database tables and data that were in the <span class="strong"><strong>master-pvc</strong></span>
test database should be found in the <span class="strong"><strong>master-upgrade</strong></span> database.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_google_cloud_environment_kubernetes_cluster"></a>1.36. google cloud environment - Kubernetes cluster</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_kubernetes_23"></a>Kubernetes</h4></div></div></div><p>The Postgres Container Suite was tested on Google Container Engine.</p><p>Here is a link to set up a Kube cluster on GCE:
<a class="ulink" href="https://kubernetes.io/docs/getting-started-guides/gce" target="_top">https://kubernetes.io/docs/getting-started-guides/gce</a></p><p>Setup the persistent disks using GCE disks by first editing
<span class="strong"><strong>examples/envvars.sh</strong></span> and set the GCE settings to match your
GCE environment.</p><p>Then create the PVs used by the examples, passing in the <span class="strong"><strong>gce</strong></span>
value as a parameter, this will cause the GCE disks to be created:</p><pre class="literallayout">examples/pv/create-pv.sh gce
examples/pv/create-pvc.sh</pre><p>Here is a link that describes more information on GCE persistent disk:
<a class="ulink" href="https://cloud.google.com/container-engine/docs/tutorials/persistent-disk/" target="_top">https://cloud.google.com/container-engine/docs/tutorials/persistent-disk/</a></p><p>To have the persistent disk examples work, you will need to specify
a <span class="strong"><strong>fsGroup</strong></span> setting in the <span class="strong"><strong>SecurityContext</strong></span> of each pod script
as follows:</p><pre class="literallayout">       "securityContext": {
        "fsGroup": 26
        },</pre><p>For our Postgres container, we have specified a UID of 26 as the user
which corresponds to the <span class="strong"><strong>fsGroup</strong></span> value.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_docker_tips"></a>2. Docker - Tips</h2></div></div></div><p><span class="strong"><strong>Send a signal to PostgreSQL</strong></span></p><p>First, find the PID of the postmaster:</p><pre class="literallayout">docker exec -it master cat /pgdata/master/postmaster.pid</pre><p>Then, send it the signal to kill it or other signal depending on what you want to do:</p><pre class="literallayout">docker exec -it master kill -SIGTERM 22</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_openshift_tips"></a>3. OpenShift - Tips</h2></div></div></div><p><span class="strong"><strong>Find PostgreSQL passwords</strong></span></p><p>The passwords used for the PostgreSQL user accounts are generated
by the OpenShift <span class="emphasis"><em>process</em></span> command.  To inspect what value was
supplied, you can inspect the master pod as follows:</p><pre class="literallayout">oc get pod ms-master -o json | grep PG</pre><p>Look for the values of the environment variables:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
PG_USER
</li><li class="listitem">
PG_PASSWORD
</li><li class="listitem">
PG_DATABASE
</li></ul></div><p><span class="strong"><strong>Password management</strong></span></p><p>Remember that if you do a database restore, you will get
whatever user IDs and passwords that were saved in the
backup.  So, if you do a restore to a new database
and use generated passwords, the new passwords will
not be the same as the passwords stored in the backup!</p><p>You have various options to deal with managing your
passwords:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
externalize your passwords using secrets instead of using generated values
</li><li class="listitem">
manually update your passwords to your known values after a restore
</li></ul></div><p>Note that you can edit the environment variables when there is a <span class="emphasis"><em>dc</em></span>
using, currently only the replicas have a <span class="emphasis"><em>dc</em></span> to avoid the possiblity
of creating multiple masters, this might need to change in the future,
to better support password management:</p><pre class="literallayout">oc env dc/pg-master-rc PG_MASTER_PASSWORD=foo PG_MASTER=user1</pre><p><span class="strong"><strong>NFS Setup</strong></span></p><p>To control the permissions of the NFS file system
certain examples make use of the <span class="strong"><strong>supplementalGroups</strong></span> security context
setting for pods.  In these examples, we specify the GID of the <span class="strong"><strong>nfsnobody</strong></span>
group (65534).  If you want to use a different GID for the supplementalGroup
then you will need to alter the NFS examples accordingly.</p><p>When the pod runs, the pod user is UID <span class="strong"><strong>26</strong></span> which is the Postgres
user ID.  By specifying the <span class="strong"><strong>supplementalGroup</strong></span> the pod will also
be added to the <span class="strong"><strong>nfsnobody</strong></span> group.  So, when you set up your NFS
mount, you can specify the permissions to be as follows:</p><pre class="literallayout">drwxrwx---.   3 nfsnobody nfsnobody   23 Dec 16 11:28 nfsfileshare</pre><p>This restricts <span class="strong"><strong>other</strong></span> users from writing to the NFS share, but will
allow the <span class="strong"><strong>nfsnobody</strong></span> group to have write access.  This way, the
NFS mount permissions can be managed to only allow certain pods
write access.</p><p>Also, remember that on systems with SELinux set to enforcing mode
that you will need to allow NFS write permissions by running
this command:</p><pre class="literallayout">sudo setsebool -P virt_use_nfs 1</pre><p>Note that supplementalGroup settings are required for NFS but you
would use the fsGroup setting for the AWS file system.  Check out
this link for details:
<a class="ulink" href="https://docs.openshift.org/latest/install_config/persistent_storage/pod_security_context.html" target="_top">https://docs.openshift.org/latest/install_config/persistent_storage/pod_security_context.html</a></p><p><span class="strong"><strong>Examine backup job log</strong></span></p><p>Database backups are implemented as a Kubernetes Job.  A Job is meant to run one time only
and not be restarted by Kubernetes.  To view jobs in OpenShift you enter:</p><pre class="literallayout">oc get jobs
oc describe job backupjob</pre><p>You can get detailed logs by referring to the pod identifier in the job <span class="emphasis"><em>describe</em></span>
output as follows:</p><pre class="literallayout">oc logs backupjob-pxh2o</pre><p><span class="strong"><strong>Backup lifecycle</strong></span></p><p>Backups require the use of network storage like NFS in OpenShift.
There is a required order of using NFS volumes in the manner
we do database backups.</p><p>So, first off, there is a one-to-one relationship between
a PV (persistent volume) and a PVC (persistence volume claim).  You
can NOT have a one-to-many relationship between PV and PVC(s).</p><p>So, to do a database backup repeatably, you will need to following
this general pattern:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
as OpenShift admin user, create a unique PV (e.g. backup-pv-mydatabase)
</li><li class="listitem">
as a project user, create a unique PVC (e.g. backup-pvc-mydatabase)
</li><li class="listitem">
reference the unique PVC within the backup-job template
</li><li class="listitem">
execute the backup job template
</li><li class="listitem">
as a project user, delete the job
</li><li class="listitem">
as a project user, delete the pvc
</li><li class="listitem">
as OpenShift admin user, delete the unique PV
</li></ul></div><p>This procedure will need to be scripted and executed by the devops team when
performing a database backup.</p><p><span class="strong"><strong>Restore lifecycle</strong></span></p><p>To perform a database restore, we do the following:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
locate the NFS path to the database backup we want to restore with
</li><li class="listitem">
edit a PV to use that NFS path
</li><li class="listitem">
edit a PV to specify a unique label
</li><li class="listitem">
create the PV
</li><li class="listitem">
edit a PVC to use the previously created PV, specifying the same label
   used in the PV
</li><li class="listitem">
edit a database template, specifying the PVC to be used for mounting
   to the /backup directory in the database pod
</li><li class="listitem">
create the database pod
</li></ul></div><p>If the /pgdata directory is blank AND the /backup directory contains
a valid Postgres backup, it is assumed the user wants to perform a
database restore.</p><p>The restore logic will copy /backup files to /pgdata before starting
the database.  It will take time for the copying of the files to
occur since this might be a large amount of data and the volumes
might be on slow networks. You can view the logs of the database pod
to measure the copy progress.</p><p><span class="strong"><strong>Log aggregation</strong></span></p><p>OpenShift can be configured to include the EFK stack for log aggregation.
OpenShift Administrators can configure the EFK stack as documented
here:</p><p><a class="ulink" href="https://docs.openshift.com/enterprise/3.1/install_config/aggregate_logging.html" target="_top">https://docs.openshift.com/enterprise/3.1/install_config/aggregate_logging.html</a></p><p><span class="strong"><strong>nss_wrapper</strong></span></p><p>If an OpenShift deployment requires that random generated UIDs be
supported by containers, the Crunchy containers can be modified
similar to those located here to support the use of nss_wrapper
to equate the random generated UIDs/GIDs by OpenShift with
the postgres user:</p><p><a class="ulink" href="https://github.com/openshift/postgresql/blob/master/9.4/root/usr/share/container-scripts/postgresql/common.sh" target="_top">https://github.com/openshift/postgresql/blob/master/9.4/root/usr/share/container-scripts/postgresql/common.sh</a></p><p><span class="strong"><strong>DNS configuration</strong></span></p><p>As of OSE 3.3, the following DNS modifications are not typically necessary
any longer….but I’m leaving them here as a reference….</p><p>Luke Meyer from Redhat wrote an excellent blog on how
to configure dnsmasq and OpenShift, it is located here:</p><p><a class="ulink" href="http://developers.redhat.com/blog/2015/11/19/dns-your-openshift-v3-cluster/" target="_top">http://developers.redhat.com/blog/2015/11/19/dns-your-openshift-v3-cluster/</a></p><p>Key things included in this blog are:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
configuring dhcp to include the local IP address in /etc/resolv.conf upon boot
</li><li class="listitem">
configuring dnsmasq
</li><li class="listitem">
configuring OpenShift dns to listen on another port
</li></ul></div><p>In my dev setup, I have OpenShifts DNS listening on 127.0.0.1:8053.
I have my dnsmasq listening on the local IP address 192.168.0.109:53</p><p>Therefore in my /etc/dhcp/dhclient.conf I have this config:</p><pre class="literallayout">prepend domain-name-servers 192.168.0.109;</pre><p>If you don’t have your DNS configured correctly, replication controllers
and deployment configs basically will not work.</p><p><span class="strong"><strong>DNS host entry</strong></span></p><p>If your OpenShift environment can not resolve your hostname via
a DNS server (external to OpenShift!), you will get errors when trying
to create a DeploymentConfig.  So, you can either install dnsmasq
and reconfigure OpenShift for that, or, you can run a DNS server
on another host and add the OpenShift host entry to that DNS server.  I
use the skybridge2 Docker container for this purpose.  You have
to remember to adjust your /etc/resolv.conf to specify this new DNS
server.</p><p><span class="strong"><strong>System policies for PV creation/listing</strong></span></p><p>For my testing, I wanted to allow the <span class="strong"><strong>system</strong></span> user to be able
to create and list persistent volumes, as of OSE 3.3, I had to
enter these commands as the <span class="strong"><strong>root</strong></span> user after installation to
modify the policies:</p><pre class="literallayout">oadm policy add-role-to-user cluster-reader system
oc describe clusterPolicyBindings :default
oadm policy add-cluster-role-to-user cluster-reader system
oc describe clusterPolicyBindings :default
oc describe clusterPolicyBindings :default
oadm policy add-cluster-role-to-user cluster-admin system</pre><p><span class="strong"><strong>Persistent volume matching</strong></span></p><p>Restoring a database from an NFS backup requires the building
of a PV which maps to the NFS backup archive path.  For example,
if you have a backup at /backups/pg-foo/2016-01-29:22:34:20
then we create a PV that maps to that NFS path.  We also use
a "label" on the PV so that the specific backup PV can be identified.</p><p>We use the pod name in the label value to make the PV unique.  This
way, the related PVC can find the right PV to map to and not some other
PV.  In the PVC, we specify the same "label" which lets Kubernetes
match to the correct PV.</p><p><span class="strong"><strong>anyuid permissions</strong></span></p><p>For my testing, I created a user named <span class="strong"><strong>test</strong></span> on OSE, then
I ran the following command to grant it permission to use the <span class="strong"><strong>anyuid</strong></span> SCC:</p><pre class="literallayout">oc adm policy add-scc-to-group anyuid system:authenticated</pre><p>This says that any authenticate user can run with the anyuid SCC which lets
them create PVCs and use the <span class="strong"><strong>fsGroup</strong></span> setting for the Postgres containers to
work using NFS.  There is most likely a smarter and more precise way to grant
this permission but this is one suggested process.</p></div></div></body></html>